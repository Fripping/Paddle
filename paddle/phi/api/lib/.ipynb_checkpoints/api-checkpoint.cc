
#include "paddle/phi/api/include/api.h"
#include <memory>

#include "glog/logging.h"
#include "paddle/common/flags.h"

#include "paddle/phi/api/lib/api_custom_impl.h"
#include "paddle/phi/api/lib/api_gen_utils.h"
#include "paddle/phi/api/lib/api_registry.h"
#include "paddle/phi/api/lib/data_transform.h"
#include "paddle/phi/api/include/tensor_utils.h"
#include "paddle/phi/api/lib/kernel_dispatch.h"
#include "paddle/phi/common/type_traits.h"
#include "paddle/phi/core/kernel_registry.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/fusion.h"

#include "paddle/phi/api/profiler/event_tracing.h"
#include "paddle/phi/api/profiler/supplement_tracing.h"

#ifdef PADDLE_WITH_DISTRIBUTE
#include "paddle/phi/infermeta/spmd_rules/rules.h"
#include "paddle/phi/core/distributed/auto_parallel/reshard/reshard_utils.h"
#endif

PD_DECLARE_bool(conv2d_disable_cudnn);
COMMON_DECLARE_int32(low_precision_op_list);

namespace paddle {
namespace experimental {


PADDLE_API Tensor abs(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "abs API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "abs", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("abs", kernel_data_type);
  }
  VLOG(6) << "abs kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("abs", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("abs infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("abs compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& abs_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "abs API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "abs", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("abs", kernel_data_type);
  }
  VLOG(6) << "abs kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("abs", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("abs infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("abs compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> accuracy(const Tensor& x, const Tensor& indices, const Tensor& label) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "accuracy API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "accuracy", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("accuracy", kernel_data_type);
  }
  VLOG(6) << "accuracy kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("accuracy", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("accuracy infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::AccuracyInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_indices), MakeMetaTensor(*input_label), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("accuracy compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_label, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor accuracy_check(const Tensor& x, const Tensor& y, const std::string& fn_name, double rtol, double atol, bool equal_nan) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "accuracy_check API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "accuracy_check", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("accuracy_check", kernel_data_type);
  }
  VLOG(6) << "accuracy_check kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["fn_name"] = fn_name;
     attrs["rtol"] = rtol;
     attrs["atol"] = atol;
     attrs["equal_nan"] = equal_nan;
     phi::RecordOpInfoSupplement("accuracy_check", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("accuracy_check infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ValueCompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, double, double, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("accuracy_check compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, fn_name, rtol, atol, equal_nan, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor acos(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acos API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acos", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acos", kernel_data_type);
  }
  VLOG(6) << "acos kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acos", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acos infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acos compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& acos_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acos API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acos", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acos", kernel_data_type);
  }
  VLOG(6) << "acos kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acos", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acos infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acos compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor acosh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acosh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acosh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acosh", kernel_data_type);
  }
  VLOG(6) << "acosh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acosh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acosh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acosh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& acosh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acosh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acosh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acosh", kernel_data_type);
  }
  VLOG(6) << "acosh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acosh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acosh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acosh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> adadelta_(Tensor& param, const Tensor& grad, Tensor& avg_squared_grad, Tensor& avg_squared_update, const Tensor& learning_rate, paddle::optional<Tensor>& master_param, float rho, float epsilon, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, avg_squared_grad, avg_squared_update, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adadelta_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adadelta", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adadelta_", kernel_data_type);
  }
  VLOG(6) << "adadelta kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_avg_squared_grad = PrepareData(avg_squared_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_avg_squared_update = PrepareData(avg_squared_update, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"avg_squared_grad", {
     (*input_avg_squared_grad).dims()}},
     {"avg_squared_update", {
     (*input_avg_squared_update).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"master_param",
     master_param_record_shapes}};
     phi::AttributeMap attrs;
     attrs["rho"] = rho;
     attrs["epsilon"] = epsilon;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("adadelta_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, avg_squared_grad, avg_squared_update, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("adadelta_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_avg_squared_grad = *input_avg_squared_grad;

  auto origin_input_avg_squared_update = *input_avg_squared_update;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::AdadeltaInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_avg_squared_grad), MakeMetaTensor(origin_input_avg_squared_update), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), rho, epsilon, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("adadelta_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_avg_squared_grad, origin_input_avg_squared_update, *input_learning_rate, input_master_param, rho, epsilon, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> adagrad_(Tensor& param, const Tensor& grad, Tensor& moment, const Tensor& learning_rate, paddle::optional<Tensor>& master_param, float epsilon, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, moment, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && grad.is_dense_tensor() && moment.is_dense_tensor() && learning_rate.is_dense_tensor() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "adagrad_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "adagrad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adagrad_", kernel_data_type);
    }
    VLOG(6) << "adagrad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"moment", {
       (*input_moment).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["epsilon"] = epsilon;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("adagrad_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("adagrad_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment = *input_moment;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::AdagradInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_moment), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), epsilon, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("adagrad_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_moment, *input_learning_rate, input_master_param, epsilon, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);

    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() && moment.is_dense_tensor() && learning_rate.is_dense_tensor() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "adagrad_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "adagrad_dense_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adagrad_", kernel_data_type);
    }
    VLOG(6) << "adagrad_dense_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {});

    auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"moment", {
       (*input_moment).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["epsilon"] = epsilon;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("adagrad_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("adagrad_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment = *input_moment;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::AdagradInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_moment), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), epsilon, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("adagrad_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_moment, *input_learning_rate, input_master_param, epsilon, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (adagrad_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> adam_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& moment1, Tensor& moment2, Tensor& beta1_pow, Tensor& beta2_pow, paddle::optional<Tensor>& master_param, const paddle::optional<Tensor>& skip_update, const Scalar& beta1, const Scalar& beta2, const Scalar& epsilon, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && grad.is_dense_tensor() && learning_rate.is_dense_tensor() && moment1.is_dense_tensor() && moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() && beta2_pow.is_dense_tensor() && (!master_param || master_param->is_dense_tensor()) && (!skip_update || skip_update->is_dense_tensor())) {

    VLOG(6) << "adam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "adam", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adam_", kernel_data_type);
    }
    VLOG(6) << "adam kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<phi::DDim> skip_update_record_shapes;
       if(input_skip_update){
         skip_update_record_shapes.push_back((*input_skip_update).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"moment1", {
       (*input_moment1).dims()}},
       {"moment2", {
       (*input_moment2).dims()}},
       {"beta1_pow", {
       (*input_beta1_pow).dims()}},
       {"beta2_pow", {
       (*input_beta2_pow).dims()}},
       {"master_param", master_param_record_shapes},
       {"skip_update",
       skip_update_record_shapes}};
       phi::AttributeMap attrs;
      switch (beta1.dtype()) {
        case DataType::FLOAT32:
            attrs["beta1"] = static_cast<float>(beta1.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["beta1"] = static_cast<double>(beta1.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["beta1"] = static_cast<float>(beta1.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["beta1"] = static_cast<float>(beta1.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["beta1"] = static_cast<int32_t>(beta1.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["beta1"] = static_cast<int64_t>(beta1.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["beta1"] = static_cast<int16_t>(beta1.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["beta1"] = static_cast<int8_t>(beta1.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["beta1"] = static_cast<uint16_t>(beta1.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["beta1"] = static_cast<uint8_t>(beta1.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["beta1"] = static_cast<bool>(beta1.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["beta1"] = static_cast<float>(beta1.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["beta1"] = static_cast<double>(beta1.to<complex128>());
            break;
        default:
            attrs["beta1"] = "";
            break;
      }
      switch (beta2.dtype()) {
        case DataType::FLOAT32:
            attrs["beta2"] = static_cast<float>(beta2.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["beta2"] = static_cast<double>(beta2.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["beta2"] = static_cast<float>(beta2.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["beta2"] = static_cast<float>(beta2.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["beta2"] = static_cast<int32_t>(beta2.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["beta2"] = static_cast<int64_t>(beta2.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["beta2"] = static_cast<int16_t>(beta2.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["beta2"] = static_cast<int8_t>(beta2.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["beta2"] = static_cast<uint16_t>(beta2.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["beta2"] = static_cast<uint8_t>(beta2.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["beta2"] = static_cast<bool>(beta2.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["beta2"] = static_cast<float>(beta2.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["beta2"] = static_cast<double>(beta2.to<complex128>());
            break;
        default:
            attrs["beta2"] = "";
            break;
      }
      switch (epsilon.dtype()) {
        case DataType::FLOAT32:
            attrs["epsilon"] = static_cast<float>(epsilon.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["epsilon"] = static_cast<double>(epsilon.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["epsilon"] = static_cast<float>(epsilon.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["epsilon"] = static_cast<float>(epsilon.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["epsilon"] = static_cast<int32_t>(epsilon.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["epsilon"] = static_cast<int64_t>(epsilon.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["epsilon"] = static_cast<int16_t>(epsilon.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["epsilon"] = static_cast<int8_t>(epsilon.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["epsilon"] = static_cast<uint16_t>(epsilon.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["epsilon"] = static_cast<uint8_t>(epsilon.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["epsilon"] = static_cast<bool>(epsilon.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["epsilon"] = static_cast<float>(epsilon.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["epsilon"] = static_cast<double>(epsilon.to<complex128>());
            break;
        default:
            attrs["epsilon"] = "";
            break;
      }
       attrs["lazy_mode"] = lazy_mode;
       attrs["min_row_size_to_use_multithread"] = min_row_size_to_use_multithread;
       attrs["multi_precision"] = multi_precision;
       attrs["use_global_beta_pow"] = use_global_beta_pow;
       phi::RecordOpInfoSupplement("adam_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);
    auto backup5 = ProcessStrideBackup(&kernel_out_5);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("adam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment1 = *input_moment1;

    auto origin_input_moment2 = *input_moment2;

    auto origin_input_beta1_pow = *input_beta1_pow;

    auto origin_input_beta2_pow = *input_beta2_pow;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

    phi::AdamInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(input_master_param), MakeMetaTensor(input_skip_update), beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, const phi::Scalar&, const phi::Scalar&, bool, int64_t, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("adam_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment1, origin_input_moment2, origin_input_beta1_pow, origin_input_beta2_pow, input_master_param, input_skip_update, phi::Scalar(beta1), phi::Scalar(beta2), phi::Scalar(epsilon), lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);
    TransStride(dev_ctx, kernel_out_5, backup5);

    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() && learning_rate.is_dense_tensor() && moment1.is_dense_tensor() && moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() && beta2_pow.is_dense_tensor() && (!master_param || master_param->is_dense_tensor()) && (!skip_update || skip_update->is_dense_tensor())) {

    VLOG(6) << "adam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "adam_dense_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adam_", kernel_data_type);
    }
    VLOG(6) << "adam_dense_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {});

    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<phi::DDim> skip_update_record_shapes;
       if(input_skip_update){
         skip_update_record_shapes.push_back((*input_skip_update).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"moment1", {
       (*input_moment1).dims()}},
       {"moment2", {
       (*input_moment2).dims()}},
       {"beta1_pow", {
       (*input_beta1_pow).dims()}},
       {"beta2_pow", {
       (*input_beta2_pow).dims()}},
       {"master_param", master_param_record_shapes},
       {"skip_update",
       skip_update_record_shapes}};
       phi::AttributeMap attrs;
      switch (beta1.dtype()) {
        case DataType::FLOAT32:
            attrs["beta1"] = static_cast<float>(beta1.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["beta1"] = static_cast<double>(beta1.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["beta1"] = static_cast<float>(beta1.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["beta1"] = static_cast<float>(beta1.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["beta1"] = static_cast<int32_t>(beta1.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["beta1"] = static_cast<int64_t>(beta1.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["beta1"] = static_cast<int16_t>(beta1.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["beta1"] = static_cast<int8_t>(beta1.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["beta1"] = static_cast<uint16_t>(beta1.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["beta1"] = static_cast<uint8_t>(beta1.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["beta1"] = static_cast<bool>(beta1.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["beta1"] = static_cast<float>(beta1.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["beta1"] = static_cast<double>(beta1.to<complex128>());
            break;
        default:
            attrs["beta1"] = "";
            break;
      }
      switch (beta2.dtype()) {
        case DataType::FLOAT32:
            attrs["beta2"] = static_cast<float>(beta2.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["beta2"] = static_cast<double>(beta2.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["beta2"] = static_cast<float>(beta2.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["beta2"] = static_cast<float>(beta2.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["beta2"] = static_cast<int32_t>(beta2.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["beta2"] = static_cast<int64_t>(beta2.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["beta2"] = static_cast<int16_t>(beta2.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["beta2"] = static_cast<int8_t>(beta2.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["beta2"] = static_cast<uint16_t>(beta2.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["beta2"] = static_cast<uint8_t>(beta2.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["beta2"] = static_cast<bool>(beta2.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["beta2"] = static_cast<float>(beta2.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["beta2"] = static_cast<double>(beta2.to<complex128>());
            break;
        default:
            attrs["beta2"] = "";
            break;
      }
      switch (epsilon.dtype()) {
        case DataType::FLOAT32:
            attrs["epsilon"] = static_cast<float>(epsilon.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["epsilon"] = static_cast<double>(epsilon.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["epsilon"] = static_cast<float>(epsilon.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["epsilon"] = static_cast<float>(epsilon.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["epsilon"] = static_cast<int32_t>(epsilon.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["epsilon"] = static_cast<int64_t>(epsilon.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["epsilon"] = static_cast<int16_t>(epsilon.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["epsilon"] = static_cast<int8_t>(epsilon.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["epsilon"] = static_cast<uint16_t>(epsilon.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["epsilon"] = static_cast<uint8_t>(epsilon.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["epsilon"] = static_cast<bool>(epsilon.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["epsilon"] = static_cast<float>(epsilon.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["epsilon"] = static_cast<double>(epsilon.to<complex128>());
            break;
        default:
            attrs["epsilon"] = "";
            break;
      }
       attrs["lazy_mode"] = lazy_mode;
       attrs["min_row_size_to_use_multithread"] = min_row_size_to_use_multithread;
       attrs["multi_precision"] = multi_precision;
       attrs["use_global_beta_pow"] = use_global_beta_pow;
       phi::RecordOpInfoSupplement("adam_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);
    auto backup5 = ProcessStrideBackup(&kernel_out_5);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("adam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment1 = *input_moment1;

    auto origin_input_moment2 = *input_moment2;

    auto origin_input_beta1_pow = *input_beta1_pow;

    auto origin_input_beta2_pow = *input_beta2_pow;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

    phi::AdamInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(input_master_param), MakeMetaTensor(input_skip_update), beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, const phi::Scalar&, const phi::Scalar&, bool, int64_t, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("adam_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment1, origin_input_moment2, origin_input_beta1_pow, origin_input_beta2_pow, input_master_param, input_skip_update, phi::Scalar(beta1), phi::Scalar(beta2), phi::Scalar(epsilon), lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);
    TransStride(dev_ctx, kernel_out_5, backup5);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (adam_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> adamax_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& moment, Tensor& inf_norm, const Tensor& beta1_pow, paddle::optional<Tensor>& master_param, float beta1, float beta2, float epsilon, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, moment, inf_norm, beta1_pow, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adamax_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adamax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adamax_", kernel_data_type);
  }
  VLOG(6) << "adamax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_inf_norm = PrepareData(inf_norm, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"moment", {
     (*input_moment).dims()}},
     {"inf_norm", {
     (*input_inf_norm).dims()}},
     {"beta1_pow", {
     (*input_beta1_pow).dims()}},
     {"master_param",
     master_param_record_shapes}};
     phi::AttributeMap attrs;
     attrs["beta1"] = beta1;
     attrs["beta2"] = beta2;
     attrs["epsilon"] = epsilon;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("adamax_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment, inf_norm, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("adamax_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_moment = *input_moment;

  auto origin_input_inf_norm = *input_inf_norm;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::AdamaxInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment), MakeMetaTensor(origin_input_inf_norm), MakeMetaTensor(*input_beta1_pow), MakeMetaTensor(input_master_param), beta1, beta2, epsilon, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("adamax_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment, origin_input_inf_norm, *input_beta1_pow, input_master_param, beta1, beta2, epsilon, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> adamw_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& moment1, Tensor& moment2, Tensor& beta1_pow, Tensor& beta2_pow, paddle::optional<Tensor>& master_param, const paddle::optional<Tensor>& skip_update, const Scalar& beta1, const Scalar& beta2, const Scalar& epsilon, float lr_ratio, float coeff, bool with_decay, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adamw_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adamw", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("adamw_", kernel_data_type);
  }
  VLOG(6) << "adamw kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<phi::DDim> skip_update_record_shapes;
     if(input_skip_update){
       skip_update_record_shapes.push_back((*input_skip_update).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"moment1", {
     (*input_moment1).dims()}},
     {"moment2", {
     (*input_moment2).dims()}},
     {"beta1_pow", {
     (*input_beta1_pow).dims()}},
     {"beta2_pow", {
     (*input_beta2_pow).dims()}},
     {"master_param", master_param_record_shapes},
     {"skip_update",
     skip_update_record_shapes}};
     phi::AttributeMap attrs;
    switch (beta1.dtype()) {
      case DataType::FLOAT32:
          attrs["beta1"] = static_cast<float>(beta1.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta1"] = static_cast<double>(beta1.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta1"] = static_cast<int32_t>(beta1.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta1"] = static_cast<int64_t>(beta1.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta1"] = static_cast<int16_t>(beta1.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta1"] = static_cast<int8_t>(beta1.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta1"] = static_cast<uint16_t>(beta1.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta1"] = static_cast<uint8_t>(beta1.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta1"] = static_cast<bool>(beta1.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta1"] = static_cast<float>(beta1.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta1"] = static_cast<double>(beta1.to<complex128>());
          break;
      default:
          attrs["beta1"] = "";
          break;
    }
    switch (beta2.dtype()) {
      case DataType::FLOAT32:
          attrs["beta2"] = static_cast<float>(beta2.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta2"] = static_cast<double>(beta2.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta2"] = static_cast<int32_t>(beta2.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta2"] = static_cast<int64_t>(beta2.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta2"] = static_cast<int16_t>(beta2.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta2"] = static_cast<int8_t>(beta2.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta2"] = static_cast<uint16_t>(beta2.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta2"] = static_cast<uint8_t>(beta2.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta2"] = static_cast<bool>(beta2.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta2"] = static_cast<float>(beta2.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta2"] = static_cast<double>(beta2.to<complex128>());
          break;
      default:
          attrs["beta2"] = "";
          break;
    }
    switch (epsilon.dtype()) {
      case DataType::FLOAT32:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["epsilon"] = static_cast<double>(epsilon.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["epsilon"] = static_cast<int32_t>(epsilon.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["epsilon"] = static_cast<int64_t>(epsilon.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["epsilon"] = static_cast<int16_t>(epsilon.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["epsilon"] = static_cast<int8_t>(epsilon.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["epsilon"] = static_cast<uint16_t>(epsilon.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["epsilon"] = static_cast<uint8_t>(epsilon.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["epsilon"] = static_cast<bool>(epsilon.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["epsilon"] = static_cast<float>(epsilon.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["epsilon"] = static_cast<double>(epsilon.to<complex128>());
          break;
      default:
          attrs["epsilon"] = "";
          break;
    }
     attrs["lr_ratio"] = lr_ratio;
     attrs["coeff"] = coeff;
     attrs["with_decay"] = with_decay;
     attrs["lazy_mode"] = lazy_mode;
     attrs["min_row_size_to_use_multithread"] = min_row_size_to_use_multithread;
     attrs["multi_precision"] = multi_precision;
     attrs["use_global_beta_pow"] = use_global_beta_pow;
     phi::RecordOpInfoSupplement("adamw_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("adamw_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_moment1 = *input_moment1;

  auto origin_input_moment2 = *input_moment2;

  auto origin_input_beta1_pow = *input_beta1_pow;

  auto origin_input_beta2_pow = *input_beta2_pow;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::AdamwInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(input_master_param), MakeMetaTensor(input_skip_update), beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, const phi::Scalar&, const phi::Scalar&, float, float, bool, bool, int64_t, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("adamw_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment1, origin_input_moment2, origin_input_beta1_pow, origin_input_beta2_pow, input_master_param, input_skip_update, phi::Scalar(beta1), phi::Scalar(beta2), phi::Scalar(epsilon), lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);

  return api_output;
}

PADDLE_API Tensor add_position_encoding(const Tensor& x, float alpha, float beta) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add_position_encoding API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add_position_encoding", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add_position_encoding", kernel_data_type);
  }
  VLOG(6) << "add_position_encoding kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     attrs["beta"] = beta;
     phi::RecordOpInfoSupplement("add_position_encoding", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add_position_encoding infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AddPositionEncodingInferMeta(MakeMetaTensor(*input_x), alpha, beta, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add_position_encoding compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, alpha, beta, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor addmm(const Tensor& input, const Tensor& x, const Tensor& y, float beta, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "addmm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "addmm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("addmm", kernel_data_type);
  }
  VLOG(6) << "addmm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["beta"] = beta;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("addmm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("addmm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AddmmInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), beta, alpha, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("addmm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_x, *input_y, beta, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& addmm_(Tensor& input, const Tensor& x, const Tensor& y, float beta, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "addmm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "addmm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("addmm", kernel_data_type);
  }
  VLOG(6) << "addmm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["beta"] = beta;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("addmm", input_shapes, attrs);
  }

  Tensor& api_output = input;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("addmm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_input = *input_input;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AddmmInferMeta(MakeMetaTensor(origin_input_input), MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), beta, alpha, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("addmm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_input, *input_x, *input_y, beta, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor affine_channel(const Tensor& x, const Tensor& scale, const Tensor& bias, const std::string& data_layout) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "affine_channel API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "affine_channel", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("affine_channel", kernel_data_type);
  }
  VLOG(6) << "affine_channel kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["data_layout"] = data_layout;
     phi::RecordOpInfoSupplement("affine_channel", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("affine_channel infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AffineChannelInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), data_layout, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("affine_channel compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, data_layout, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& affine_channel_(Tensor& x, const Tensor& scale, const Tensor& bias, const std::string& data_layout) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "affine_channel API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "affine_channel", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("affine_channel", kernel_data_type);
  }
  VLOG(6) << "affine_channel kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["data_layout"] = data_layout;
     phi::RecordOpInfoSupplement("affine_channel", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("affine_channel infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AffineChannelInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), data_layout, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("affine_channel compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_scale, *input_bias, data_layout, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor affine_grid(const Tensor& input, const IntArray& output_shape, bool align_corners) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "affine_grid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "affine_grid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("affine_grid", kernel_data_type);
  }
  VLOG(6) << "affine_grid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_shape"] = output_shape.GetData();
     attrs["align_corners"] = align_corners;
     phi::RecordOpInfoSupplement("affine_grid", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("affine_grid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AffineGridInferMeta(MakeMetaTensor(*input_input), output_shape, align_corners, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("affine_grid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, phi::IntArray(output_shape), align_corners, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor all(const Tensor& x, const std::vector<int64_t>& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "all API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "all", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("all", kernel_data_type);
  }
  VLOG(6) << "all kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("all", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("all infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("all compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor all_gather(const Tensor& x, int ring_id, int nranks) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "all_gather API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "all_gather", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("all_gather", kernel_data_type);
  }
  VLOG(6) << "all_gather kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["nranks"] = nranks;
     phi::RecordOpInfoSupplement("all_gather", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("all_gather infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllGatherInferMeta(MakeMetaTensor(*input_x), nranks, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("all_gather compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, nranks, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor allclose(const Tensor& x, const Tensor& y, const Scalar& rtol, const Scalar& atol, bool equal_nan) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "allclose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "allclose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("allclose", kernel_data_type);
  }
  VLOG(6) << "allclose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
    switch (rtol.dtype()) {
      case DataType::FLOAT32:
          attrs["rtol"] = static_cast<float>(rtol.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["rtol"] = static_cast<double>(rtol.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["rtol"] = static_cast<float>(rtol.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["rtol"] = static_cast<float>(rtol.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["rtol"] = static_cast<int32_t>(rtol.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["rtol"] = static_cast<int64_t>(rtol.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["rtol"] = static_cast<int16_t>(rtol.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["rtol"] = static_cast<int8_t>(rtol.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["rtol"] = static_cast<uint16_t>(rtol.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["rtol"] = static_cast<uint8_t>(rtol.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["rtol"] = static_cast<bool>(rtol.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["rtol"] = static_cast<float>(rtol.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["rtol"] = static_cast<double>(rtol.to<complex128>());
          break;
      default:
          attrs["rtol"] = "";
          break;
    }
    switch (atol.dtype()) {
      case DataType::FLOAT32:
          attrs["atol"] = static_cast<float>(atol.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["atol"] = static_cast<double>(atol.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["atol"] = static_cast<float>(atol.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["atol"] = static_cast<float>(atol.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["atol"] = static_cast<int32_t>(atol.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["atol"] = static_cast<int64_t>(atol.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["atol"] = static_cast<int16_t>(atol.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["atol"] = static_cast<int8_t>(atol.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["atol"] = static_cast<uint16_t>(atol.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["atol"] = static_cast<uint8_t>(atol.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["atol"] = static_cast<bool>(atol.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["atol"] = static_cast<float>(atol.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["atol"] = static_cast<double>(atol.to<complex128>());
          break;
      default:
          attrs["atol"] = "";
          break;
    }
     attrs["equal_nan"] = equal_nan;
     phi::RecordOpInfoSupplement("allclose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("allclose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllValueCompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("allclose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, phi::Scalar(rtol), phi::Scalar(atol), equal_nan, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor amax(const Tensor& x, const std::vector<int64_t>& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "amax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("amax", kernel_data_type);
  }
  VLOG(6) << "amax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("amax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("amax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("amax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor amin(const Tensor& x, const std::vector<int64_t>& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "amin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("amin", kernel_data_type);
  }
  VLOG(6) << "amin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("amin", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("amin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("amin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor angle(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "angle API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "angle", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("angle", kernel_data_type);
  }
  VLOG(6) << "angle kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("angle", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("angle infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("angle compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor any(const Tensor& x, const std::vector<int64_t>& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "any API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "any", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("any", kernel_data_type);
  }
  VLOG(6) << "any kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("any", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("any infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("any compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor apply_per_channel_scale(const Tensor& x, const Tensor& scales) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scales);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "apply_per_channel_scale API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "apply_per_channel_scale", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("apply_per_channel_scale", kernel_data_type);
  }
  VLOG(6) << "apply_per_channel_scale kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scales = PrepareData(scales, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scales", {
     (*input_scales).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("apply_per_channel_scale", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("apply_per_channel_scale infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ApplyPerChannelScaleInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scales), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("apply_per_channel_scale compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scales, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor argmax(const Tensor& x, const Scalar& axis, bool keepdims, bool flatten, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "argmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("argmax", kernel_data_type);
  }
  VLOG(6) << "argmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     attrs["keepdims"] = keepdims;
     attrs["flatten"] = flatten;
     phi::RecordOpInfoSupplement("argmax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("argmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ArgMinMaxInferMeta(MakeMetaTensor(*input_x), axis, keepdims, flatten, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("argmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(axis), keepdims, flatten, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor argmin(const Tensor& x, const Scalar& axis, bool keepdims, bool flatten, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argmin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "argmin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("argmin", kernel_data_type);
  }
  VLOG(6) << "argmin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     attrs["keepdims"] = keepdims;
     attrs["flatten"] = flatten;
     phi::RecordOpInfoSupplement("argmin", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("argmin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ArgMinMaxInferMeta(MakeMetaTensor(*input_x), axis, keepdims, flatten, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("argmin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(axis), keepdims, flatten, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> argsort(const Tensor& x, int axis, bool descending, bool stable) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argsort API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "argsort", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("argsort", kernel_data_type);
  }
  VLOG(6) << "argsort kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["descending"] = descending;
     attrs["stable"] = stable;
     phi::RecordOpInfoSupplement("argsort", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("argsort infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ArgsortInferMeta(MakeMetaTensor(*input_x), axis, descending, stable, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("argsort compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, descending, stable, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor as_complex(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "as_complex API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_complex", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("as_complex", kernel_data_type);
  }
  VLOG(6) << "as_complex kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("as_complex", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("as_complex infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AsComplexInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("as_complex compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor as_real(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "as_real API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_real", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("as_real", kernel_data_type);
  }
  VLOG(6) << "as_real kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("as_real", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("as_real infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AsRealInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("as_real compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor as_strided(const Tensor& input, const std::vector<int64_t>& dims, const std::vector<int64_t>& stride, int64_t offset) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "as_strided API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_strided", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("as_strided", kernel_data_type);
  }
  VLOG(6) << "as_strided kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["dims"] = dims;
     attrs["stride"] = stride;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("as_strided", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("as_strided infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::vector<int64_t>&, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("as_strided compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, dims, stride, offset, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> asgd_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& d, Tensor& y, const Tensor& n, paddle::optional<Tensor>& master_param, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, d, y, n, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asgd_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asgd", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asgd_", kernel_data_type);
  }
  VLOG(6) << "asgd kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_d = PrepareData(d, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_n = PrepareData(n, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"d", {
     (*input_d).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"n", {
     (*input_n).dims()}},
     {"master_param",
     master_param_record_shapes}};
     phi::AttributeMap attrs;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("asgd_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, d, y, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asgd_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_d = *input_d;

  auto origin_input_y = *input_y;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::ASGDInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_d), MakeMetaTensor(origin_input_y), MakeMetaTensor(*input_n), MakeMetaTensor(input_master_param), multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asgd_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_d, origin_input_y, *input_n, input_master_param, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API Tensor asin(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asin", kernel_data_type);
  }
  VLOG(6) << "asin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asin", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& asin_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asin", kernel_data_type);
  }
  VLOG(6) << "asin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asin", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor asinh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asinh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asinh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asinh", kernel_data_type);
  }
  VLOG(6) << "asinh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asinh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asinh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asinh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& asinh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asinh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asinh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asinh", kernel_data_type);
  }
  VLOG(6) << "asinh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asinh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asinh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asinh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor& assign_out_(const Tensor& x, Tensor& output) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign_out_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign_out_", kernel_data_type);
  }
  VLOG(6) << "assign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("assign_out_", input_shapes, attrs);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign_out_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign_out_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor assign_pos(const Tensor& x, const Tensor& cum_count, const Tensor& eff_num_len) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, cum_count, eff_num_len);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign_pos API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign_pos", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign_pos", kernel_data_type);
  }
  VLOG(6) << "assign_pos kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cum_count = PrepareData(cum_count, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_eff_num_len = PrepareData(eff_num_len, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"cum_count", {
     (*input_cum_count).dims()}},
     {"eff_num_len", {
     (*input_eff_num_len).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("assign_pos", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign_pos infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AssignPosInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_cum_count), MakeMetaTensor(*input_eff_num_len), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign_pos compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_cum_count, *input_eff_num_len, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& assign_value_(Tensor& output, const std::vector<int>& shape, DataType dtype, const std::vector<phi::Scalar>& values, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, output);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign_value_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign_value", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign_value_", kernel_data_type);
  }
  VLOG(6) << "assign_value kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape;
     attrs["values"] = "";
     phi::RecordOpInfoSupplement("assign_value_", input_shapes, attrs);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign_value_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<int>&, DataType, const std::vector<phi::Scalar>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign_value_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, shape, dtype, values, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor atan(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atan", kernel_data_type);
  }
  VLOG(6) << "atan kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atan", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atan infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atan compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& atan_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atan", kernel_data_type);
  }
  VLOG(6) << "atan kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atan", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atan infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atan compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor atan2(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atan2 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan2", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atan2", kernel_data_type);
  }
  VLOG(6) << "atan2 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atan2", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atan2 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Atan2InferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atan2 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor atanh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atanh", kernel_data_type);
  }
  VLOG(6) << "atanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atanh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& atanh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atanh", kernel_data_type);
  }
  VLOG(6) << "atanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atanh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> attention_lstm(const Tensor& x, const Tensor& c0, const paddle::optional<Tensor>& h0, const Tensor& attention_weight, const paddle::optional<Tensor>& attention_bias, const paddle::optional<Tensor>& attention_scalar, const paddle::optional<Tensor>& attention_scalar_bias, const Tensor& lstm_weight, const Tensor& lstm_bias, const std::string& gate_activation, const std::string& cell_activation, const std::string& candidate_activation) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, c0, h0, attention_weight, attention_bias, attention_scalar, attention_scalar_bias, lstm_weight, lstm_bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "attention_lstm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "attention_lstm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("attention_lstm", kernel_data_type);
  }
  VLOG(6) << "attention_lstm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_c0 = PrepareData(c0, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_h0 = PrepareData(h0, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attention_weight = PrepareData(attention_weight, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attention_bias = PrepareData(attention_bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attention_scalar = PrepareData(attention_scalar, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attention_scalar_bias = PrepareData(attention_scalar_bias, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_lstm_weight = PrepareData(lstm_weight, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_lstm_bias = PrepareData(lstm_bias, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> h0_record_shapes;
     if(input_h0){
       h0_record_shapes.push_back((*input_h0).dims());
     }
     std::vector<phi::DDim> attention_bias_record_shapes;
     if(input_attention_bias){
       attention_bias_record_shapes.push_back((*input_attention_bias).dims());
     }
     std::vector<phi::DDim> attention_scalar_record_shapes;
     if(input_attention_scalar){
       attention_scalar_record_shapes.push_back((*input_attention_scalar).dims());
     }
     std::vector<phi::DDim> attention_scalar_bias_record_shapes;
     if(input_attention_scalar_bias){
       attention_scalar_bias_record_shapes.push_back((*input_attention_scalar_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"c0", {
     (*input_c0).dims()}},
     {"h0", h0_record_shapes},
     {"attention_weight", {
     (*input_attention_weight).dims()}},
     {"attention_bias", attention_bias_record_shapes},
     {"attention_scalar", attention_scalar_record_shapes},
     {"attention_scalar_bias", attention_scalar_bias_record_shapes},
     {"lstm_weight", {
     (*input_lstm_weight).dims()}},
     {"lstm_bias", {
     (*input_lstm_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["gate_activation"] = gate_activation;
     attrs["cell_activation"] = cell_activation;
     attrs["candidate_activation"] = candidate_activation;
     phi::RecordOpInfoSupplement("attention_lstm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("attention_lstm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::AttentionLstmInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_c0), MakeMetaTensor(input_h0), MakeMetaTensor(*input_attention_weight), MakeMetaTensor(input_attention_bias), MakeMetaTensor(input_attention_scalar), MakeMetaTensor(input_attention_scalar_bias), MakeMetaTensor(*input_lstm_weight), MakeMetaTensor(*input_lstm_bias), gate_activation, cell_activation, candidate_activation, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("attention_lstm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_c0, input_h0, *input_attention_weight, input_attention_bias, input_attention_scalar, input_attention_scalar_bias, *input_lstm_weight, *input_lstm_bias, gate_activation, cell_activation, candidate_activation, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> auc(const Tensor& x, const Tensor& label, const Tensor& stat_pos, const Tensor& stat_neg, const paddle::optional<Tensor>& ins_tag_weight, const std::string& curve, int num_thresholds, int slide_steps) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, stat_pos, stat_neg, ins_tag_weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "auc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "auc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("auc", kernel_data_type);
  }
  VLOG(6) << "auc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_stat_pos = PrepareData(stat_pos, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_stat_neg = PrepareData(stat_neg, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_ins_tag_weight = PrepareData(ins_tag_weight, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> ins_tag_weight_record_shapes;
     if(input_ins_tag_weight){
       ins_tag_weight_record_shapes.push_back((*input_ins_tag_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"stat_pos", {
     (*input_stat_pos).dims()}},
     {"stat_neg", {
     (*input_stat_neg).dims()}},
     {"ins_tag_weight",
     ins_tag_weight_record_shapes}};
     phi::AttributeMap attrs;
     attrs["curve"] = curve;
     attrs["num_thresholds"] = num_thresholds;
     attrs["slide_steps"] = slide_steps;
     phi::RecordOpInfoSupplement("auc", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("auc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::AucInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_label), MakeMetaTensor(*input_stat_pos), MakeMetaTensor(*input_stat_neg), MakeMetaTensor(input_ins_tag_weight), curve, num_thresholds, slide_steps, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("auc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, *input_stat_pos, *input_stat_neg, input_ins_tag_weight, curve, num_thresholds, slide_steps, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&> average_accumulates_(const Tensor& param, Tensor& in_sum_1, Tensor& in_sum_2, Tensor& in_sum_3, Tensor& in_num_accumulates, Tensor& in_old_num_accumulates, Tensor& in_num_updates, float average_window, int64_t max_average_window, int64_t min_average_window) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, in_sum_1, in_sum_2, in_sum_3, in_num_accumulates, in_old_num_accumulates, in_num_updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "average_accumulates_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "average_accumulates", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("average_accumulates_", kernel_data_type);
  }
  VLOG(6) << "average_accumulates kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_sum_1 = PrepareData(in_sum_1, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_sum_2 = PrepareData(in_sum_2, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_sum_3 = PrepareData(in_sum_3, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_num_accumulates = PrepareData(in_num_accumulates, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_old_num_accumulates = PrepareData(in_old_num_accumulates, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_num_updates = PrepareData(in_num_updates, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"in_sum_1", {
     (*input_in_sum_1).dims()}},
     {"in_sum_2", {
     (*input_in_sum_2).dims()}},
     {"in_sum_3", {
     (*input_in_sum_3).dims()}},
     {"in_num_accumulates", {
     (*input_in_num_accumulates).dims()}},
     {"in_old_num_accumulates", {
     (*input_in_old_num_accumulates).dims()}},
     {"in_num_updates", {
     (*input_in_num_updates).dims()}}};
     phi::AttributeMap attrs;
     attrs["average_window"] = average_window;
     attrs["max_average_window"] = max_average_window;
     attrs["min_average_window"] = min_average_window;
     phi::RecordOpInfoSupplement("average_accumulates_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&> api_output{in_sum_1, in_sum_2, in_sum_3, in_num_accumulates, in_old_num_accumulates, in_num_updates};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("average_accumulates_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_in_sum_1 = *input_in_sum_1;

  auto origin_input_in_sum_2 = *input_in_sum_2;

  auto origin_input_in_sum_3 = *input_in_sum_3;

  auto origin_input_in_num_accumulates = *input_in_num_accumulates;

  auto origin_input_in_old_num_accumulates = *input_in_old_num_accumulates;

  auto origin_input_in_num_updates = *input_in_num_updates;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::AverageAccumulatesInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(origin_input_in_sum_1), MakeMetaTensor(origin_input_in_sum_2), MakeMetaTensor(origin_input_in_sum_3), MakeMetaTensor(origin_input_in_num_accumulates), MakeMetaTensor(origin_input_in_old_num_accumulates), MakeMetaTensor(origin_input_in_num_updates), average_window, max_average_window, min_average_window, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int64_t, int64_t, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("average_accumulates_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_param, origin_input_in_sum_1, origin_input_in_sum_2, origin_input_in_sum_3, origin_input_in_num_accumulates, origin_input_in_old_num_accumulates, origin_input_in_num_updates, average_window, max_average_window, min_average_window, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);

  return api_output;
}

PADDLE_API Tensor batch_fc(const Tensor& input, const Tensor& w, const Tensor& bias) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, w, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "batch_fc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "batch_fc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("batch_fc", kernel_data_type);
  }
  VLOG(6) << "batch_fc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"w", {
     (*input_w).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("batch_fc", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("batch_fc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BatchFCInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_w), MakeMetaTensor(*input_bias), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("batch_fc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_w, *input_bias, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bce_loss(const Tensor& input, const Tensor& label) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bce_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bce_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bce_loss", kernel_data_type);
  }
  VLOG(6) << "bce_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bce_loss", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bce_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BCELossInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bce_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bce_loss_(Tensor& input, const Tensor& label) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bce_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bce_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bce_loss", kernel_data_type);
  }
  VLOG(6) << "bce_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bce_loss", input_shapes, attrs);
  }

  Tensor& api_output = input;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bce_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_input = *input_input;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BCELossInferMeta(MakeMetaTensor(origin_input_input), MakeMetaTensor(*input_label), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bce_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_input, *input_label, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> beam_search(const Tensor& pre_ids, const Tensor& pre_scores, const paddle::optional<Tensor>& ids, const Tensor& scores, int level, int beam_size, int end_id, bool is_accumulated) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(pre_ids);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(pre_ids, pre_scores, ids, scores);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "beam_search API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "beam_search", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("beam_search", kernel_data_type);
  }
  VLOG(6) << "beam_search kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_pre_ids = PrepareData(pre_ids, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pre_scores = PrepareData(pre_scores, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_ids = PrepareData(ids, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scores = PrepareData(scores, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> ids_record_shapes;
     if(input_ids){
       ids_record_shapes.push_back((*input_ids).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"pre_ids", {
     (*input_pre_ids).dims()}},
     {"pre_scores", {
     (*input_pre_scores).dims()}},
     {"ids", ids_record_shapes},
     {"scores", {
     (*input_scores).dims()}}};
     phi::AttributeMap attrs;
     attrs["level"] = level;
     attrs["beam_size"] = beam_size;
     attrs["end_id"] = end_id;
     attrs["is_accumulated"] = is_accumulated;
     phi::RecordOpInfoSupplement("beam_search", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("beam_search infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::BeamSearchInferMeta(MakeMetaTensor(*input_pre_ids), MakeMetaTensor(*input_pre_scores), MakeMetaTensor(input_ids), MakeMetaTensor(*input_scores), level, beam_size, end_id, is_accumulated, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, int, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("beam_search compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_pre_ids, *input_pre_scores, input_ids, *input_scores, level, beam_size, end_id, is_accumulated, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bernoulli(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bernoulli API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bernoulli", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bernoulli", kernel_data_type);
  }
  VLOG(6) << "bernoulli kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bernoulli", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bernoulli infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bernoulli compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bicubic_interp(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bicubic_interp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bicubic_interp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bicubic_interp", kernel_data_type);
  }
  VLOG(6) << "bicubic_interp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor",
     scale_tensor_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("bicubic_interp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bicubic_interp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_out_size), size_tensor_metas, MakeMetaTensor(input_scale_tensor), data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bicubic_interp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bilinear(const Tensor& x, const Tensor& y, const Tensor& weight, const paddle::optional<Tensor>& bias) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bilinear API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bilinear", kernel_data_type);
  }
  VLOG(6) << "bilinear kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bilinear", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bilinear infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BilinearInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bilinear compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, input_bias, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bilinear_interp(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bilinear_interp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear_interp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bilinear_interp", kernel_data_type);
  }
  VLOG(6) << "bilinear_interp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor",
     scale_tensor_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("bilinear_interp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bilinear_interp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_out_size), size_tensor_metas, MakeMetaTensor(input_scale_tensor), data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bilinear_interp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bincount(const Tensor& x, const paddle::optional<Tensor>& weights, const Scalar& minlength) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, weights);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bincount API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bincount", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bincount", kernel_data_type);
  }
  VLOG(6) << "bincount kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weights = PrepareData(weights, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> weights_record_shapes;
     if(input_weights){
       weights_record_shapes.push_back((*input_weights).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"weights",
     weights_record_shapes}};
     phi::AttributeMap attrs;
    switch (minlength.dtype()) {
      case DataType::FLOAT32:
          attrs["minlength"] = static_cast<float>(minlength.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["minlength"] = static_cast<double>(minlength.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["minlength"] = static_cast<float>(minlength.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["minlength"] = static_cast<float>(minlength.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["minlength"] = static_cast<int32_t>(minlength.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["minlength"] = static_cast<int64_t>(minlength.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["minlength"] = static_cast<int16_t>(minlength.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["minlength"] = static_cast<int8_t>(minlength.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["minlength"] = static_cast<uint16_t>(minlength.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["minlength"] = static_cast<uint8_t>(minlength.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["minlength"] = static_cast<bool>(minlength.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["minlength"] = static_cast<float>(minlength.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["minlength"] = static_cast<double>(minlength.to<complex128>());
          break;
      default:
          attrs["minlength"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("bincount", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bincount infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BincountInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_weights), minlength, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bincount compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_weights, phi::Scalar(minlength), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor binomial(const Tensor& count, const Tensor& prob) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(count, prob);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "binomial API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "binomial", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("binomial", kernel_data_type);
  }
  VLOG(6) << "binomial kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_count = PrepareData(count, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_prob = PrepareData(prob, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"count", {
     (*input_count).dims()}},
     {"prob", {
     (*input_prob).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("binomial", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("binomial infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BinomialInferMeta(MakeMetaTensor(*input_count), MakeMetaTensor(*input_prob), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("binomial compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_count, *input_prob, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> bipartite_match(const Tensor& dist_mat, const std::string& match_type, float dist_threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dist_mat);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(dist_mat);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bipartite_match API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bipartite_match", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bipartite_match", kernel_data_type);
  }
  VLOG(6) << "bipartite_match kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_dist_mat = PrepareData(dist_mat, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"dist_mat", {
     (*input_dist_mat).dims()}}};
     phi::AttributeMap attrs;
     attrs["match_type"] = match_type;
     attrs["dist_threshold"] = dist_threshold;
     phi::RecordOpInfoSupplement("bipartite_match", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bipartite_match infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::BipartiteMatchInferMeta(MakeMetaTensor(*input_dist_mat), match_type, dist_threshold, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bipartite_match compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_dist_mat, match_type, dist_threshold, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor bitwise_and(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_and API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_and", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_and", kernel_data_type);
  }
  VLOG(6) << "bitwise_and kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_and", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_and infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_and compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_and_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_and API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_and", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_and", kernel_data_type);
  }
  VLOG(6) << "bitwise_and kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_and", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_and infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_and compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bitwise_left_shift(const Tensor& x, const Tensor& y, bool is_arithmetic) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_left_shift API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_left_shift", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_left_shift", kernel_data_type);
  }
  VLOG(6) << "bitwise_left_shift kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_arithmetic"] = is_arithmetic;
     phi::RecordOpInfoSupplement("bitwise_left_shift", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_left_shift infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BitwiseShiftInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), is_arithmetic, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_left_shift compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, is_arithmetic, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_left_shift_(Tensor& x, const Tensor& y, bool is_arithmetic) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_left_shift API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_left_shift", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_left_shift", kernel_data_type);
  }
  VLOG(6) << "bitwise_left_shift kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_arithmetic"] = is_arithmetic;
     phi::RecordOpInfoSupplement("bitwise_left_shift", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_left_shift infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BitwiseShiftInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), is_arithmetic, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_left_shift compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, is_arithmetic, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bitwise_not(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_not API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_not", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_not", kernel_data_type);
  }
  VLOG(6) << "bitwise_not kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_not", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_not infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_not compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_not_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_not API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_not", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_not", kernel_data_type);
  }
  VLOG(6) << "bitwise_not kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_not", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_not infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_not compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bitwise_or(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_or API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_or", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_or", kernel_data_type);
  }
  VLOG(6) << "bitwise_or kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_or", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_or infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_or compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_or_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_or API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_or", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_or", kernel_data_type);
  }
  VLOG(6) << "bitwise_or kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_or", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_or infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_or compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bitwise_right_shift(const Tensor& x, const Tensor& y, bool is_arithmetic) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_right_shift API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_right_shift", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_right_shift", kernel_data_type);
  }
  VLOG(6) << "bitwise_right_shift kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_arithmetic"] = is_arithmetic;
     phi::RecordOpInfoSupplement("bitwise_right_shift", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_right_shift infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BitwiseShiftInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), is_arithmetic, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_right_shift compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, is_arithmetic, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_right_shift_(Tensor& x, const Tensor& y, bool is_arithmetic) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_right_shift API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_right_shift", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_right_shift", kernel_data_type);
  }
  VLOG(6) << "bitwise_right_shift kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_arithmetic"] = is_arithmetic;
     phi::RecordOpInfoSupplement("bitwise_right_shift", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_right_shift infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BitwiseShiftInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), is_arithmetic, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_right_shift compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, is_arithmetic, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bitwise_xor(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_xor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_xor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_xor", kernel_data_type);
  }
  VLOG(6) << "bitwise_xor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_xor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_xor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_xor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& bitwise_xor_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_xor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_xor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bitwise_xor", kernel_data_type);
  }
  VLOG(6) << "bitwise_xor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bitwise_xor", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bitwise_xor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bitwise_xor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor bmm(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bmm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bmm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bmm", kernel_data_type);
  }
  VLOG(6) << "bmm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bmm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bmm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BmmInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bmm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor box_clip(const Tensor& input, const Tensor& im_info) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, im_info);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "box_clip API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "box_clip", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("box_clip", kernel_data_type);
  }
  VLOG(6) << "box_clip kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_im_info = PrepareData(im_info, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"im_info", {
     (*input_im_info).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("box_clip", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("box_clip infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BoxClipInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_im_info), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("box_clip compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_im_info, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor box_coder(const Tensor& prior_box, const paddle::optional<Tensor>& prior_box_var, const Tensor& target_box, const std::string& code_type, bool box_normalized, int axis, const std::vector<float>& variance) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(prior_box, prior_box_var, target_box);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "box_coder API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "box_coder", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("box_coder", kernel_data_type);
  }
  VLOG(6) << "box_coder kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_prior_box = PrepareData(prior_box, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_prior_box_var = PrepareData(prior_box_var, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_target_box = PrepareData(target_box, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> prior_box_var_record_shapes;
     if(input_prior_box_var){
       prior_box_var_record_shapes.push_back((*input_prior_box_var).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"prior_box", {
     (*input_prior_box).dims()}},
     {"prior_box_var", prior_box_var_record_shapes},
     {"target_box", {
     (*input_target_box).dims()}}};
     phi::AttributeMap attrs;
     attrs["code_type"] = code_type;
     attrs["box_normalized"] = box_normalized;
     attrs["axis"] = axis;
     attrs["variance"] = variance;
     phi::RecordOpInfoSupplement("box_coder", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("box_coder infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::BoxCoderInferMeta(MakeMetaTensor(*input_prior_box), MakeMetaTensor(input_prior_box_var), MakeMetaTensor(*input_target_box), code_type, box_normalized, axis, variance, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, bool, int, const std::vector<float>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("box_coder compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_prior_box, input_prior_box_var, *input_target_box, code_type, box_normalized, axis, variance, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::vector<Tensor> broadcast_tensors(const std::vector<Tensor>& input) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "broadcast_tensors API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "broadcast_tensors", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("broadcast_tensors", kernel_data_type);
  }
  VLOG(6) << "broadcast_tensors kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input_vec = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_input(input_input_vec->size());
  for (size_t i = 0; i < input_input.size(); ++i) {
    input_input[i] = &input_input_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_input.size());
     for (size_t i = 0; i < input_input.size(); ++i) {
       ddims_vec.emplace_back((*input_input[i]).dims());
     }
     input_shapes.emplace_back("input", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("broadcast_tensors", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(input.size(), &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("broadcast_tensors infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto input_meta_vec = MakeMetaTensor(input_input);
  std::vector<const phi::MetaTensor*> input_metas(input_meta_vec.size());
  for (size_t i = 0; i < input_meta_vec.size(); ++i) {
    input_metas[i] = &input_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::BroadcastTensorsInferMeta(input_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("broadcast_tensors compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_input, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor c_allgather(const Tensor& x, int ring_id, int nranks, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allgather API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allgather", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allgather", kernel_data_type);
  }
  VLOG(6) << "c_allgather kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["nranks"] = nranks;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_allgather", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allgather infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllGatherInferMeta(MakeMetaTensor(*input_x), nranks, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allgather compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, nranks, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor c_allreduce_max(const Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_max", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_max", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_allreduce_max_(Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_max", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_max", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_allreduce_min(const Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_min API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_min", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_min", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_min kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_min", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_min infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_min compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_allreduce_min_(Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_min API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_min", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_min", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_min kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_min", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_min infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_min compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_allreduce_prod(const Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_prod API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_prod", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_prod", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_prod kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_prod", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_prod infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_prod compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_allreduce_prod_(Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_prod API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_prod", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_prod", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_prod kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_prod", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_prod infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_prod compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_allreduce_sum(const Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_sum", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_sum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_allreduce_sum_(Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_allreduce_sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_allreduce_sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_allreduce_sum", kernel_data_type);
  }
  VLOG(6) << "c_allreduce_sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_allreduce_sum", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_allreduce_sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AllReduceInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_allreduce_sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_broadcast(const Tensor& x, int ring_id, int root, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_broadcast API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_broadcast", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_broadcast", kernel_data_type);
  }
  VLOG(6) << "c_broadcast kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["root"] = root;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_broadcast", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_broadcast infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_broadcast compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, root, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_broadcast_(Tensor& x, int ring_id, int root, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_broadcast API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_broadcast", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_broadcast", kernel_data_type);
  }
  VLOG(6) << "c_broadcast kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["root"] = root;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_broadcast", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_broadcast infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_broadcast compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, root, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_concat(const Tensor& x, int rank, int nranks, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_concat API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_concat", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_concat", kernel_data_type);
  }
  VLOG(6) << "c_concat kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["rank"] = rank;
     attrs["nranks"] = nranks;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_concat", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_concat infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CConcatInferMeta(MakeMetaTensor(*input_x), nranks, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_concat compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, rank, nranks, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor c_identity(const Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_identity API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_identity", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_identity", kernel_data_type);
  }
  VLOG(6) << "c_identity kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_identity", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_identity infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CIdentityInferMeta(MakeMetaTensor(*input_x), ring_id, use_calc_stream, use_model_parallel, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_identity compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_identity_(Tensor& x, int ring_id, bool use_calc_stream, bool use_model_parallel) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_identity API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_identity", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_identity", kernel_data_type);
  }
  VLOG(6) << "c_identity kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["use_calc_stream"] = use_calc_stream;
     attrs["use_model_parallel"] = use_model_parallel;
     phi::RecordOpInfoSupplement("c_identity", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_identity infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CIdentityInferMeta(MakeMetaTensor(origin_input_x), ring_id, use_calc_stream, use_model_parallel, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_identity compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, use_calc_stream, use_model_parallel, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_reduce_sum(const Tensor& x, int ring_id, int root_id, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_reduce_sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_reduce_sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_reduce_sum", kernel_data_type);
  }
  VLOG(6) << "c_reduce_sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["root_id"] = root_id;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_reduce_sum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_reduce_sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DistReduceInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_reduce_sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, root_id, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_reduce_sum_(Tensor& x, int ring_id, int root_id, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_reduce_sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_reduce_sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_reduce_sum", kernel_data_type);
  }
  VLOG(6) << "c_reduce_sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["root_id"] = root_id;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_reduce_sum", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_reduce_sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DistReduceInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_reduce_sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, root_id, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_scatter(const Tensor& x, int ring_id, int root, int nranks, bool use_calc_stream) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_scatter API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_scatter", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_scatter", kernel_data_type);
  }
  VLOG(6) << "c_scatter kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["root"] = root;
     attrs["nranks"] = nranks;
     attrs["use_calc_stream"] = use_calc_stream;
     phi::RecordOpInfoSupplement("c_scatter", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_scatter infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CScatterInferMeta(MakeMetaTensor(*input_x), ring_id, root, nranks, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_scatter compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, root, nranks, use_calc_stream, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor c_sync_calc_stream(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_sync_calc_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_sync_calc_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_sync_calc_stream", kernel_data_type);
  }
  VLOG(6) << "c_sync_calc_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("c_sync_calc_stream", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_sync_calc_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_sync_calc_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_sync_calc_stream_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_sync_calc_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_sync_calc_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_sync_calc_stream", kernel_data_type);
  }
  VLOG(6) << "c_sync_calc_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("c_sync_calc_stream", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_sync_calc_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_sync_calc_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor c_sync_comm_stream(const Tensor& x, int ring_id) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_sync_comm_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_sync_comm_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_sync_comm_stream", kernel_data_type);
  }
  VLOG(6) << "c_sync_comm_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     phi::RecordOpInfoSupplement("c_sync_comm_stream", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_sync_comm_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_sync_comm_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, ring_id, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& c_sync_comm_stream_(Tensor& x, int ring_id) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_sync_comm_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_sync_comm_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_sync_comm_stream", kernel_data_type);
  }
  VLOG(6) << "c_sync_comm_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     phi::RecordOpInfoSupplement("c_sync_comm_stream", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_sync_comm_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_sync_comm_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, ring_id, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor calc_reduced_attn_scores(const Tensor& q, const Tensor& k, const Tensor& softmax_lse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(q);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, softmax_lse);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "calc_reduced_attn_scores API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "calc_reduced_attn_scores", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("calc_reduced_attn_scores", kernel_data_type);
  }
  VLOG(6) << "calc_reduced_attn_scores kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax_lse = PrepareData(softmax_lse, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"softmax_lse", {
     (*input_softmax_lse).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("calc_reduced_attn_scores", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("calc_reduced_attn_scores infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CalcReducedAttnScoresInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_softmax_lse), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("calc_reduced_attn_scores compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_softmax_lse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cast(const Tensor& x, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cast API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cast", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cast", kernel_data_type);
  }
  VLOG(6) << "cast kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cast", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cast infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CastInferMeta(MakeMetaTensor(*input_x), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cast compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& cast_(Tensor& x, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cast API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cast", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cast", kernel_data_type);
  }
  VLOG(6) << "cast kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cast", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cast infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CastInferMeta(MakeMetaTensor(origin_input_x), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cast compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor ceil(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "ceil API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ceil", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ceil", kernel_data_type);
  }
  VLOG(6) << "ceil kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("ceil", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("ceil infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("ceil compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& ceil_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "ceil API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ceil", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ceil", kernel_data_type);
  }
  VLOG(6) << "ceil kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("ceil", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("ceil infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("ceil compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor celu(const Tensor& x, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "celu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "celu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("celu", kernel_data_type);
  }
  VLOG(6) << "celu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("celu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("celu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("celu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor channel_shuffle(const Tensor& x, int groups, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "channel_shuffle API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "channel_shuffle", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("channel_shuffle", kernel_data_type);
  }
  VLOG(6) << "channel_shuffle kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("channel_shuffle", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("channel_shuffle infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ChannelShuffleInferMeta(MakeMetaTensor(*input_x), groups, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("channel_shuffle compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, groups, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&, Tensor> check_finite_and_unscale_(std::vector<Tensor>& x, const Tensor& scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "check_finite_and_unscale_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_finite_and_unscale", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("check_finite_and_unscale_", kernel_data_type);
  }
  VLOG(6) << "check_finite_and_unscale kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_x;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_x_vec;
  if (kernel_result.has_fallback_cpu) {
    input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_x.resize(input_x_vec->size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      input_x[i] = &input_x_vec->at(i);
    }
  }
  else {
    input_x = TensorToConstDenseTensorPtr(x);
  }
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"scale", {
     (*input_scale).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("check_finite_and_unscale_", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>&, Tensor> api_output{x, Tensor()};
  auto kernel_out_0 = SetInplaceVectorKernelOutput(x.size(), &std::get<0>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_0.size(); ++i) {
      kernel_out_0[i] = const_cast<phi::DenseTensor*>(input_x[i]);
    }
  }
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("check_finite_and_unscale_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CheckFiniteAndUnscaleInferMeta(x_metas, MakeMetaTensor(*input_scale), kernel_out_0_metas, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, std::vector<phi::DenseTensor*>, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("check_finite_and_unscale_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, *input_scale, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    for (size_t i = 0; i < x.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(x.at(i).impl().get());
      *target_ptr = *kernel_out_0.at(i);
    }
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> check_numerics(const Tensor& tensor, const std::string& op_type, const std::string& var_name, int check_nan_inf_level, int stack_height_limit, const std::string& output_dir) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "check_numerics API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_numerics", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("check_numerics", kernel_data_type);
  }
  VLOG(6) << "check_numerics kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_tensor = PrepareData(tensor, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"tensor", {
     (*input_tensor).dims()}}};
     phi::AttributeMap attrs;
     attrs["op_type"] = op_type;
     attrs["var_name"] = var_name;
     attrs["check_nan_inf_level"] = check_nan_inf_level;
     attrs["stack_height_limit"] = stack_height_limit;
     attrs["output_dir"] = output_dir;
     phi::RecordOpInfoSupplement("check_numerics", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("check_numerics infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CheckNumericsInferMeta(MakeMetaTensor(*input_tensor), op_type, var_name, check_nan_inf_level, stack_height_limit, output_dir, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, const std::string&, int, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("check_numerics compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_tensor, op_type, var_name, check_nan_inf_level, stack_height_limit, output_dir, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cholesky(const Tensor& x, bool upper) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cholesky API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cholesky", kernel_data_type);
  }
  VLOG(6) << "cholesky kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     phi::RecordOpInfoSupplement("cholesky", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cholesky infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CholeskyInferMeta(MakeMetaTensor(*input_x), upper, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cholesky compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, upper, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cholesky_solve(const Tensor& x, const Tensor& y, bool upper) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cholesky_solve API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky_solve", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cholesky_solve", kernel_data_type);
  }
  VLOG(6) << "cholesky_solve kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     phi::RecordOpInfoSupplement("cholesky_solve", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cholesky_solve infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CholeskySolveInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), upper, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cholesky_solve compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, upper, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> class_center_sample(const Tensor& label, int num_classes, int num_samples, int ring_id, int rank, int nranks, bool fix_seed, int seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(label);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "class_center_sample API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "class_center_sample", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("class_center_sample", kernel_data_type);
  }
  VLOG(6) << "class_center_sample kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["num_classes"] = num_classes;
     attrs["num_samples"] = num_samples;
     attrs["ring_id"] = ring_id;
     attrs["rank"] = rank;
     attrs["nranks"] = nranks;
     attrs["fix_seed"] = fix_seed;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("class_center_sample", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("class_center_sample infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ClassCenterSampleInferMeta(MakeMetaTensor(*input_label), num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, int, int, bool, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("class_center_sample compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor clip(const Tensor& x, const Scalar& min, const Scalar& max) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "clip API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip", kernel_data_type);
  }
  VLOG(6) << "clip kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (min.dtype()) {
      case DataType::FLOAT32:
          attrs["min"] = static_cast<float>(min.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["min"] = static_cast<double>(min.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["min"] = static_cast<float>(min.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["min"] = static_cast<float>(min.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["min"] = static_cast<bool>(min.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["min"] = static_cast<float>(min.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["min"] = static_cast<double>(min.to<complex128>());
          break;
      default:
          attrs["min"] = "";
          break;
    }
    switch (max.dtype()) {
      case DataType::FLOAT32:
          attrs["max"] = static_cast<float>(max.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max"] = static_cast<double>(max.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max"] = static_cast<float>(max.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max"] = static_cast<float>(max.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max"] = static_cast<bool>(max.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max"] = static_cast<float>(max.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max"] = static_cast<double>(max.to<complex128>());
          break;
      default:
          attrs["max"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("clip", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("clip infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("clip compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& clip_(Tensor& x, const Scalar& min, const Scalar& max) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "clip API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip", kernel_data_type);
  }
  VLOG(6) << "clip kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (min.dtype()) {
      case DataType::FLOAT32:
          attrs["min"] = static_cast<float>(min.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["min"] = static_cast<double>(min.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["min"] = static_cast<float>(min.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["min"] = static_cast<float>(min.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["min"] = static_cast<bool>(min.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["min"] = static_cast<float>(min.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["min"] = static_cast<double>(min.to<complex128>());
          break;
      default:
          attrs["min"] = "";
          break;
    }
    switch (max.dtype()) {
      case DataType::FLOAT32:
          attrs["max"] = static_cast<float>(max.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max"] = static_cast<double>(max.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max"] = static_cast<float>(max.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max"] = static_cast<float>(max.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max"] = static_cast<bool>(max.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max"] = static_cast<float>(max.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max"] = static_cast<double>(max.to<complex128>());
          break;
      default:
          attrs["max"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("clip", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("clip infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("clip compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor clip_by_norm(const Tensor& x, float max_norm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "clip_by_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "clip_by_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip_by_norm", kernel_data_type);
    }
    VLOG(6) << "clip_by_norm kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       attrs["max_norm"] = max_norm;
       phi::RecordOpInfoSupplement("clip_by_norm", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("clip_by_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ClipByNormInferMeta(MakeMetaTensor(*input_x), max_norm, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("clip_by_norm compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, max_norm, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "clip_by_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "clip_by_norm_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip_by_norm", kernel_data_type);
    }
    VLOG(6) << "clip_by_norm_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       attrs["max_norm"] = max_norm;
       phi::RecordOpInfoSupplement("clip_by_norm", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("clip_by_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ClipByNormInferMeta(MakeMetaTensor(*input_x), max_norm, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, float, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("clip_by_norm compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, max_norm, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (clip_by_norm) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<std::vector<Tensor>, Tensor> coalesce_tensor(const std::vector<Tensor>& input, DataType dtype, bool copy_data, bool set_constant, bool persist_output, float constant, bool use_align, int align_size, int size_of_dtype, const std::vector<int64_t>& concated_shapes, const std::vector<int64_t>& concated_ranks) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "coalesce_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "coalesce_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("coalesce_tensor", kernel_data_type);
  }
  VLOG(6) << "coalesce_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input_vec = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_input(input_input_vec->size());
  for (size_t i = 0; i < input_input.size(); ++i) {
    input_input[i] = &input_input_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_input.size());
     for (size_t i = 0; i < input_input.size(); ++i) {
       ddims_vec.emplace_back((*input_input[i]).dims());
     }
     input_shapes.emplace_back("input", ddims_vec);
     phi::AttributeMap attrs;
     attrs["copy_data"] = copy_data;
     attrs["set_constant"] = set_constant;
     attrs["persist_output"] = persist_output;
     attrs["constant"] = constant;
     attrs["use_align"] = use_align;
     attrs["align_size"] = align_size;
     attrs["size_of_dtype"] = size_of_dtype;
     attrs["concated_shapes"] = concated_shapes;
     attrs["concated_ranks"] = concated_ranks;
     phi::RecordOpInfoSupplement("coalesce_tensor", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(input.size(), &std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("coalesce_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto input_meta_vec = MakeMetaTensor(input_input);
  std::vector<const phi::MetaTensor*> input_metas(input_meta_vec.size());
  for (size_t i = 0; i < input_meta_vec.size(); ++i) {
    input_metas[i] = &input_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CoalesceTensorInferMeta(input_metas, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks, kernel_out_0_metas, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, DataType, bool, bool, bool, float, bool, int, int, const std::vector<int64_t>&, const std::vector<int64_t>&, std::vector<phi::DenseTensor*>, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("coalesce_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> collect_fpn_proposals(const std::vector<Tensor>& multi_level_rois, const std::vector<Tensor>& multi_level_scores, const paddle::optional<std::vector<Tensor>>& multi_level_rois_num, int post_nms_topn) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(multi_level_rois);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(multi_level_rois, multi_level_scores, multi_level_rois_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "collect_fpn_proposals API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "collect_fpn_proposals", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("collect_fpn_proposals", kernel_data_type);
  }
  VLOG(6) << "collect_fpn_proposals kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_multi_level_rois_vec = PrepareData(multi_level_rois, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_multi_level_rois(input_multi_level_rois_vec->size());
  for (size_t i = 0; i < input_multi_level_rois.size(); ++i) {
    input_multi_level_rois[i] = &input_multi_level_rois_vec->at(i);
  }
  auto input_multi_level_scores_vec = PrepareData(multi_level_scores, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_multi_level_scores(input_multi_level_scores_vec->size());
  for (size_t i = 0; i < input_multi_level_scores.size(); ++i) {
    input_multi_level_scores[i] = &input_multi_level_scores_vec->at(i);
  }
  auto input_multi_level_rois_num_vec = PrepareData(multi_level_rois_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_multi_level_rois_num;
  if (input_multi_level_rois_num_vec){
    input_multi_level_rois_num = paddle::optional<std::vector<const phi::DenseTensor*>>(input_multi_level_rois_num_vec->size());
    for (size_t i = 0; i < input_multi_level_rois_num_vec->size(); ++i) {
      input_multi_level_rois_num->at(i) = &input_multi_level_rois_num_vec->at(i);
    }
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_multi_level_rois.size());
     for (size_t i = 0; i < input_multi_level_rois.size(); ++i) {
       ddims_vec.emplace_back((*input_multi_level_rois[i]).dims());
     }
     input_shapes.emplace_back("multi_level_rois", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_multi_level_scores.size());
     for (size_t i = 0; i < input_multi_level_scores.size(); ++i) {
       ddims_vec.emplace_back((*input_multi_level_scores[i]).dims());
     }
     input_shapes.emplace_back("multi_level_scores", ddims_vec);
     ddims_vec.clear();
     if (input_multi_level_rois_num){
       ddims_vec.reserve(input_multi_level_rois_num->size());
       for (size_t i = 0; i < input_multi_level_rois_num->size(); ++i) {
         ddims_vec.emplace_back((*input_multi_level_rois_num->at(i)).dims());
       }
     }
     input_shapes.emplace_back("multi_level_rois_num", ddims_vec);
     phi::AttributeMap attrs;
     attrs["post_nms_topn"] = post_nms_topn;
     phi::RecordOpInfoSupplement("collect_fpn_proposals", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("collect_fpn_proposals infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto multi_level_rois_meta_vec = MakeMetaTensor(input_multi_level_rois);
  std::vector<const phi::MetaTensor*> multi_level_rois_metas(multi_level_rois_meta_vec.size());
  for (size_t i = 0; i < multi_level_rois_meta_vec.size(); ++i) {
    multi_level_rois_metas[i] = &multi_level_rois_meta_vec[i];
  }

  auto multi_level_scores_meta_vec = MakeMetaTensor(input_multi_level_scores);
  std::vector<const phi::MetaTensor*> multi_level_scores_metas(multi_level_scores_meta_vec.size());
  for (size_t i = 0; i < multi_level_scores_meta_vec.size(); ++i) {
    multi_level_scores_metas[i] = &multi_level_scores_meta_vec[i];
  }

  auto multi_level_rois_num_meta_vec = MakeMetaTensor(input_multi_level_rois_num);
  paddle::optional<std::vector<const phi::MetaTensor*>> multi_level_rois_num_metas(multi_level_rois_num_meta_vec.size());
  for (size_t i = 0; i < multi_level_rois_num_meta_vec.size(); ++i) {
    multi_level_rois_num_metas->at(i) = &multi_level_rois_num_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CollectFpnProposalsInferMeta(multi_level_rois_metas, multi_level_scores_metas, multi_level_rois_num_metas, post_nms_topn, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("collect_fpn_proposals compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_multi_level_rois, input_multi_level_scores, input_multi_level_rois_num, post_nms_topn, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor complex(const Tensor& real, const Tensor& imag) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(real);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(real, imag);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "complex API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "complex", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("complex", kernel_data_type);
  }
  VLOG(6) << "complex kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_real = PrepareData(real, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_imag = PrepareData(imag, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"real", {
     (*input_real).dims()}},
     {"imag", {
     (*input_imag).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("complex", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("complex infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ComplexInferMeta(MakeMetaTensor(*input_real), MakeMetaTensor(*input_imag), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("complex compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_real, *input_imag, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor concat(const std::vector<Tensor>& x, const Scalar& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "concat API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "concat", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("concat", kernel_data_type);
  }
  VLOG(6) << "concat kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("concat", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("concat infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ConcatInferMeta(x_metas, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("concat compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conj(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conj API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conj", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conj", kernel_data_type);
  }
  VLOG(6) << "conj kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("conj", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conj infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conj compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conv2d(const Tensor& input, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d", kernel_data_type);
  }
  VLOG(6) << "conv2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["dilations"] = dilations;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ConvInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), strides, paddings, padding_algorithm, dilations, groups, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, const std::vector<int>&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, strides, paddings, padding_algorithm, dilations, groups, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conv2d_transpose(const Tensor& x, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv2d_transpose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_transpose", kernel_data_type);
  }
  VLOG(6) << "conv2d_transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_transpose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_transpose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_transpose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conv2d_transpose_bias(const Tensor& x, const Tensor& filter, const Tensor& bias, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv2d_transpose_bias API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_transpose_bias", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_transpose_bias", kernel_data_type);
  }
  VLOG(6) << "conv2d_transpose_bias kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_transpose_bias", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_transpose_bias infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_transpose_bias compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_bias, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conv3d(const Tensor& input, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv3d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv3d", kernel_data_type);
  }
  VLOG(6) << "conv3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv3d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv3d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Conv3DInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv3d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor conv3d_transpose(const Tensor& x, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv3d_transpose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d_transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv3d_transpose", kernel_data_type);
  }
  VLOG(6) << "conv3d_transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv3d_transpose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv3d_transpose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ConvTransposeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv3d_transpose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor copy_to(const Tensor& x, const Place& place, bool blocking) {
  return copy_to_impl(x, place, blocking);
}
PADDLE_API Tensor copysign(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "copysign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "copysign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("copysign", kernel_data_type);
  }
  VLOG(6) << "copysign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("copysign", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("copysign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("copysign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& copysign_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "copysign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "copysign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("copysign", kernel_data_type);
  }
  VLOG(6) << "copysign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("copysign", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("copysign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("copysign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor correlation(const Tensor& input1, const Tensor& input2, int pad_size, int kernel_size, int max_displacement, int stride1, int stride2, int corr_type_multiply) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input1);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input1, input2);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "correlation API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "correlation", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("correlation", kernel_data_type);
  }
  VLOG(6) << "correlation kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input1 = PrepareData(input1, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_input2 = PrepareData(input2, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input1", {
     (*input_input1).dims()}},
     {"input2", {
     (*input_input2).dims()}}};
     phi::AttributeMap attrs;
     attrs["pad_size"] = pad_size;
     attrs["kernel_size"] = kernel_size;
     attrs["max_displacement"] = max_displacement;
     attrs["stride1"] = stride1;
     attrs["stride2"] = stride2;
     attrs["corr_type_multiply"] = corr_type_multiply;
     phi::RecordOpInfoSupplement("correlation", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("correlation infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CorrelationInferMeta(MakeMetaTensor(*input_input1), MakeMetaTensor(*input_input2), pad_size, kernel_size, max_displacement, stride1, stride2, corr_type_multiply, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("correlation compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input1, *input_input2, pad_size, kernel_size, max_displacement, stride1, stride2, corr_type_multiply, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cos(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cos API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cos", kernel_data_type);
  }
  VLOG(6) << "cos kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cos", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cos infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cos compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& cos_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cos API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cos", kernel_data_type);
  }
  VLOG(6) << "cos kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cos", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cos infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cos compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor cosh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cosh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cosh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cosh", kernel_data_type);
  }
  VLOG(6) << "cosh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cosh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cosh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cosh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& cosh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cosh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cosh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cosh", kernel_data_type);
  }
  VLOG(6) << "cosh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cosh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cosh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cosh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor crf_decoding(const Tensor& emission, const Tensor& transition, const paddle::optional<Tensor>& label, const paddle::optional<Tensor>& length) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(emission);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(emission, transition, label, length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "crf_decoding API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "crf_decoding", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("crf_decoding", kernel_data_type);
  }
  VLOG(6) << "crf_decoding kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_emission = PrepareData(emission, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_transition = PrepareData(transition, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_length = PrepareData(length, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> label_record_shapes;
     if(input_label){
       label_record_shapes.push_back((*input_label).dims());
     }
     std::vector<phi::DDim> length_record_shapes;
     if(input_length){
       length_record_shapes.push_back((*input_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"emission", {
     (*input_emission).dims()}},
     {"transition", {
     (*input_transition).dims()}},
     {"label", label_record_shapes},
     {"length",
     length_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("crf_decoding", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("crf_decoding infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CrfDecodingInferMeta(MakeMetaTensor(*input_emission), MakeMetaTensor(*input_transition), MakeMetaTensor(input_label), MakeMetaTensor(input_length), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("crf_decoding compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_emission, *input_transition, input_label, input_length, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor crop(const Tensor& x, const IntArray& shape, const IntArray& offsets) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "crop API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "crop", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("crop", kernel_data_type);
  }
  VLOG(6) << "crop kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     attrs["offsets"] = offsets.GetData();
     phi::RecordOpInfoSupplement("crop", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("crop infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CropInferMeta(MakeMetaTensor(*input_x), shape, offsets, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("crop compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shape), phi::IntArray(offsets), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cross(const Tensor& x, const Tensor& y, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cross API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cross", kernel_data_type);
  }
  VLOG(6) << "cross kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cross", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cross infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CrossInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cross compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> cross_entropy_with_softmax(const Tensor& input, const Tensor& label, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cross_entropy_with_softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross_entropy_with_softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cross_entropy_with_softmax", kernel_data_type);
  }
  VLOG(6) << "cross_entropy_with_softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["soft_label"] = soft_label;
     attrs["use_softmax"] = use_softmax;
     attrs["numeric_stable_mode"] = numeric_stable_mode;
     attrs["ignore_index"] = ignore_index;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cross_entropy_with_softmax", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cross_entropy_with_softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CrossEntropyWithSoftmaxInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cross_entropy_with_softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor> cross_entropy_with_softmax_(Tensor& input, const Tensor& label, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cross_entropy_with_softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross_entropy_with_softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cross_entropy_with_softmax", kernel_data_type);
  }
  VLOG(6) << "cross_entropy_with_softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["soft_label"] = soft_label;
     attrs["use_softmax"] = use_softmax;
     attrs["numeric_stable_mode"] = numeric_stable_mode;
     attrs["ignore_index"] = ignore_index;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cross_entropy_with_softmax", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor> api_output{input, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cross_entropy_with_softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_input = *input_input;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CrossEntropyWithSoftmaxInferMeta(MakeMetaTensor(origin_input_input), MakeMetaTensor(*input_label), soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cross_entropy_with_softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_input, *input_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> ctc_align(const Tensor& input, const paddle::optional<Tensor>& input_length, int blank, bool merge_repeated, int padding_value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, input_length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "ctc_align API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ctc_align", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ctc_align", kernel_data_type);
  }
  VLOG(6) << "ctc_align kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_input_length = PrepareData(input_length, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> input_length_record_shapes;
     if(input_input_length){
       input_length_record_shapes.push_back((*input_input_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"input_length",
     input_length_record_shapes}};
     phi::AttributeMap attrs;
     attrs["blank"] = blank;
     attrs["merge_repeated"] = merge_repeated;
     attrs["padding_value"] = padding_value;
     phi::RecordOpInfoSupplement("ctc_align", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("ctc_align infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CtcAlignInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(input_input_length), blank, merge_repeated, padding_value, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, bool, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("ctc_align compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, input_input_length, blank, merge_repeated, padding_value, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> cudnn_lstm(const Tensor& x, const Tensor& init_h, const Tensor& init_c, const paddle::optional<Tensor>& w, const paddle::optional<std::vector<Tensor>>& weight_list, const paddle::optional<Tensor>& sequence_length, float dropout_prob, bool is_bidirec, int hidden_size, int num_layers, bool is_test, int seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, init_h, init_c, w, weight_list, sequence_length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cudnn_lstm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cudnn_lstm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cudnn_lstm", kernel_data_type);
  }
  VLOG(6) << "cudnn_lstm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_init_h = PrepareData(init_h, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_init_c = PrepareData(init_c, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight_list_vec = PrepareData(weight_list, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_weight_list;
  if (input_weight_list_vec){
    input_weight_list = paddle::optional<std::vector<const phi::DenseTensor*>>(input_weight_list_vec->size());
    for (size_t i = 0; i < input_weight_list_vec->size(); ++i) {
      input_weight_list->at(i) = &input_weight_list_vec->at(i);
    }
  }
  auto input_sequence_length = PrepareData(sequence_length, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> w_record_shapes;
     if(input_w){
       w_record_shapes.push_back((*input_w).dims());
     }
     std::vector<phi::DDim> sequence_length_record_shapes;
     if(input_sequence_length){
       sequence_length_record_shapes.push_back((*input_sequence_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"init_h", {
     (*input_init_h).dims()}},
     {"init_c", {
     (*input_init_c).dims()}},
     {"w", w_record_shapes},
     {"sequence_length",
     sequence_length_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_weight_list){
       ddims_vec.reserve(input_weight_list->size());
       for (size_t i = 0; i < input_weight_list->size(); ++i) {
         ddims_vec.emplace_back((*input_weight_list->at(i)).dims());
       }
     }
     input_shapes.emplace_back("weight_list", ddims_vec);
     phi::AttributeMap attrs;
     attrs["dropout_prob"] = dropout_prob;
     attrs["is_bidirec"] = is_bidirec;
     attrs["hidden_size"] = hidden_size;
     attrs["num_layers"] = num_layers;
     attrs["is_test"] = is_test;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("cudnn_lstm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cudnn_lstm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto weight_list_meta_vec = MakeMetaTensor(input_weight_list);
  paddle::optional<std::vector<const phi::MetaTensor*>> weight_list_metas(weight_list_meta_vec.size());
  for (size_t i = 0; i < weight_list_meta_vec.size(); ++i) {
    weight_list_metas->at(i) = &weight_list_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);

  phi::CudnnLSTMInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_init_h), MakeMetaTensor(*input_init_c), MakeMetaTensor(input_w), weight_list_metas, MakeMetaTensor(input_sequence_length), dropout_prob, is_bidirec, hidden_size, num_layers, is_test, seed, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, float, bool, int, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cudnn_lstm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_init_h, *input_init_c, input_w, input_weight_list, input_sequence_length, dropout_prob, is_bidirec, hidden_size, num_layers, is_test, seed, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output), std::get<2>(api_output), std::get<4>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor> cummax(const Tensor& x, int axis, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cummax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cummax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cummax", kernel_data_type);
  }
  VLOG(6) << "cummax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cummax", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cummax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CumWithIndicesInferMeta(MakeMetaTensor(*input_x), axis, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cummax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, dtype, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> cummin(const Tensor& x, int axis, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cummin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cummin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cummin", kernel_data_type);
  }
  VLOG(6) << "cummin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cummin", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cummin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::CumWithIndicesInferMeta(MakeMetaTensor(*input_x), axis, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cummin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, dtype, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor cumprod(const Tensor& x, int dim, bool exclusive, bool reverse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumprod API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumprod", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumprod", kernel_data_type);
  }
  VLOG(6) << "cumprod kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim"] = dim;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("cumprod", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumprod infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMetaCheckAxis(MakeMetaTensor(*input_x), dim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumprod compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, dim, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& cumprod_(Tensor& x, int dim, bool exclusive, bool reverse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumprod API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumprod", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumprod", kernel_data_type);
  }
  VLOG(6) << "cumprod kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim"] = dim;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("cumprod", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumprod infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMetaCheckAxis(MakeMetaTensor(origin_input_x), dim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumprod compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, dim, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor cumsum(const Tensor& x, const Scalar& axis, bool flatten, bool exclusive, bool reverse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumsum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumsum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumsum", kernel_data_type);
  }
  VLOG(6) << "cumsum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     attrs["flatten"] = flatten;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("cumsum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumsum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CumScalarAxisInferMeta(MakeMetaTensor(*input_x), axis, flatten, exclusive, reverse, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumsum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(axis), flatten, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& cumsum_(Tensor& x, const Scalar& axis, bool flatten, bool exclusive, bool reverse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumsum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumsum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumsum", kernel_data_type);
  }
  VLOG(6) << "cumsum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     attrs["flatten"] = flatten;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("cumsum", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumsum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CumScalarAxisInferMeta(MakeMetaTensor(origin_input_x), axis, flatten, exclusive, reverse, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumsum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(axis), flatten, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor cvm(const Tensor& x, const Tensor& cvm, bool use_cvm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, cvm);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cvm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cvm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cvm", kernel_data_type);
  }
  VLOG(6) << "cvm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cvm = PrepareData(cvm, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"cvm", {
     (*input_cvm).dims()}}};
     phi::AttributeMap attrs;
     attrs["use_cvm"] = use_cvm;
     phi::RecordOpInfoSupplement("cvm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cvm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CvmInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_cvm), use_cvm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cvm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_cvm, use_cvm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor data(const std::string& name, const IntArray& shape, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "data API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "data", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("data", kernel_data_type);
  }
  VLOG(6) << "data kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["name"] = name;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("data", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("data infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DataInferMeta(name, shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::string&, const phi::IntArray&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("data compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, name, phi::IntArray(shape), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> decayed_adagrad(const Tensor& param, const Tensor& grad, const Tensor& moment, const Tensor& learning_rate, float decay, float epsilon) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, moment, learning_rate);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "decayed_adagrad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "decayed_adagrad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("decayed_adagrad", kernel_data_type);
  }
  VLOG(6) << "decayed_adagrad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"moment", {
     (*input_moment).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}}};
     phi::AttributeMap attrs;
     attrs["decay"] = decay;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("decayed_adagrad", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("decayed_adagrad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::DecayedAdagradInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_moment), MakeMetaTensor(*input_learning_rate), decay, epsilon, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("decayed_adagrad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_param, *input_grad, *input_moment, *input_learning_rate, decay, epsilon, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor decode_jpeg(const Tensor& x, const std::string& mode, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "decode_jpeg API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "decode_jpeg", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("decode_jpeg", kernel_data_type);
  }
  VLOG(6) << "decode_jpeg kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("decode_jpeg", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("decode_jpeg infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DecodeJpegInferMeta(MakeMetaTensor(*input_x), mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("decode_jpeg compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor deformable_conv(const Tensor& x, const Tensor& offset, const Tensor& filter, const paddle::optional<Tensor>& mask, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, int deformable_groups, int groups, int im2col_step) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, offset, filter, mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "deformable_conv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "deformable_conv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("deformable_conv", kernel_data_type);
  }
  VLOG(6) << "deformable_conv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_offset = PrepareData(offset, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> mask_record_shapes;
     if(input_mask){
       mask_record_shapes.push_back((*input_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"offset", {
     (*input_offset).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"mask",
     mask_record_shapes}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     attrs["deformable_groups"] = deformable_groups;
     attrs["groups"] = groups;
     attrs["im2col_step"] = im2col_step;
     phi::RecordOpInfoSupplement("deformable_conv", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("deformable_conv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DeformableConvInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_offset), MakeMetaTensor(*input_filter), MakeMetaTensor(input_mask), strides, paddings, dilations, deformable_groups, groups, im2col_step, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("deformable_conv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_offset, *input_filter, input_mask, strides, paddings, dilations, deformable_groups, groups, im2col_step, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor depend(const Tensor& x, const std::vector<Tensor>& dep) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, dep);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "depend API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depend", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depend", kernel_data_type);
  }
  VLOG(6) << "depend kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dep_vec = PrepareData(dep, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_dep(input_dep_vec->size());
  for (size_t i = 0; i < input_dep.size(); ++i) {
    input_dep[i] = &input_dep_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_dep.size());
     for (size_t i = 0; i < input_dep.size(); ++i) {
       ddims_vec.emplace_back((*input_dep[i]).dims());
     }
     input_shapes.emplace_back("dep", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("depend", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depend infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depend compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_dep, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor depthwise_conv2d(const Tensor& input, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "depthwise_conv2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depthwise_conv2d", kernel_data_type);
  }
  VLOG(6) << "depthwise_conv2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("depthwise_conv2d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depthwise_conv2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DepthwiseConvInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depthwise_conv2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor depthwise_conv2d_transpose(const Tensor& x, const Tensor& filter, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "depthwise_conv2d_transpose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d_transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depthwise_conv2d_transpose", kernel_data_type);
  }
  VLOG(6) << "depthwise_conv2d_transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("depthwise_conv2d_transpose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depthwise_conv2d_transpose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depthwise_conv2d_transpose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dequantize_abs_max(const Tensor& x, const Tensor& scale, float max_range) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dequantize_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dequantize_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dequantize_abs_max", kernel_data_type);
  }
  VLOG(6) << "dequantize_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["max_range"] = max_range;
     phi::RecordOpInfoSupplement("dequantize_abs_max", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dequantize_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DequantizeAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), max_range, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dequantize_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, max_range, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dequantize_log(const Tensor& x, const Tensor& dict) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, dict);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dequantize_log API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dequantize_log", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dequantize_log", kernel_data_type);
  }
  VLOG(6) << "dequantize_log kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dict = PrepareData(dict, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"dict", {
     (*input_dict).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("dequantize_log", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dequantize_log infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DequantizeLogInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_dict), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dequantize_log compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_dict, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor det(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "det API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "determinant", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("det", kernel_data_type);
  }
  VLOG(6) << "determinant kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("det", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("det infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("det compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> detection_map(const Tensor& detect_res, const Tensor& label, const paddle::optional<Tensor>& has_state, const paddle::optional<Tensor>& pos_count, const paddle::optional<Tensor>& true_pos, const paddle::optional<Tensor>& false_pos, int class_num, int background_label, float overlap_threshold, bool evaluate_difficult, const std::string& ap_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(detect_res);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(detect_res, label, has_state, pos_count, true_pos, false_pos);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "detection_map API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "detection_map", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("detection_map", kernel_data_type);
  }
  VLOG(6) << "detection_map kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_detect_res = PrepareData(detect_res, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_has_state = PrepareData(has_state, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pos_count = PrepareData(pos_count, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_true_pos = PrepareData(true_pos, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_false_pos = PrepareData(false_pos, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> has_state_record_shapes;
     if(input_has_state){
       has_state_record_shapes.push_back((*input_has_state).dims());
     }
     std::vector<phi::DDim> pos_count_record_shapes;
     if(input_pos_count){
       pos_count_record_shapes.push_back((*input_pos_count).dims());
     }
     std::vector<phi::DDim> true_pos_record_shapes;
     if(input_true_pos){
       true_pos_record_shapes.push_back((*input_true_pos).dims());
     }
     std::vector<phi::DDim> false_pos_record_shapes;
     if(input_false_pos){
       false_pos_record_shapes.push_back((*input_false_pos).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"detect_res", {
     (*input_detect_res).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"has_state", has_state_record_shapes},
     {"pos_count", pos_count_record_shapes},
     {"true_pos", true_pos_record_shapes},
     {"false_pos",
     false_pos_record_shapes}};
     phi::AttributeMap attrs;
     attrs["class_num"] = class_num;
     attrs["background_label"] = background_label;
     attrs["overlap_threshold"] = overlap_threshold;
     attrs["evaluate_difficult"] = evaluate_difficult;
     attrs["ap_type"] = ap_type;
     phi::RecordOpInfoSupplement("detection_map", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("detection_map infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::DetectionMapInferMeta(MakeMetaTensor(*input_detect_res), MakeMetaTensor(*input_label), MakeMetaTensor(input_has_state), MakeMetaTensor(input_pos_count), MakeMetaTensor(input_true_pos), MakeMetaTensor(input_false_pos), class_num, background_label, overlap_threshold, evaluate_difficult, ap_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, int, float, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("detection_map compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_detect_res, *input_label, input_has_state, input_pos_count, input_true_pos, input_false_pos, class_num, background_label, overlap_threshold, evaluate_difficult, ap_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> dgc(const Tensor& u, const Tensor& v, const Tensor& grad, const paddle::optional<Tensor>& param, const Tensor& current_step, const Tensor& nranks, float m, bool use_nesterov, const std::vector<float>& sparsity, float rampup_begin_step, float rampup_step, float regular_coeff, int regular_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(u, v, grad, param, current_step, nranks);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dgc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dgc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dgc", kernel_data_type);
  }
  VLOG(6) << "dgc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_u = PrepareData(u, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_current_step = PrepareData(current_step, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_nranks = PrepareData(nranks, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> param_record_shapes;
     if(input_param){
       param_record_shapes.push_back((*input_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"u", {
     (*input_u).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"param", param_record_shapes},
     {"current_step", {
     (*input_current_step).dims()}},
     {"nranks", {
     (*input_nranks).dims()}}};
     phi::AttributeMap attrs;
     attrs["m"] = m;
     attrs["use_nesterov"] = use_nesterov;
     attrs["sparsity"] = sparsity;
     attrs["rampup_begin_step"] = rampup_begin_step;
     attrs["rampup_step"] = rampup_step;
     attrs["regular_coeff"] = regular_coeff;
     attrs["regular_type"] = regular_type;
     phi::RecordOpInfoSupplement("dgc", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dgc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::DgcInferMeta(MakeMetaTensor(*input_u), MakeMetaTensor(*input_v), MakeMetaTensor(*input_grad), MakeMetaTensor(input_param), MakeMetaTensor(*input_current_step), MakeMetaTensor(*input_nranks), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, float, bool, const std::vector<float>&, float, float, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dgc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_u, *input_v, *input_grad, input_param, *input_current_step, *input_nranks, m, use_nesterov, sparsity, rampup_begin_step, rampup_step, regular_coeff, regular_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dgc_clip_by_norm(const Tensor& x, const Tensor& current_step, float max_norm, float rampup_begin_step) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, current_step);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor() && current_step.is_dense_tensor()) {

    VLOG(6) << "dgc_clip_by_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "dgc_clip_by_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dgc_clip_by_norm", kernel_data_type);
    }
    VLOG(6) << "dgc_clip_by_norm kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_current_step = PrepareData(current_step, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"current_step", {
       (*input_current_step).dims()}}};
       phi::AttributeMap attrs;
       attrs["max_norm"] = max_norm;
       attrs["rampup_begin_step"] = rampup_begin_step;
       phi::RecordOpInfoSupplement("dgc_clip_by_norm", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("dgc_clip_by_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ClipByNormInferMeta(MakeMetaTensor(*input_x), max_norm, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("dgc_clip_by_norm compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_current_step, max_norm, rampup_begin_step, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows() && current_step.is_dense_tensor()) {

    VLOG(6) << "dgc_clip_by_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "dgc_clip_by_norm_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dgc_clip_by_norm", kernel_data_type);
    }
    VLOG(6) << "dgc_clip_by_norm_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    auto input_current_step = PrepareData(current_step, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"current_step", {
       (*input_current_step).dims()}}};
       phi::AttributeMap attrs;
       attrs["max_norm"] = max_norm;
       attrs["rampup_begin_step"] = rampup_begin_step;
       phi::RecordOpInfoSupplement("dgc_clip_by_norm", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("dgc_clip_by_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ClipByNormInferMeta(MakeMetaTensor(*input_x), max_norm, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::DenseTensor&, float, float, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("dgc_clip_by_norm compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_current_step, max_norm, rampup_begin_step, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (dgc_clip_by_norm) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> dgc_momentum(const Tensor& param, const Tensor& grad, const Tensor& velocity, const Tensor& learning_rate, const paddle::optional<Tensor>& master_param, const Tensor& current_step_tensor, const Tensor& nranks_tensor, float mu, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad, float rampup_begin_step) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, velocity, learning_rate, master_param, current_step_tensor, nranks_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dgc_momentum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dgc_momentum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dgc_momentum", kernel_data_type);
  }
  VLOG(6) << "dgc_momentum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_velocity = PrepareData(velocity, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_current_step_tensor = PrepareData(current_step_tensor, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_nranks_tensor = PrepareData(nranks_tensor, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"velocity", {
     (*input_velocity).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"master_param", master_param_record_shapes},
     {"current_step_tensor", {
     (*input_current_step_tensor).dims()}},
     {"nranks_tensor", {
     (*input_nranks_tensor).dims()}}};
     phi::AttributeMap attrs;
     attrs["mu"] = mu;
     attrs["use_nesterov"] = use_nesterov;
     attrs["regularization_method"] = regularization_method;
     attrs["regularization_coeff"] = regularization_coeff;
     attrs["multi_precision"] = multi_precision;
     attrs["rescale_grad"] = rescale_grad;
     attrs["rampup_begin_step"] = rampup_begin_step;
     phi::RecordOpInfoSupplement("dgc_momentum", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dgc_momentum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::DGCMomentumInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_velocity), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), MakeMetaTensor(*input_current_step_tensor), MakeMetaTensor(*input_nranks_tensor), mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, rampup_begin_step, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, float, bool, const std::string&, float, bool, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dgc_momentum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_param, *input_grad, *input_velocity, *input_learning_rate, input_master_param, *input_current_step_tensor, *input_nranks_tensor, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, rampup_begin_step, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor diag(const Tensor& x, int offset, float padding_value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diag API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diag", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("diag", kernel_data_type);
  }
  VLOG(6) << "diag kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["padding_value"] = padding_value;
     phi::RecordOpInfoSupplement("diag", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("diag infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DiagInferMeta(MakeMetaTensor(*input_x), offset, padding_value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("diag compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, offset, padding_value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor diag_embed(const Tensor& input, int offset, int dim1, int dim2) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diag_embed API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diag_embed", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("diag_embed", kernel_data_type);
  }
  VLOG(6) << "diag_embed kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["dim1"] = dim1;
     attrs["dim2"] = dim2;
     phi::RecordOpInfoSupplement("diag_embed", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("diag_embed infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DiagEmbedInferMeta(MakeMetaTensor(*input_input), offset, dim1, dim2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("diag_embed compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, offset, dim1, dim2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor diagonal(const Tensor& x, int offset, int axis1, int axis2) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diagonal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diagonal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("diagonal", kernel_data_type);
  }
  VLOG(6) << "diagonal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["axis1"] = axis1;
     attrs["axis2"] = axis2;
     phi::RecordOpInfoSupplement("diagonal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("diagonal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DiagonalInferMeta(MakeMetaTensor(*input_x), offset, axis1, axis2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("diagonal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, offset, axis1, axis2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor digamma(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "digamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "digamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("digamma", kernel_data_type);
  }
  VLOG(6) << "digamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("digamma", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("digamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("digamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& digamma_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "digamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "digamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("digamma", kernel_data_type);
  }
  VLOG(6) << "digamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("digamma", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("digamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("digamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor dirichlet(const Tensor& alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(alpha);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dirichlet API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dirichlet", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dirichlet", kernel_data_type);
  }
  VLOG(6) << "dirichlet kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_alpha = PrepareData(alpha, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"alpha", {
     (*input_alpha).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("dirichlet", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dirichlet infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DirichletInferMeta(MakeMetaTensor(*input_alpha), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dirichlet compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor disable_check_model_nan_inf(const Tensor& x, int flag) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "disable_check_model_nan_inf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("disable_check_model_nan_inf", kernel_data_type);
  }
  VLOG(6) << "check_model_nan_inf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["flag"] = flag;
     phi::RecordOpInfoSupplement("disable_check_model_nan_inf", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("disable_check_model_nan_inf infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("disable_check_model_nan_inf compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, flag, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dist(const Tensor& x, const Tensor& y, float p) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dist API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dist", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dist", kernel_data_type);
  }
  VLOG(6) << "dist kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["p"] = p;
     phi::RecordOpInfoSupplement("dist", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dist infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DistInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), p, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dist compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, p, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dot(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dot API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dot", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dot", kernel_data_type);
  }
  VLOG(6) << "dot kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("dot", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dot infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DotInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dot compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dpsgd(const Tensor& param, const Tensor& grad, const Tensor& learning_rate, float clip, float batch_size, float sigma, int seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dpsgd API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dpsgd", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dpsgd", kernel_data_type);
  }
  VLOG(6) << "dpsgd kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}}};
     phi::AttributeMap attrs;
     attrs["clip"] = clip;
     attrs["batch_size"] = batch_size;
     attrs["sigma"] = sigma;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("dpsgd", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dpsgd infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::DpsgdInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), clip, batch_size, sigma, seed, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, float, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dpsgd compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_param, *input_grad, *input_learning_rate, clip, batch_size, sigma, seed, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor dropout(const Tensor& x, const paddle::optional<Tensor>& seed_tensor, const Scalar& p, bool is_test, const std::string& mode, int seed, bool fix_seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, seed_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dropout API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dropout", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dropout", kernel_data_type);
  }
  VLOG(6) << "dropout kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed_tensor = PrepareData(seed_tensor, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> seed_tensor_record_shapes;
     if(input_seed_tensor){
       seed_tensor_record_shapes.push_back((*input_seed_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"seed_tensor",
     seed_tensor_record_shapes}};
     phi::AttributeMap attrs;
    switch (p.dtype()) {
      case DataType::FLOAT32:
          attrs["p"] = static_cast<float>(p.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["p"] = static_cast<double>(p.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["p"] = static_cast<float>(p.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["p"] = static_cast<float>(p.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["p"] = static_cast<int32_t>(p.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["p"] = static_cast<int64_t>(p.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["p"] = static_cast<int16_t>(p.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["p"] = static_cast<int8_t>(p.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["p"] = static_cast<uint16_t>(p.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["p"] = static_cast<uint8_t>(p.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["p"] = static_cast<bool>(p.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["p"] = static_cast<float>(p.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["p"] = static_cast<double>(p.to<complex128>());
          break;
      default:
          attrs["p"] = "";
          break;
    }
     attrs["is_test"] = is_test;
     attrs["mode"] = mode;
     attrs["seed"] = seed;
     attrs["fix_seed"] = fix_seed;
     phi::RecordOpInfoSupplement("dropout", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dropout infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::DropoutInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_seed_tensor), p, is_test, mode, seed, fix_seed, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, bool, const std::string&, int, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dropout compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_seed_tensor, phi::Scalar(p), is_test, mode, seed, fix_seed, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API std::tuple<Tensor, Tensor> edit_distance(const Tensor& hyps, const Tensor& refs, const paddle::optional<Tensor>& hypslength, const paddle::optional<Tensor>& refslength, bool normalized) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(DataType::FLOAT32);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(hyps, refs, hypslength, refslength);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "edit_distance API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "edit_distance", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("edit_distance", kernel_data_type);
  }
  VLOG(6) << "edit_distance kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_hyps = PrepareData(hyps, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_refs = PrepareData(refs, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_hypslength = PrepareData(hypslength, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_refslength = PrepareData(refslength, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> hypslength_record_shapes;
     if(input_hypslength){
       hypslength_record_shapes.push_back((*input_hypslength).dims());
     }
     std::vector<phi::DDim> refslength_record_shapes;
     if(input_refslength){
       refslength_record_shapes.push_back((*input_refslength).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"hyps", {
     (*input_hyps).dims()}},
     {"refs", {
     (*input_refs).dims()}},
     {"hypslength", hypslength_record_shapes},
     {"refslength",
     refslength_record_shapes}};
     phi::AttributeMap attrs;
     attrs["normalized"] = normalized;
     phi::RecordOpInfoSupplement("edit_distance", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("edit_distance infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::EditDistanceInferMeta(MakeMetaTensor(*input_hyps), MakeMetaTensor(*input_refs), MakeMetaTensor(input_hypslength), MakeMetaTensor(input_refslength), normalized, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("edit_distance compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_hyps, *input_refs, input_hypslength, input_refslength, normalized, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eig(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eig API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eig", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eig", kernel_data_type);
  }
  VLOG(6) << "eig kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("eig", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eig infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::EigInferMeta(MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eig compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eigh(const Tensor& x, const std::string& UPLO) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eigh", kernel_data_type);
  }
  VLOG(6) << "eigh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["UPLO"] = UPLO;
     phi::RecordOpInfoSupplement("eigh", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eigh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::EighInferMeta(MakeMetaTensor(*input_x), UPLO, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eigh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, UPLO, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor eigvals(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigvals API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigvals", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eigvals", kernel_data_type);
  }
  VLOG(6) << "eigvals kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("eigvals", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eigvals infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::EigvalsInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eigvals compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eigvalsh(const Tensor& x, const std::string& uplo, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigvalsh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigvalsh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eigvalsh", kernel_data_type);
  }
  VLOG(6) << "eigvalsh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["uplo"] = uplo;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("eigvalsh", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eigvalsh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::EigvalshInferMeta(MakeMetaTensor(*input_x), uplo, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eigvalsh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, uplo, is_test, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor elu(const Tensor& x, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elu", kernel_data_type);
  }
  VLOG(6) << "elu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("elu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& elu_(Tensor& x, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elu", kernel_data_type);
  }
  VLOG(6) << "elu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("elu", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor empty(const IntArray& shape, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "empty API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "empty", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("empty", kernel_data_type);
  }
  VLOG(6) << "empty kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("empty", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("empty infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::IntArray&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("empty compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::IntArray(shape), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor empty_like(const Tensor& x, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, x);

  kernel_data_type = ParseDataTypeWithInputOrder(dtype, x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "empty_like API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "empty_like", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("empty_like", kernel_data_type);
  }
  VLOG(6) << "empty_like kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("empty_like", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("empty_like infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateLikeInferMeta(MakeMetaTensor(*input_x), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("empty_like compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor enable_check_model_nan_inf(const Tensor& x, int flag) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "enable_check_model_nan_inf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("enable_check_model_nan_inf", kernel_data_type);
  }
  VLOG(6) << "check_model_nan_inf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["flag"] = flag;
     phi::RecordOpInfoSupplement("enable_check_model_nan_inf", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("enable_check_model_nan_inf infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("enable_check_model_nan_inf compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, flag, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor equal_all(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "equal_all API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "equal_all", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("equal_all", kernel_data_type);
  }
  VLOG(6) << "equal_all kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("equal_all", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("equal_all infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareAllInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("equal_all compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor erf(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erf", kernel_data_type);
  }
  VLOG(6) << "erf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erf", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erf infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erf compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& erf_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erf", kernel_data_type);
  }
  VLOG(6) << "erf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erf", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erf infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erf compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor erfinv(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erfinv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erfinv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erfinv", kernel_data_type);
  }
  VLOG(6) << "erfinv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erfinv", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erfinv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erfinv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& erfinv_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erfinv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erfinv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erfinv", kernel_data_type);
  }
  VLOG(6) << "erfinv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erfinv", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erfinv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erfinv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor exp(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("exp", kernel_data_type);
  }
  VLOG(6) << "exp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("exp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("exp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("exp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& exp_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("exp", kernel_data_type);
  }
  VLOG(6) << "exp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("exp", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("exp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("exp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor expand(const Tensor& x, const IntArray& shape) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expand API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expand", kernel_data_type);
  }
  VLOG(6) << "expand kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("expand", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expand infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ExpandInferMeta(MakeMetaTensor(*input_x), shape, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expand compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shape), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor expand_as(const Tensor& x, const paddle::optional<Tensor>& y, const std::vector<int>& target_shape) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expand_as API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand_as", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expand_as", kernel_data_type);
  }
  VLOG(6) << "expand_as kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> y_record_shapes;
     if(input_y){
       y_record_shapes.push_back((*input_y).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y",
     y_record_shapes}};
     phi::AttributeMap attrs;
     attrs["target_shape"] = target_shape;
     phi::RecordOpInfoSupplement("expand_as", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expand_as infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ExpandAsInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_y), target_shape, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expand_as compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_y, target_shape, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor expm1(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expm1 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expm1", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expm1", kernel_data_type);
  }
  VLOG(6) << "expm1 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("expm1", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expm1 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expm1 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& expm1_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expm1 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expm1", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expm1", kernel_data_type);
  }
  VLOG(6) << "expm1 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("expm1", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expm1 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expm1 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor& exponential_(Tensor& x, float lam) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exponential_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exponential", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("exponential_", kernel_data_type);
  }
  VLOG(6) << "exponential kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["lam"] = lam;
     phi::RecordOpInfoSupplement("exponential_", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("exponential_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("exponential_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, lam, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor eye(const Scalar& num_rows, const Scalar& num_columns, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "eye API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eye", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eye", kernel_data_type);
  }
  VLOG(6) << "eye kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
    switch (num_rows.dtype()) {
      case DataType::FLOAT32:
          attrs["num_rows"] = static_cast<float>(num_rows.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["num_rows"] = static_cast<double>(num_rows.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["num_rows"] = static_cast<float>(num_rows.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["num_rows"] = static_cast<float>(num_rows.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["num_rows"] = static_cast<int32_t>(num_rows.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["num_rows"] = static_cast<int64_t>(num_rows.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["num_rows"] = static_cast<int16_t>(num_rows.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["num_rows"] = static_cast<int8_t>(num_rows.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["num_rows"] = static_cast<uint16_t>(num_rows.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["num_rows"] = static_cast<uint8_t>(num_rows.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["num_rows"] = static_cast<bool>(num_rows.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["num_rows"] = static_cast<float>(num_rows.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["num_rows"] = static_cast<double>(num_rows.to<complex128>());
          break;
      default:
          attrs["num_rows"] = "";
          break;
    }
    switch (num_columns.dtype()) {
      case DataType::FLOAT32:
          attrs["num_columns"] = static_cast<float>(num_columns.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["num_columns"] = static_cast<double>(num_columns.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["num_columns"] = static_cast<float>(num_columns.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["num_columns"] = static_cast<float>(num_columns.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["num_columns"] = static_cast<int32_t>(num_columns.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["num_columns"] = static_cast<int64_t>(num_columns.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["num_columns"] = static_cast<int16_t>(num_columns.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["num_columns"] = static_cast<int8_t>(num_columns.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["num_columns"] = static_cast<uint16_t>(num_columns.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["num_columns"] = static_cast<uint8_t>(num_columns.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["num_columns"] = static_cast<bool>(num_columns.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["num_columns"] = static_cast<float>(num_columns.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["num_columns"] = static_cast<double>(num_columns.to<complex128>());
          break;
      default:
          attrs["num_columns"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("eye", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eye infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::EyeInferMeta(num_rows, num_columns, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::Scalar&, const phi::Scalar&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eye compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::Scalar(num_rows), phi::Scalar(num_columns), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fake_channel_wise_dequantize_max_abs(const Tensor& x, const std::vector<Tensor>& scales, const std::vector<int>& quant_bits, int quant_axis, int x_num_col_dims) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scales);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_channel_wise_dequantize_max_abs API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_channel_wise_dequantize_max_abs", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_channel_wise_dequantize_max_abs", kernel_data_type);
  }
  VLOG(6) << "fake_channel_wise_dequantize_max_abs kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scales_vec = PrepareData(scales, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_scales(input_scales_vec->size());
  for (size_t i = 0; i < input_scales.size(); ++i) {
    input_scales[i] = &input_scales_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_scales.size());
     for (size_t i = 0; i < input_scales.size(); ++i) {
       ddims_vec.emplace_back((*input_scales[i]).dims());
     }
     input_shapes.emplace_back("scales", ddims_vec);
     phi::AttributeMap attrs;
     attrs["quant_bits"] = quant_bits;
     attrs["quant_axis"] = quant_axis;
     attrs["x_num_col_dims"] = x_num_col_dims;
     phi::RecordOpInfoSupplement("fake_channel_wise_dequantize_max_abs", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_channel_wise_dequantize_max_abs infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto scales_meta_vec = MakeMetaTensor(input_scales);
  std::vector<const phi::MetaTensor*> scales_metas(scales_meta_vec.size());
  for (size_t i = 0; i < scales_meta_vec.size(); ++i) {
    scales_metas[i] = &scales_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FakeChannelWiseDequantizeMaxAbsInferMeta(MakeMetaTensor(*input_x), scales_metas, quant_bits, quant_axis, x_num_col_dims, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<int>&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_channel_wise_dequantize_max_abs compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scales, quant_bits, quant_axis, x_num_col_dims, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fake_channel_wise_quantize_abs_max(const Tensor& x, int bit_length, int round_type, int quant_axis, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_channel_wise_quantize_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_channel_wise_quantize_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_channel_wise_quantize_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_channel_wise_quantize_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["bit_length"] = bit_length;
     attrs["round_type"] = round_type;
     attrs["quant_axis"] = quant_axis;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("fake_channel_wise_quantize_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_channel_wise_quantize_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FakeChannelWiseQuantizeAbsMaxInferMeta(MakeMetaTensor(*input_x), bit_length, round_type, quant_axis, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_channel_wise_quantize_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, bit_length, round_type, quant_axis, is_test, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fake_channel_wise_quantize_dequantize_abs_max(const Tensor& x, int bit_length, int round_type, int quant_axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_channel_wise_quantize_dequantize_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_channel_wise_quantize_dequantize_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_channel_wise_quantize_dequantize_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_channel_wise_quantize_dequantize_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["bit_length"] = bit_length;
     attrs["round_type"] = round_type;
     attrs["quant_axis"] = quant_axis;
     phi::RecordOpInfoSupplement("fake_channel_wise_quantize_dequantize_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_channel_wise_quantize_dequantize_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FakeChannelWiseQuantizeDequantizeAbsMaxInferMeta(MakeMetaTensor(*input_x), bit_length, round_type, quant_axis, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_channel_wise_quantize_dequantize_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, bit_length, round_type, quant_axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fake_dequantize_max_abs(const Tensor& x, const Tensor& scale, float max_range) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_dequantize_max_abs API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_dequantize_max_abs", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_dequantize_max_abs", kernel_data_type);
  }
  VLOG(6) << "fake_dequantize_max_abs kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["max_range"] = max_range;
     phi::RecordOpInfoSupplement("fake_dequantize_max_abs", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_dequantize_max_abs infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FakeDequantizeMaxAbsInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), max_range, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_dequantize_max_abs compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, max_range, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fake_quantize_abs_max(const Tensor& x, int bit_length, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["bit_length"] = bit_length;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FakeQuantizeAbsMaxInferMeta(MakeMetaTensor(*input_x), bit_length, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, bit_length, round_type, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fake_quantize_dequantize_abs_max(const Tensor& x, int bit_length, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_dequantize_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_dequantize_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_dequantize_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_dequantize_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["bit_length"] = bit_length;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_dequantize_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_dequantize_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FakeQuantizeAbsMaxInferMeta(MakeMetaTensor(*input_x), bit_length, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_dequantize_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, bit_length, round_type, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> fake_quantize_dequantize_moving_average_abs_max(const Tensor& x, const Tensor& in_scale, const paddle::optional<Tensor>& in_accum, const paddle::optional<Tensor>& in_state, float moving_rate, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, in_accum, in_state);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_dequantize_moving_average_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_dequantize_moving_average_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_dequantize_moving_average_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_dequantize_moving_average_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_accum = PrepareData(in_accum, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_state = PrepareData(in_state, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> in_accum_record_shapes;
     if(input_in_accum){
       in_accum_record_shapes.push_back((*input_in_accum).dims());
     }
     std::vector<phi::DDim> in_state_record_shapes;
     if(input_in_state){
       in_state_record_shapes.push_back((*input_in_state).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"in_accum", in_accum_record_shapes},
     {"in_state",
     in_state_record_shapes}};
     phi::AttributeMap attrs;
     attrs["moving_rate"] = moving_rate;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_dequantize_moving_average_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_dequantize_moving_average_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FakeQuantOrWithDequantMovingAverageAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_in_scale), MakeMetaTensor(input_in_accum), MakeMetaTensor(input_in_state), moving_rate, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_dequantize_moving_average_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_in_scale, input_in_accum, input_in_state, moving_rate, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, Tensor, Tensor> fake_quantize_dequantize_moving_average_abs_max_(const Tensor& x, Tensor& in_scale, const paddle::optional<Tensor>& in_accum, const paddle::optional<Tensor>& in_state, float moving_rate, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, in_accum, in_state);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_dequantize_moving_average_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_dequantize_moving_average_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_dequantize_moving_average_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_dequantize_moving_average_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_accum = PrepareData(in_accum, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_state = PrepareData(in_state, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> in_accum_record_shapes;
     if(input_in_accum){
       in_accum_record_shapes.push_back((*input_in_accum).dims());
     }
     std::vector<phi::DDim> in_state_record_shapes;
     if(input_in_state){
       in_state_record_shapes.push_back((*input_in_state).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"in_accum", in_accum_record_shapes},
     {"in_state",
     in_state_record_shapes}};
     phi::AttributeMap attrs;
     attrs["moving_rate"] = moving_rate;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_dequantize_moving_average_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor&, Tensor, Tensor> api_output{Tensor(), in_scale, Tensor(), Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_dequantize_moving_average_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_in_scale = *input_in_scale;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FakeQuantOrWithDequantMovingAverageAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(origin_input_in_scale), MakeMetaTensor(input_in_accum), MakeMetaTensor(input_in_state), moving_rate, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_dequantize_moving_average_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, origin_input_in_scale, input_in_accum, input_in_state, moving_rate, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> fake_quantize_moving_average_abs_max(const Tensor& x, const Tensor& in_scale, const paddle::optional<Tensor>& in_accum, const paddle::optional<Tensor>& in_state, float moving_rate, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, in_accum, in_state);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_moving_average_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_moving_average_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_moving_average_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_moving_average_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_accum = PrepareData(in_accum, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_state = PrepareData(in_state, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> in_accum_record_shapes;
     if(input_in_accum){
       in_accum_record_shapes.push_back((*input_in_accum).dims());
     }
     std::vector<phi::DDim> in_state_record_shapes;
     if(input_in_state){
       in_state_record_shapes.push_back((*input_in_state).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"in_accum", in_accum_record_shapes},
     {"in_state",
     in_state_record_shapes}};
     phi::AttributeMap attrs;
     attrs["moving_rate"] = moving_rate;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_moving_average_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_moving_average_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FakeQuantOrWithDequantMovingAverageAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_in_scale), MakeMetaTensor(input_in_accum), MakeMetaTensor(input_in_state), moving_rate, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_moving_average_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_in_scale, input_in_accum, input_in_state, moving_rate, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, Tensor, Tensor> fake_quantize_moving_average_abs_max_(const Tensor& x, Tensor& in_scale, const paddle::optional<Tensor>& in_accum, const paddle::optional<Tensor>& in_state, float moving_rate, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, in_accum, in_state);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_moving_average_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_moving_average_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_moving_average_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_moving_average_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_accum = PrepareData(in_accum, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_state = PrepareData(in_state, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> in_accum_record_shapes;
     if(input_in_accum){
       in_accum_record_shapes.push_back((*input_in_accum).dims());
     }
     std::vector<phi::DDim> in_state_record_shapes;
     if(input_in_state){
       in_state_record_shapes.push_back((*input_in_state).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"in_accum", in_accum_record_shapes},
     {"in_state",
     in_state_record_shapes}};
     phi::AttributeMap attrs;
     attrs["moving_rate"] = moving_rate;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_moving_average_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor&, Tensor, Tensor> api_output{Tensor(), in_scale, Tensor(), Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_moving_average_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_in_scale = *input_in_scale;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FakeQuantOrWithDequantMovingAverageAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(origin_input_in_scale), MakeMetaTensor(input_in_accum), MakeMetaTensor(input_in_state), moving_rate, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_moving_average_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, origin_input_in_scale, input_in_accum, input_in_state, moving_rate, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> fake_quantize_range_abs_max(const Tensor& x, const Tensor& in_scale, const paddle::optional<Tensor>& iter, int window_size, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, iter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_range_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_range_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_range_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_range_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_iter = PrepareData(iter, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> iter_record_shapes;
     if(input_iter){
       iter_record_shapes.push_back((*input_iter).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"iter",
     iter_record_shapes}};
     phi::AttributeMap attrs;
     attrs["window_size"] = window_size;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_range_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_range_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FakeQuantizeRangeAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_in_scale), MakeMetaTensor(input_iter), window_size, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_range_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_in_scale, input_iter, window_size, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, Tensor> fake_quantize_range_abs_max_(const Tensor& x, Tensor& in_scale, const paddle::optional<Tensor>& iter, int window_size, int bit_length, bool is_test, int round_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, in_scale, iter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fake_quantize_range_abs_max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fake_quantize_range_abs_max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fake_quantize_range_abs_max", kernel_data_type);
  }
  VLOG(6) << "fake_quantize_range_abs_max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_scale = PrepareData(in_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_iter = PrepareData(iter, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> iter_record_shapes;
     if(input_iter){
       iter_record_shapes.push_back((*input_iter).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"in_scale", {
     (*input_in_scale).dims()}},
     {"iter",
     iter_record_shapes}};
     phi::AttributeMap attrs;
     attrs["window_size"] = window_size;
     attrs["bit_length"] = bit_length;
     attrs["is_test"] = is_test;
     attrs["round_type"] = round_type;
     phi::RecordOpInfoSupplement("fake_quantize_range_abs_max", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor&, Tensor> api_output{Tensor(), in_scale, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fake_quantize_range_abs_max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_in_scale = *input_in_scale;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FakeQuantizeRangeAbsMaxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(origin_input_in_scale), MakeMetaTensor(input_iter), window_size, bit_length, is_test, round_type, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fake_quantize_range_abs_max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, origin_input_in_scale, input_iter, window_size, bit_length, is_test, round_type, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);

  return api_output;
}

PADDLE_API Tensor fft_c2c(const Tensor& x, const std::vector<int64_t>& axes, const std::string& normalization, bool forward) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_c2c API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2c", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_c2c", kernel_data_type);
  }
  VLOG(6) << "fft_c2c kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     phi::RecordOpInfoSupplement("fft_c2c", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_c2c infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FFTC2CInferMeta(MakeMetaTensor(*input_x), axes, normalization, forward, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_c2c compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axes, normalization, forward, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fft_c2r(const Tensor& x, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, int64_t last_dim_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_c2r API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2r", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_c2r", kernel_data_type);
  }
  VLOG(6) << "fft_c2r kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     attrs["last_dim_size"] = last_dim_size;
     phi::RecordOpInfoSupplement("fft_c2r", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_c2r infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FFTC2RInferMeta(MakeMetaTensor(*input_x), axes, normalization, forward, last_dim_size, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_c2r compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axes, normalization, forward, last_dim_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fft_r2c(const Tensor& x, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, bool onesided) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_r2c API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_r2c", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_r2c", kernel_data_type);
  }
  VLOG(6) << "fft_r2c kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     attrs["onesided"] = onesided;
     phi::RecordOpInfoSupplement("fft_r2c", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_r2c infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FFTR2CInferMeta(MakeMetaTensor(*input_x), axes, normalization, forward, onesided, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_r2c compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axes, normalization, forward, onesided, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fill(const Tensor& x, const Scalar& value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill", kernel_data_type);
  }
  VLOG(6) << "fill kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("fill", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& fill_(Tensor& x, const Scalar& value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill", kernel_data_type);
  }
  VLOG(6) << "fill kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("fill", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor fill_diagonal(const Tensor& x, float value, int offset, bool wrap) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["value"] = value;
     attrs["offset"] = offset;
     attrs["wrap"] = wrap;
     phi::RecordOpInfoSupplement("fill_diagonal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalInferMeta(MakeMetaTensor(*input_x), value, offset, wrap, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, value, offset, wrap, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& fill_diagonal_(Tensor& x, float value, int offset, bool wrap) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["value"] = value;
     attrs["offset"] = offset;
     attrs["wrap"] = wrap;
     phi::RecordOpInfoSupplement("fill_diagonal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalInferMeta(MakeMetaTensor(origin_input_x), value, offset, wrap, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, value, offset, wrap, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor fill_diagonal_tensor(const Tensor& x, const Tensor& y, int64_t offset, int dim1, int dim2) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal_tensor", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["dim1"] = dim1;
     attrs["dim2"] = dim2;
     phi::RecordOpInfoSupplement("fill_diagonal_tensor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalTensorInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), offset, dim1, dim2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, offset, dim1, dim2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& fill_diagonal_tensor_(Tensor& x, const Tensor& y, int64_t offset, int dim1, int dim2) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal_tensor", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["dim1"] = dim1;
     attrs["dim2"] = dim2;
     phi::RecordOpInfoSupplement("fill_diagonal_tensor", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalTensorInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), offset, dim1, dim2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, offset, dim1, dim2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> flash_attn(const Tensor& q, const Tensor& k, const Tensor& v, const paddle::optional<Tensor>& fixed_seed_offset, const paddle::optional<Tensor>& attn_mask, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(q);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, fixed_seed_offset, attn_mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flash_attn API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn", kernel_data_type);
  }
  VLOG(6) << "flash_attn kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fixed_seed_offset = PrepareData(fixed_seed_offset, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fixed_seed_offset_record_shapes;
     if(input_fixed_seed_offset){
       fixed_seed_offset_record_shapes.push_back((*input_fixed_seed_offset).dims());
     }
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"fixed_seed_offset", fixed_seed_offset_record_shapes},
     {"attn_mask",
     attn_mask_record_shapes}};
     phi::AttributeMap attrs;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["return_softmax"] = return_softmax;
     attrs["is_test"] = is_test;
     attrs["rng_name"] = rng_name;
     phi::RecordOpInfoSupplement("flash_attn", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FlashAttnInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, bool, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, input_fixed_seed_offset, input_attn_mask, dropout, causal, return_softmax, is_test, rng_name, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> flash_attn_qkvpacked(const Tensor& qkv, const paddle::optional<Tensor>& fixed_seed_offset, const paddle::optional<Tensor>& attn_mask, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(qkv);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(qkv, fixed_seed_offset, attn_mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flash_attn_qkvpacked API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_qkvpacked", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_qkvpacked", kernel_data_type);
  }
  VLOG(6) << "flash_attn_qkvpacked kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_qkv = PrepareData(qkv, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fixed_seed_offset = PrepareData(fixed_seed_offset, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fixed_seed_offset_record_shapes;
     if(input_fixed_seed_offset){
       fixed_seed_offset_record_shapes.push_back((*input_fixed_seed_offset).dims());
     }
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"qkv", {
     (*input_qkv).dims()}},
     {"fixed_seed_offset", fixed_seed_offset_record_shapes},
     {"attn_mask",
     attn_mask_record_shapes}};
     phi::AttributeMap attrs;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["return_softmax"] = return_softmax;
     attrs["is_test"] = is_test;
     attrs["rng_name"] = rng_name;
     phi::RecordOpInfoSupplement("flash_attn_qkvpacked", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_qkvpacked infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FlashAttnQKVPackedInferMeta(MakeMetaTensor(*input_qkv), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, bool, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_qkvpacked compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_qkv, input_fixed_seed_offset, input_attn_mask, dropout, causal, return_softmax, is_test, rng_name, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> flash_attn_unpadded(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& cu_seqlens_q, const Tensor& cu_seqlens_k, const paddle::optional<Tensor>& fixed_seed_offset, const paddle::optional<Tensor>& attn_mask, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(q);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, attn_mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flash_attn_unpadded API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_unpadded", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_unpadded", kernel_data_type);
  }
  VLOG(6) << "flash_attn_unpadded kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_q = PrepareData(cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_k = PrepareData(cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fixed_seed_offset = PrepareData(fixed_seed_offset, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fixed_seed_offset_record_shapes;
     if(input_fixed_seed_offset){
       fixed_seed_offset_record_shapes.push_back((*input_fixed_seed_offset).dims());
     }
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"cu_seqlens_q", {
     (*input_cu_seqlens_q).dims()}},
     {"cu_seqlens_k", {
     (*input_cu_seqlens_k).dims()}},
     {"fixed_seed_offset", fixed_seed_offset_record_shapes},
     {"attn_mask",
     attn_mask_record_shapes}};
     phi::AttributeMap attrs;
     attrs["max_seqlen_q"] = max_seqlen_q;
     attrs["max_seqlen_k"] = max_seqlen_k;
     attrs["scale"] = scale;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["return_softmax"] = return_softmax;
     attrs["is_test"] = is_test;
     attrs["rng_name"] = rng_name;
     phi::RecordOpInfoSupplement("flash_attn_unpadded", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_unpadded infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FlashAttnInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int64_t, int64_t, float, float, bool, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_unpadded compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_cu_seqlens_q, *input_cu_seqlens_k, input_fixed_seed_offset, input_attn_mask, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, is_test, rng_name, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor> flash_attn_varlen_qkvpacked(const Tensor& qkv, const Tensor& cu_seqlens_q, const Tensor& cu_seqlens_k, const paddle::optional<Tensor>& fixed_seed_offset, const paddle::optional<Tensor>& attn_mask, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name, bool varlen_padded) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(qkv);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(qkv, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, attn_mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flash_attn_varlen_qkvpacked API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_varlen_qkvpacked", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_varlen_qkvpacked", kernel_data_type);
  }
  VLOG(6) << "flash_attn_varlen_qkvpacked kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_qkv = PrepareData(qkv, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_q = PrepareData(cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_k = PrepareData(cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fixed_seed_offset = PrepareData(fixed_seed_offset, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fixed_seed_offset_record_shapes;
     if(input_fixed_seed_offset){
       fixed_seed_offset_record_shapes.push_back((*input_fixed_seed_offset).dims());
     }
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"qkv", {
     (*input_qkv).dims()}},
     {"cu_seqlens_q", {
     (*input_cu_seqlens_q).dims()}},
     {"cu_seqlens_k", {
     (*input_cu_seqlens_k).dims()}},
     {"fixed_seed_offset", fixed_seed_offset_record_shapes},
     {"attn_mask",
     attn_mask_record_shapes}};
     phi::AttributeMap attrs;
     attrs["max_seqlen_q"] = max_seqlen_q;
     attrs["max_seqlen_k"] = max_seqlen_k;
     attrs["scale"] = scale;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["return_softmax"] = return_softmax;
     attrs["is_test"] = is_test;
     attrs["rng_name"] = rng_name;
     attrs["varlen_padded"] = varlen_padded;
     phi::RecordOpInfoSupplement("flash_attn_varlen_qkvpacked", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_varlen_qkvpacked infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FlashAttnQKVPackedInferMeta(MakeMetaTensor(*input_qkv), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int64_t, int64_t, float, float, bool, bool, bool, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_varlen_qkvpacked compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_qkv, *input_cu_seqlens_q, *input_cu_seqlens_k, input_fixed_seed_offset, input_attn_mask, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, is_test, rng_name, varlen_padded, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> flash_attn_with_sparse_mask(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& attn_mask_start_row_indices, const paddle::optional<Tensor>& fixed_seed_offset, float dropout, bool causal, int attn_mask_start_row, bool return_softmax, bool is_test, const std::string& rng_name) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(q);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, attn_mask_start_row_indices, fixed_seed_offset);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flash_attn_with_sparse_mask API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_with_sparse_mask", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_with_sparse_mask", kernel_data_type);
  }
  VLOG(6) << "flash_attn_with_sparse_mask kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask_start_row_indices = PrepareData(attn_mask_start_row_indices, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fixed_seed_offset = PrepareData(fixed_seed_offset, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fixed_seed_offset_record_shapes;
     if(input_fixed_seed_offset){
       fixed_seed_offset_record_shapes.push_back((*input_fixed_seed_offset).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"attn_mask_start_row_indices", {
     (*input_attn_mask_start_row_indices).dims()}},
     {"fixed_seed_offset",
     fixed_seed_offset_record_shapes}};
     phi::AttributeMap attrs;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["attn_mask_start_row"] = attn_mask_start_row;
     attrs["return_softmax"] = return_softmax;
     attrs["is_test"] = is_test;
     attrs["rng_name"] = rng_name;
     phi::RecordOpInfoSupplement("flash_attn_with_sparse_mask", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_with_sparse_mask infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::FlashAttnInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, bool, int, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_with_sparse_mask compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_attn_mask_start_row_indices, input_fixed_seed_offset, dropout, causal, attn_mask_start_row, return_softmax, is_test, rng_name, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor flatten(const Tensor& x, int start_axis, int stop_axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flatten API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flatten", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flatten", kernel_data_type);
  }
  VLOG(6) << "flatten kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["start_axis"] = start_axis;
     attrs["stop_axis"] = stop_axis;
     phi::RecordOpInfoSupplement("flatten", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  kernel_out->ShareBufferWith(*input_x);
  kernel_out->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flatten infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FlattenInferMeta(MakeMetaTensor(*input_x), start_axis, stop_axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flatten compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, start_axis, stop_axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

    phi::DenseTensor * x_remap = static_cast<phi::DenseTensor*>(x.impl().get());
    x_remap->ShareBufferWith(*kernel_out);
    kernel_out->ShareInplaceVersionCounterWith(*x_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& flatten_(Tensor& x, int start_axis, int stop_axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flatten API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flatten", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flatten", kernel_data_type);
  }
  VLOG(6) << "flatten kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["start_axis"] = start_axis;
     attrs["stop_axis"] = stop_axis;
     phi::RecordOpInfoSupplement("flatten", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flatten infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FlattenInferMeta(MakeMetaTensor(origin_input_x), start_axis, stop_axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flatten compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, start_axis, stop_axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor flip(const Tensor& x, const std::vector<int>& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flip API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flip", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flip", kernel_data_type);
  }
  VLOG(6) << "flip kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("flip", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flip infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FlipInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flip compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor floor(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("floor", kernel_data_type);
  }
  VLOG(6) << "floor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("floor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("floor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("floor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& floor_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("floor", kernel_data_type);
  }
  VLOG(6) << "floor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("floor", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("floor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("floor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor fmax(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fmax", kernel_data_type);
  }
  VLOG(6) << "fmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fmax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fmin(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fmin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fmin", kernel_data_type);
  }
  VLOG(6) << "fmin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fmin", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fmin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fmin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fold(const Tensor& x, const std::vector<int>& output_sizes, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fold API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fold", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fold", kernel_data_type);
  }
  VLOG(6) << "fold kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_sizes"] = output_sizes;
     attrs["kernel_sizes"] = kernel_sizes;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     phi::RecordOpInfoSupplement("fold", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fold infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FoldInferMeta(MakeMetaTensor(*input_x), output_sizes, kernel_sizes, strides, paddings, dilations, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fold compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, output_sizes, kernel_sizes, strides, paddings, dilations, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fractional_max_pool2d(const Tensor& x, const std::vector<int>& output_size, const std::vector<int>& kernel_size, float random_u, bool return_mask) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fractional_max_pool2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fractional_max_pool2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fractional_max_pool2d", kernel_data_type);
  }
  VLOG(6) << "fractional_max_pool2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_size"] = output_size;
     attrs["kernel_size"] = kernel_size;
     attrs["random_u"] = random_u;
     attrs["return_mask"] = return_mask;
     phi::RecordOpInfoSupplement("fractional_max_pool2d", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fractional_max_pool2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FractionalMaxPoolInferMeta(MakeMetaTensor(*input_x), output_size, kernel_size, random_u, return_mask, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fractional_max_pool2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, output_size, kernel_size, random_u, return_mask, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fractional_max_pool3d(const Tensor& x, const std::vector<int>& output_size, const std::vector<int>& kernel_size, float random_u, bool return_mask) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fractional_max_pool3d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fractional_max_pool3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fractional_max_pool3d", kernel_data_type);
  }
  VLOG(6) << "fractional_max_pool3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_size"] = output_size;
     attrs["kernel_size"] = kernel_size;
     attrs["random_u"] = random_u;
     attrs["return_mask"] = return_mask;
     phi::RecordOpInfoSupplement("fractional_max_pool3d", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fractional_max_pool3d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FractionalMaxPoolInferMeta(MakeMetaTensor(*input_x), output_size, kernel_size, random_u, return_mask, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fractional_max_pool3d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, output_size, kernel_size, random_u, return_mask, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor frame(const Tensor& x, int frame_length, int hop_length, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "frame API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frame", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("frame", kernel_data_type);
  }
  VLOG(6) << "frame kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["frame_length"] = frame_length;
     attrs["hop_length"] = hop_length;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("frame", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("frame infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FrameInferMeta(MakeMetaTensor(*input_x), frame_length, hop_length, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("frame compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, frame_length, hop_length, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor frobenius_norm(const Tensor& x, const IntArray& axis, bool keep_dim, bool reduce_all) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "frobenius_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frobenius_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("frobenius_norm", kernel_data_type);
  }
  VLOG(6) << "frobenius_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keep_dim"] = keep_dim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("frobenius_norm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("frobenius_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceIntArrayAxisInferMetaBase(MakeMetaTensor(*input_x), axis, keep_dim, reduce_all, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("frobenius_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keep_dim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> ftrl(const Tensor& param, const Tensor& squared_accumulator, const Tensor& linear_accumulator, const Tensor& grad, const Tensor& learning_rate, float l1, float l2, float lr_power) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, squared_accumulator, linear_accumulator, grad, learning_rate);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && squared_accumulator.is_dense_tensor() && linear_accumulator.is_dense_tensor() && grad.is_dense_tensor() && learning_rate.is_dense_tensor()) {

    VLOG(6) << "ftrl API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "ftrl", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ftrl", kernel_data_type);
    }
    VLOG(6) << "ftrl kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_squared_accumulator = PrepareData(squared_accumulator, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_linear_accumulator = PrepareData(linear_accumulator, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"squared_accumulator", {
       (*input_squared_accumulator).dims()}},
       {"linear_accumulator", {
       (*input_linear_accumulator).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}}};
       phi::AttributeMap attrs;
       attrs["l1"] = l1;
       attrs["l2"] = l2;
       attrs["lr_power"] = lr_power;
       phi::RecordOpInfoSupplement("ftrl", input_shapes, attrs);
    }

    std::tuple<Tensor, Tensor, Tensor> api_output;
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("ftrl infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::FtrlInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(*input_squared_accumulator), MakeMetaTensor(*input_linear_accumulator), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), l1, l2, lr_power, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("ftrl compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_param, *input_squared_accumulator, *input_linear_accumulator, *input_grad, *input_learning_rate, l1, l2, lr_power, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (param.is_dense_tensor() && squared_accumulator.is_dense_tensor() && linear_accumulator.is_dense_tensor() && grad.is_selected_rows() && learning_rate.is_dense_tensor()) {

    VLOG(6) << "ftrl API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "ftrl_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ftrl", kernel_data_type);
    }
    VLOG(6) << "ftrl_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_squared_accumulator = PrepareData(squared_accumulator, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_linear_accumulator = PrepareData(linear_accumulator, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {});

    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"squared_accumulator", {
       (*input_squared_accumulator).dims()}},
       {"linear_accumulator", {
       (*input_linear_accumulator).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}}};
       phi::AttributeMap attrs;
       attrs["l1"] = l1;
       attrs["l2"] = l2;
       attrs["lr_power"] = lr_power;
       phi::RecordOpInfoSupplement("ftrl", input_shapes, attrs);
    }

    std::tuple<Tensor, Tensor, Tensor> api_output;
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("ftrl infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::FtrlInferMeta(MakeMetaTensor(*input_param), MakeMetaTensor(*input_squared_accumulator), MakeMetaTensor(*input_linear_accumulator), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), l1, l2, lr_power, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, float, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("ftrl compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_param, *input_squared_accumulator, *input_linear_accumulator, *input_grad, *input_learning_rate, l1, l2, lr_power, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (ftrl) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor full(const IntArray& shape, const Scalar& value, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "full API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full", kernel_data_type);
  }
  VLOG(6) << "full kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("full", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::IntArray&, const phi::Scalar&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::IntArray(shape), phi::Scalar(value), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& full_(Tensor& output, const IntArray& shape, const Scalar& value, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full_", kernel_data_type);
  }
  VLOG(6) << "full kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("full_", input_shapes, attrs);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::IntArray&, const phi::Scalar&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::IntArray(shape), phi::Scalar(value), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor full_batch_size_like(const Tensor& input, const std::vector<int>& shape, DataType dtype, const Scalar& value, int input_dim_idx, int output_dim_idx, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_batch_size_like API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_batch_size_like", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full_batch_size_like", kernel_data_type);
  }
  VLOG(6) << "full_batch_size_like kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape;
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     attrs["input_dim_idx"] = input_dim_idx;
     attrs["output_dim_idx"] = output_dim_idx;
     phi::RecordOpInfoSupplement("full_batch_size_like", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full_batch_size_like infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FullBatchSizeLikeInferMeta(MakeMetaTensor(*input_input), shape, value, dtype, input_dim_idx, output_dim_idx, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::Scalar&, DataType, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full_batch_size_like compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, shape, phi::Scalar(value), dtype, input_dim_idx, output_dim_idx, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor full_int_array(const std::vector<int64_t>& value, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "full_int_array API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_int_array", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full_int_array", kernel_data_type);
  }
  VLOG(6) << "full_int_array kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["value"] = value;
     phi::RecordOpInfoSupplement("full_int_array", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full_int_array infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateVecShapeInferMeta(value, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<int64_t>&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full_int_array compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, value, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor full_like(const Tensor& x, const Scalar& value, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, x);

  kernel_data_type = ParseDataTypeWithInputOrder(dtype, x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_like API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_like", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full_like", kernel_data_type);
  }
  VLOG(6) << "full_like kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("full_like", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full_like infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CreateLikeInferMeta(MakeMetaTensor(*input_x), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full_like compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(value), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor full_with_tensor(const Tensor& value, const IntArray& shape, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_with_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_with_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("full_with_tensor", kernel_data_type);
  }
  VLOG(6) << "full_with_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"value", {
     (*input_value).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("full_with_tensor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("full_with_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FullWithTensorInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("full_with_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_value, phi::IntArray(shape), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> fused_batch_norm_act(const Tensor& x, const Tensor& scale, const Tensor& bias, const Tensor& mean, const Tensor& variance, float momentum, float epsilon, const std::string& act_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, mean, variance);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_batch_norm_act API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_batch_norm_act", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_batch_norm_act", kernel_data_type);
  }
  VLOG(6) << "fused_batch_norm_act kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["act_type"] = act_type;
     phi::RecordOpInfoSupplement("fused_batch_norm_act", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
      kernel_out_1->ShareBufferWith(*input_mean);
      kernel_out_1->ShareInplaceVersionCounterWith(*input_mean);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
      kernel_out_2->ShareBufferWith(*input_variance);
      kernel_out_2->ShareInplaceVersionCounterWith(*input_variance);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_batch_norm_act infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::FusedBatchNormActInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), MakeMetaTensor(*input_mean), MakeMetaTensor(*input_variance), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_batch_norm_act compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_mean, *input_variance, momentum, epsilon, act_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    phi::DenseTensor * mean_remap = static_cast<phi::DenseTensor*>(mean.impl().get());
    mean_remap->ShareBufferWith(*kernel_out_1);
    kernel_out_1->ShareInplaceVersionCounterWith(*mean_remap);

    phi::DenseTensor * variance_remap = static_cast<phi::DenseTensor*>(variance.impl().get());
    variance_remap->ShareBufferWith(*kernel_out_2);
    kernel_out_2->ShareInplaceVersionCounterWith(*variance_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> fused_bn_add_activation(const Tensor& x, const Tensor& z, const Tensor& scale, const Tensor& bias, const Tensor& mean, const Tensor& variance, float momentum, float epsilon, const std::string& act_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, z, scale, bias, mean, variance);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_bn_add_activation API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_bn_add_activation", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_bn_add_activation", kernel_data_type);
  }
  VLOG(6) << "fused_bn_add_activation kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_z = PrepareData(z, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"z", {
     (*input_z).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["act_type"] = act_type;
     phi::RecordOpInfoSupplement("fused_bn_add_activation", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
      kernel_out_1->ShareBufferWith(*input_mean);
      kernel_out_1->ShareInplaceVersionCounterWith(*input_mean);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
      kernel_out_2->ShareBufferWith(*input_variance);
      kernel_out_2->ShareInplaceVersionCounterWith(*input_variance);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_bn_add_activation infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::FusedBatchNormActInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), MakeMetaTensor(*input_mean), MakeMetaTensor(*input_variance), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_bn_add_activation compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_z, *input_scale, *input_bias, *input_mean, *input_variance, momentum, epsilon, act_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    phi::DenseTensor * mean_remap = static_cast<phi::DenseTensor*>(mean.impl().get());
    mean_remap->ShareBufferWith(*kernel_out_1);
    kernel_out_1->ShareInplaceVersionCounterWith(*mean_remap);

    phi::DenseTensor * variance_remap = static_cast<phi::DenseTensor*>(variance.impl().get());
    variance_remap->ShareBufferWith(*kernel_out_2);
    kernel_out_2->ShareInplaceVersionCounterWith(*variance_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>, Tensor> fused_multi_transformer(const Tensor& x, const std::vector<Tensor>& ln_scales, const std::vector<Tensor>& ln_biases, const std::vector<Tensor>& qkv_weights, const paddle::optional<std::vector<Tensor>>& qkv_biases, const paddle::optional<std::vector<Tensor>>& cache_kvs, const paddle::optional<std::vector<Tensor>>& pre_caches, const paddle::optional<Tensor>& rotary_tensor, const paddle::optional<Tensor>& beam_offset, const paddle::optional<Tensor>& time_step, const paddle::optional<Tensor>& seq_lengths, const paddle::optional<Tensor>& src_mask, const std::vector<Tensor>& out_linear_weights, const paddle::optional<std::vector<Tensor>>& out_linear_biases, const std::vector<Tensor>& ffn_ln_scales, const std::vector<Tensor>& ffn_ln_biases, const std::vector<Tensor>& ffn1_weights, const paddle::optional<std::vector<Tensor>>& ffn1_biases, const std::vector<Tensor>& ffn2_weights, const paddle::optional<std::vector<Tensor>>& ffn2_biases, bool pre_layer_norm, float epsilon, float residual_alpha, float dropout_rate, int rotary_emb_dims, bool is_test, const std::string& dropout_implementation, const std::string& act_method, bool trans_qkvw, int ring_id, const std::string& norm_type, bool use_neox_rotary_style, int gqa_group_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, ln_scales, ln_biases, qkv_weights, qkv_biases, cache_kvs, pre_caches, rotary_tensor, beam_offset, time_step, seq_lengths, src_mask, out_linear_weights, out_linear_biases, ffn_ln_scales, ffn_ln_biases, ffn1_weights, ffn1_biases, ffn2_weights, ffn2_biases);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_multi_transformer API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_multi_transformer", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_multi_transformer", kernel_data_type);
  }
  VLOG(6) << "fused_multi_transformer kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_ln_scales_vec = PrepareData(ln_scales, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ln_scales(input_ln_scales_vec->size());
  for (size_t i = 0; i < input_ln_scales.size(); ++i) {
    input_ln_scales[i] = &input_ln_scales_vec->at(i);
  }
  auto input_ln_biases_vec = PrepareData(ln_biases, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ln_biases(input_ln_biases_vec->size());
  for (size_t i = 0; i < input_ln_biases.size(); ++i) {
    input_ln_biases[i] = &input_ln_biases_vec->at(i);
  }
  auto input_qkv_weights_vec = PrepareData(qkv_weights, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_qkv_weights(input_qkv_weights_vec->size());
  for (size_t i = 0; i < input_qkv_weights.size(); ++i) {
    input_qkv_weights[i] = &input_qkv_weights_vec->at(i);
  }
  auto input_qkv_biases_vec = PrepareData(qkv_biases, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_qkv_biases;
  if (input_qkv_biases_vec){
    input_qkv_biases = paddle::optional<std::vector<const phi::DenseTensor*>>(input_qkv_biases_vec->size());
    for (size_t i = 0; i < input_qkv_biases_vec->size(); ++i) {
      input_qkv_biases->at(i) = &input_qkv_biases_vec->at(i);
    }
  }
  auto input_cache_kvs_vec = PrepareData(cache_kvs, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_cache_kvs;
  if (input_cache_kvs_vec){
    input_cache_kvs = paddle::optional<std::vector<const phi::DenseTensor*>>(input_cache_kvs_vec->size());
    for (size_t i = 0; i < input_cache_kvs_vec->size(); ++i) {
      input_cache_kvs->at(i) = &input_cache_kvs_vec->at(i);
    }
  }
  auto input_pre_caches_vec = PrepareData(pre_caches, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_pre_caches;
  if (input_pre_caches_vec){
    input_pre_caches = paddle::optional<std::vector<const phi::DenseTensor*>>(input_pre_caches_vec->size());
    for (size_t i = 0; i < input_pre_caches_vec->size(); ++i) {
      input_pre_caches->at(i) = &input_pre_caches_vec->at(i);
    }
  }
  auto input_rotary_tensor = PrepareData(rotary_tensor, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beam_offset = PrepareData(beam_offset, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_time_step = PrepareData(time_step, GetKernelInputArgDef(kernel.InputAt(9), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seq_lengths = PrepareData(seq_lengths, GetKernelInputArgDef(kernel.InputAt(10), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_mask = PrepareData(src_mask, GetKernelInputArgDef(kernel.InputAt(11), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_linear_weights_vec = PrepareData(out_linear_weights, GetKernelInputArgDef(kernel.InputAt(12), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_out_linear_weights(input_out_linear_weights_vec->size());
  for (size_t i = 0; i < input_out_linear_weights.size(); ++i) {
    input_out_linear_weights[i] = &input_out_linear_weights_vec->at(i);
  }
  auto input_out_linear_biases_vec = PrepareData(out_linear_biases, GetKernelInputArgDef(kernel.InputAt(13), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_out_linear_biases;
  if (input_out_linear_biases_vec){
    input_out_linear_biases = paddle::optional<std::vector<const phi::DenseTensor*>>(input_out_linear_biases_vec->size());
    for (size_t i = 0; i < input_out_linear_biases_vec->size(); ++i) {
      input_out_linear_biases->at(i) = &input_out_linear_biases_vec->at(i);
    }
  }
  auto input_ffn_ln_scales_vec = PrepareData(ffn_ln_scales, GetKernelInputArgDef(kernel.InputAt(14), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ffn_ln_scales(input_ffn_ln_scales_vec->size());
  for (size_t i = 0; i < input_ffn_ln_scales.size(); ++i) {
    input_ffn_ln_scales[i] = &input_ffn_ln_scales_vec->at(i);
  }
  auto input_ffn_ln_biases_vec = PrepareData(ffn_ln_biases, GetKernelInputArgDef(kernel.InputAt(15), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ffn_ln_biases(input_ffn_ln_biases_vec->size());
  for (size_t i = 0; i < input_ffn_ln_biases.size(); ++i) {
    input_ffn_ln_biases[i] = &input_ffn_ln_biases_vec->at(i);
  }
  auto input_ffn1_weights_vec = PrepareData(ffn1_weights, GetKernelInputArgDef(kernel.InputAt(16), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ffn1_weights(input_ffn1_weights_vec->size());
  for (size_t i = 0; i < input_ffn1_weights.size(); ++i) {
    input_ffn1_weights[i] = &input_ffn1_weights_vec->at(i);
  }
  auto input_ffn1_biases_vec = PrepareData(ffn1_biases, GetKernelInputArgDef(kernel.InputAt(17), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_ffn1_biases;
  if (input_ffn1_biases_vec){
    input_ffn1_biases = paddle::optional<std::vector<const phi::DenseTensor*>>(input_ffn1_biases_vec->size());
    for (size_t i = 0; i < input_ffn1_biases_vec->size(); ++i) {
      input_ffn1_biases->at(i) = &input_ffn1_biases_vec->at(i);
    }
  }
  auto input_ffn2_weights_vec = PrepareData(ffn2_weights, GetKernelInputArgDef(kernel.InputAt(18), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_ffn2_weights(input_ffn2_weights_vec->size());
  for (size_t i = 0; i < input_ffn2_weights.size(); ++i) {
    input_ffn2_weights[i] = &input_ffn2_weights_vec->at(i);
  }
  auto input_ffn2_biases_vec = PrepareData(ffn2_biases, GetKernelInputArgDef(kernel.InputAt(19), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_ffn2_biases;
  if (input_ffn2_biases_vec){
    input_ffn2_biases = paddle::optional<std::vector<const phi::DenseTensor*>>(input_ffn2_biases_vec->size());
    for (size_t i = 0; i < input_ffn2_biases_vec->size(); ++i) {
      input_ffn2_biases->at(i) = &input_ffn2_biases_vec->at(i);
    }
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> rotary_tensor_record_shapes;
     if(input_rotary_tensor){
       rotary_tensor_record_shapes.push_back((*input_rotary_tensor).dims());
     }
     std::vector<phi::DDim> beam_offset_record_shapes;
     if(input_beam_offset){
       beam_offset_record_shapes.push_back((*input_beam_offset).dims());
     }
     std::vector<phi::DDim> time_step_record_shapes;
     if(input_time_step){
       time_step_record_shapes.push_back((*input_time_step).dims());
     }
     std::vector<phi::DDim> seq_lengths_record_shapes;
     if(input_seq_lengths){
       seq_lengths_record_shapes.push_back((*input_seq_lengths).dims());
     }
     std::vector<phi::DDim> src_mask_record_shapes;
     if(input_src_mask){
       src_mask_record_shapes.push_back((*input_src_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"rotary_tensor", rotary_tensor_record_shapes},
     {"beam_offset", beam_offset_record_shapes},
     {"time_step", time_step_record_shapes},
     {"seq_lengths", seq_lengths_record_shapes},
     {"src_mask",
     src_mask_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_ln_scales.size());
     for (size_t i = 0; i < input_ln_scales.size(); ++i) {
       ddims_vec.emplace_back((*input_ln_scales[i]).dims());
     }
     input_shapes.emplace_back("ln_scales", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_ln_biases.size());
     for (size_t i = 0; i < input_ln_biases.size(); ++i) {
       ddims_vec.emplace_back((*input_ln_biases[i]).dims());
     }
     input_shapes.emplace_back("ln_biases", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_qkv_weights.size());
     for (size_t i = 0; i < input_qkv_weights.size(); ++i) {
       ddims_vec.emplace_back((*input_qkv_weights[i]).dims());
     }
     input_shapes.emplace_back("qkv_weights", ddims_vec);
     ddims_vec.clear();
     if (input_qkv_biases){
       ddims_vec.reserve(input_qkv_biases->size());
       for (size_t i = 0; i < input_qkv_biases->size(); ++i) {
         ddims_vec.emplace_back((*input_qkv_biases->at(i)).dims());
       }
     }
     input_shapes.emplace_back("qkv_biases", ddims_vec);
     ddims_vec.clear();
     if (input_cache_kvs){
       ddims_vec.reserve(input_cache_kvs->size());
       for (size_t i = 0; i < input_cache_kvs->size(); ++i) {
         ddims_vec.emplace_back((*input_cache_kvs->at(i)).dims());
       }
     }
     input_shapes.emplace_back("cache_kvs", ddims_vec);
     ddims_vec.clear();
     if (input_pre_caches){
       ddims_vec.reserve(input_pre_caches->size());
       for (size_t i = 0; i < input_pre_caches->size(); ++i) {
         ddims_vec.emplace_back((*input_pre_caches->at(i)).dims());
       }
     }
     input_shapes.emplace_back("pre_caches", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_out_linear_weights.size());
     for (size_t i = 0; i < input_out_linear_weights.size(); ++i) {
       ddims_vec.emplace_back((*input_out_linear_weights[i]).dims());
     }
     input_shapes.emplace_back("out_linear_weights", ddims_vec);
     ddims_vec.clear();
     if (input_out_linear_biases){
       ddims_vec.reserve(input_out_linear_biases->size());
       for (size_t i = 0; i < input_out_linear_biases->size(); ++i) {
         ddims_vec.emplace_back((*input_out_linear_biases->at(i)).dims());
       }
     }
     input_shapes.emplace_back("out_linear_biases", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_ffn_ln_scales.size());
     for (size_t i = 0; i < input_ffn_ln_scales.size(); ++i) {
       ddims_vec.emplace_back((*input_ffn_ln_scales[i]).dims());
     }
     input_shapes.emplace_back("ffn_ln_scales", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_ffn_ln_biases.size());
     for (size_t i = 0; i < input_ffn_ln_biases.size(); ++i) {
       ddims_vec.emplace_back((*input_ffn_ln_biases[i]).dims());
     }
     input_shapes.emplace_back("ffn_ln_biases", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_ffn1_weights.size());
     for (size_t i = 0; i < input_ffn1_weights.size(); ++i) {
       ddims_vec.emplace_back((*input_ffn1_weights[i]).dims());
     }
     input_shapes.emplace_back("ffn1_weights", ddims_vec);
     ddims_vec.clear();
     if (input_ffn1_biases){
       ddims_vec.reserve(input_ffn1_biases->size());
       for (size_t i = 0; i < input_ffn1_biases->size(); ++i) {
         ddims_vec.emplace_back((*input_ffn1_biases->at(i)).dims());
       }
     }
     input_shapes.emplace_back("ffn1_biases", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_ffn2_weights.size());
     for (size_t i = 0; i < input_ffn2_weights.size(); ++i) {
       ddims_vec.emplace_back((*input_ffn2_weights[i]).dims());
     }
     input_shapes.emplace_back("ffn2_weights", ddims_vec);
     ddims_vec.clear();
     if (input_ffn2_biases){
       ddims_vec.reserve(input_ffn2_biases->size());
       for (size_t i = 0; i < input_ffn2_biases->size(); ++i) {
         ddims_vec.emplace_back((*input_ffn2_biases->at(i)).dims());
       }
     }
     input_shapes.emplace_back("ffn2_biases", ddims_vec);
     phi::AttributeMap attrs;
     attrs["pre_layer_norm"] = pre_layer_norm;
     attrs["epsilon"] = epsilon;
     attrs["residual_alpha"] = residual_alpha;
     attrs["dropout_rate"] = dropout_rate;
     attrs["rotary_emb_dims"] = rotary_emb_dims;
     attrs["is_test"] = is_test;
     attrs["dropout_implementation"] = dropout_implementation;
     attrs["act_method"] = act_method;
     attrs["trans_qkvw"] = trans_qkvw;
     attrs["ring_id"] = ring_id;
     attrs["norm_type"] = norm_type;
     attrs["use_neox_rotary_style"] = use_neox_rotary_style;
     attrs["gqa_group_size"] = gqa_group_size;
     phi::RecordOpInfoSupplement("fused_multi_transformer", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(out_linear_weights.size(), &std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_multi_transformer infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto ln_scales_meta_vec = MakeMetaTensor(input_ln_scales);
  std::vector<const phi::MetaTensor*> ln_scales_metas(ln_scales_meta_vec.size());
  for (size_t i = 0; i < ln_scales_meta_vec.size(); ++i) {
    ln_scales_metas[i] = &ln_scales_meta_vec[i];
  }

  auto ln_biases_meta_vec = MakeMetaTensor(input_ln_biases);
  std::vector<const phi::MetaTensor*> ln_biases_metas(ln_biases_meta_vec.size());
  for (size_t i = 0; i < ln_biases_meta_vec.size(); ++i) {
    ln_biases_metas[i] = &ln_biases_meta_vec[i];
  }

  auto qkv_weights_meta_vec = MakeMetaTensor(input_qkv_weights);
  std::vector<const phi::MetaTensor*> qkv_weights_metas(qkv_weights_meta_vec.size());
  for (size_t i = 0; i < qkv_weights_meta_vec.size(); ++i) {
    qkv_weights_metas[i] = &qkv_weights_meta_vec[i];
  }

  auto qkv_biases_meta_vec = MakeMetaTensor(input_qkv_biases);
  paddle::optional<std::vector<const phi::MetaTensor*>> qkv_biases_metas(qkv_biases_meta_vec.size());
  for (size_t i = 0; i < qkv_biases_meta_vec.size(); ++i) {
    qkv_biases_metas->at(i) = &qkv_biases_meta_vec[i];
  }

  auto cache_kvs_meta_vec = MakeMetaTensor(input_cache_kvs);
  paddle::optional<std::vector<const phi::MetaTensor*>> cache_kvs_metas(cache_kvs_meta_vec.size());
  for (size_t i = 0; i < cache_kvs_meta_vec.size(); ++i) {
    cache_kvs_metas->at(i) = &cache_kvs_meta_vec[i];
  }

  auto pre_caches_meta_vec = MakeMetaTensor(input_pre_caches);
  paddle::optional<std::vector<const phi::MetaTensor*>> pre_caches_metas(pre_caches_meta_vec.size());
  for (size_t i = 0; i < pre_caches_meta_vec.size(); ++i) {
    pre_caches_metas->at(i) = &pre_caches_meta_vec[i];
  }

  auto out_linear_weights_meta_vec = MakeMetaTensor(input_out_linear_weights);
  std::vector<const phi::MetaTensor*> out_linear_weights_metas(out_linear_weights_meta_vec.size());
  for (size_t i = 0; i < out_linear_weights_meta_vec.size(); ++i) {
    out_linear_weights_metas[i] = &out_linear_weights_meta_vec[i];
  }

  auto out_linear_biases_meta_vec = MakeMetaTensor(input_out_linear_biases);
  paddle::optional<std::vector<const phi::MetaTensor*>> out_linear_biases_metas(out_linear_biases_meta_vec.size());
  for (size_t i = 0; i < out_linear_biases_meta_vec.size(); ++i) {
    out_linear_biases_metas->at(i) = &out_linear_biases_meta_vec[i];
  }

  auto ffn_ln_scales_meta_vec = MakeMetaTensor(input_ffn_ln_scales);
  std::vector<const phi::MetaTensor*> ffn_ln_scales_metas(ffn_ln_scales_meta_vec.size());
  for (size_t i = 0; i < ffn_ln_scales_meta_vec.size(); ++i) {
    ffn_ln_scales_metas[i] = &ffn_ln_scales_meta_vec[i];
  }

  auto ffn_ln_biases_meta_vec = MakeMetaTensor(input_ffn_ln_biases);
  std::vector<const phi::MetaTensor*> ffn_ln_biases_metas(ffn_ln_biases_meta_vec.size());
  for (size_t i = 0; i < ffn_ln_biases_meta_vec.size(); ++i) {
    ffn_ln_biases_metas[i] = &ffn_ln_biases_meta_vec[i];
  }

  auto ffn1_weights_meta_vec = MakeMetaTensor(input_ffn1_weights);
  std::vector<const phi::MetaTensor*> ffn1_weights_metas(ffn1_weights_meta_vec.size());
  for (size_t i = 0; i < ffn1_weights_meta_vec.size(); ++i) {
    ffn1_weights_metas[i] = &ffn1_weights_meta_vec[i];
  }

  auto ffn1_biases_meta_vec = MakeMetaTensor(input_ffn1_biases);
  paddle::optional<std::vector<const phi::MetaTensor*>> ffn1_biases_metas(ffn1_biases_meta_vec.size());
  for (size_t i = 0; i < ffn1_biases_meta_vec.size(); ++i) {
    ffn1_biases_metas->at(i) = &ffn1_biases_meta_vec[i];
  }

  auto ffn2_weights_meta_vec = MakeMetaTensor(input_ffn2_weights);
  std::vector<const phi::MetaTensor*> ffn2_weights_metas(ffn2_weights_meta_vec.size());
  for (size_t i = 0; i < ffn2_weights_meta_vec.size(); ++i) {
    ffn2_weights_metas[i] = &ffn2_weights_meta_vec[i];
  }

  auto ffn2_biases_meta_vec = MakeMetaTensor(input_ffn2_biases);
  paddle::optional<std::vector<const phi::MetaTensor*>> ffn2_biases_metas(ffn2_biases_meta_vec.size());
  for (size_t i = 0; i < ffn2_biases_meta_vec.size(); ++i) {
    ffn2_biases_metas->at(i) = &ffn2_biases_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::FusedMultiTransformerInferMeta(MakeMetaTensor(*input_x), ln_scales_metas, ln_biases_metas, qkv_weights_metas, qkv_biases_metas, cache_kvs_metas, pre_caches_metas, MakeMetaTensor(input_rotary_tensor), MakeMetaTensor(input_beam_offset), MakeMetaTensor(input_time_step), MakeMetaTensor(input_seq_lengths), MakeMetaTensor(input_src_mask), out_linear_weights_metas, out_linear_biases_metas, ffn_ln_scales_metas, ffn_ln_biases_metas, ffn1_weights_metas, ffn1_biases_metas, ffn2_weights_metas, ffn2_biases_metas, pre_layer_norm, epsilon, residual_alpha, dropout_rate, rotary_emb_dims, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, norm_type, use_neox_rotary_style, gqa_group_size, kernel_out_0_metas, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, bool, float, float, float, int, bool, const std::string&, const std::string&, bool, int, const std::string&, bool, int, std::vector<phi::DenseTensor*>, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_multi_transformer compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_ln_scales, input_ln_biases, input_qkv_weights, input_qkv_biases, input_cache_kvs, input_pre_caches, input_rotary_tensor, input_beam_offset, input_time_step, input_seq_lengths, input_src_mask, input_out_linear_weights, input_out_linear_biases, input_ffn_ln_scales, input_ffn_ln_biases, input_ffn1_weights, input_ffn1_biases, input_ffn2_weights, input_ffn2_biases, pre_layer_norm, epsilon, residual_alpha, dropout_rate, rotary_emb_dims, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, norm_type, use_neox_rotary_style, gqa_group_size, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fused_softmax_mask(const Tensor& x, const Tensor& mask) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_softmax_mask API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_softmax_mask", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_softmax_mask", kernel_data_type);
  }
  VLOG(6) << "fused_softmax_mask kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fused_softmax_mask", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_softmax_mask infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SoftmaxMaskFuseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_softmax_mask compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor fused_softmax_mask_upper_triangle(const Tensor& X) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(X);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_softmax_mask_upper_triangle API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_softmax_mask_upper_triangle", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_softmax_mask_upper_triangle", kernel_data_type);
  }
  VLOG(6) << "fused_softmax_mask_upper_triangle kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_X = PrepareData(X, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"X", {
     (*input_X).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fused_softmax_mask_upper_triangle", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_softmax_mask_upper_triangle infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_X), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_softmax_mask_upper_triangle compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_X, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor gammaincc(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gammaincc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaincc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaincc", kernel_data_type);
  }
  VLOG(6) << "gammaincc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaincc", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaincc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaincc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& gammaincc_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gammaincc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaincc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaincc", kernel_data_type);
  }
  VLOG(6) << "gammaincc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaincc", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaincc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaincc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor gammaln(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gammaln API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaln", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaln", kernel_data_type);
  }
  VLOG(6) << "gammaln kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaln", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaln infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaln compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& gammaln_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gammaln API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaln", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaln", kernel_data_type);
  }
  VLOG(6) << "gammaln kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaln", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaln infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaln compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor gather(const Tensor& x, const Tensor& index, const Scalar& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gather", kernel_data_type);
  }
  VLOG(6) << "gather kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("gather", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gather infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GatherInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gather compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor gather_nd(const Tensor& x, const Tensor& index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather_nd API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_nd", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gather_nd", kernel_data_type);
  }
  VLOG(6) << "gather_nd kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gather_nd", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gather_nd infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GatherNdInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gather_nd compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor gather_tree(const Tensor& ids, const Tensor& parents) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(ids);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(ids, parents);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather_tree API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_tree", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gather_tree", kernel_data_type);
  }
  VLOG(6) << "gather_tree kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_ids = PrepareData(ids, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_parents = PrepareData(parents, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"ids", {
     (*input_ids).dims()}},
     {"parents", {
     (*input_parents).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gather_tree", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gather_tree infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GatherTreeMeta(MakeMetaTensor(*input_ids), MakeMetaTensor(*input_parents), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gather_tree compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_ids, *input_parents, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor gaussian(const IntArray& shape, float mean, float std, int seed, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "gaussian API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gaussian", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gaussian", kernel_data_type);
  }
  VLOG(6) << "gaussian kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     attrs["mean"] = mean;
     attrs["std"] = std;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("gaussian", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gaussian infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GaussianInferMeta(shape, mean, std, seed, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::IntArray&, float, float, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gaussian compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::IntArray(shape), mean, std, seed, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor gaussian_inplace(const Tensor& x, float mean, float std, int seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gaussian_inplace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gaussian_inplace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gaussian_inplace", kernel_data_type);
  }
  VLOG(6) << "gaussian_inplace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["mean"] = mean;
     attrs["std"] = std;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("gaussian_inplace", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gaussian_inplace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gaussian_inplace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, mean, std, seed, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& gaussian_inplace_(Tensor& x, float mean, float std, int seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gaussian_inplace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gaussian_inplace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gaussian_inplace", kernel_data_type);
  }
  VLOG(6) << "gaussian_inplace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["mean"] = mean;
     attrs["std"] = std;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("gaussian_inplace", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gaussian_inplace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gaussian_inplace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, mean, std, seed, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor gelu(const Tensor& x, bool approximate) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gelu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gelu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gelu", kernel_data_type);
  }
  VLOG(6) << "gelu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["approximate"] = approximate;
     phi::RecordOpInfoSupplement("gelu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gelu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gelu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, approximate, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> generate_proposals(const Tensor& scores, const Tensor& bbox_deltas, const Tensor& im_shape, const Tensor& anchors, const Tensor& variances, int pre_nms_top_n, int post_nms_top_n, float nms_thresh, float min_size, float eta, bool pixel_offset) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(anchors);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(scores, bbox_deltas, im_shape, anchors, variances);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "generate_proposals API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "generate_proposals", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("generate_proposals", kernel_data_type);
  }
  VLOG(6) << "generate_proposals kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_scores = PrepareData(scores, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bbox_deltas = PrepareData(bbox_deltas, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_im_shape = PrepareData(im_shape, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_anchors = PrepareData(anchors, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variances = PrepareData(variances, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"scores", {
     (*input_scores).dims()}},
     {"bbox_deltas", {
     (*input_bbox_deltas).dims()}},
     {"im_shape", {
     (*input_im_shape).dims()}},
     {"anchors", {
     (*input_anchors).dims()}},
     {"variances", {
     (*input_variances).dims()}}};
     phi::AttributeMap attrs;
     attrs["pre_nms_top_n"] = pre_nms_top_n;
     attrs["post_nms_top_n"] = post_nms_top_n;
     attrs["nms_thresh"] = nms_thresh;
     attrs["min_size"] = min_size;
     attrs["eta"] = eta;
     attrs["pixel_offset"] = pixel_offset;
     phi::RecordOpInfoSupplement("generate_proposals", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("generate_proposals infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GenerateProposalsV2InferMeta(MakeMetaTensor(*input_scores), MakeMetaTensor(*input_bbox_deltas), MakeMetaTensor(*input_im_shape), MakeMetaTensor(*input_anchors), MakeMetaTensor(*input_variances), pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("generate_proposals compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_scores, *input_bbox_deltas, *input_im_shape, *input_anchors, *input_variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> graph_khop_sampler(const Tensor& row, const Tensor& colptr, const Tensor& x, const paddle::optional<Tensor>& eids, const std::vector<int>& sample_sizes, bool return_eids) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(row);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(row, colptr, x, eids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "graph_khop_sampler API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_khop_sampler", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("graph_khop_sampler", kernel_data_type);
  }
  VLOG(6) << "graph_khop_sampler kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_row = PrepareData(row, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_colptr = PrepareData(colptr, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_eids = PrepareData(eids, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> eids_record_shapes;
     if(input_eids){
       eids_record_shapes.push_back((*input_eids).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"row", {
     (*input_row).dims()}},
     {"colptr", {
     (*input_colptr).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"eids",
     eids_record_shapes}};
     phi::AttributeMap attrs;
     attrs["sample_sizes"] = sample_sizes;
     attrs["return_eids"] = return_eids;
     phi::RecordOpInfoSupplement("graph_khop_sampler", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("graph_khop_sampler infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);

  phi::GraphKhopSamplerInferMeta(MakeMetaTensor(*input_row), MakeMetaTensor(*input_colptr), MakeMetaTensor(*input_x), MakeMetaTensor(input_eids), sample_sizes, return_eids, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("graph_khop_sampler compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_row, *input_colptr, *input_x, input_eids, sample_sizes, return_eids, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> graph_sample_neighbors(const Tensor& row, const Tensor& colptr, const Tensor& x, const paddle::optional<Tensor>& eids, const paddle::optional<Tensor>& perm_buffer, int sample_size, bool return_eids, bool flag_perm_buffer) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(row);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(row, colptr, x, eids, perm_buffer);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "graph_sample_neighbors API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_sample_neighbors", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("graph_sample_neighbors", kernel_data_type);
  }
  VLOG(6) << "graph_sample_neighbors kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_row = PrepareData(row, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_colptr = PrepareData(colptr, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_eids = PrepareData(eids, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_perm_buffer = PrepareData(perm_buffer, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> eids_record_shapes;
     if(input_eids){
       eids_record_shapes.push_back((*input_eids).dims());
     }
     std::vector<phi::DDim> perm_buffer_record_shapes;
     if(input_perm_buffer){
       perm_buffer_record_shapes.push_back((*input_perm_buffer).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"row", {
     (*input_row).dims()}},
     {"colptr", {
     (*input_colptr).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"eids", eids_record_shapes},
     {"perm_buffer",
     perm_buffer_record_shapes}};
     phi::AttributeMap attrs;
     attrs["sample_size"] = sample_size;
     attrs["return_eids"] = return_eids;
     attrs["flag_perm_buffer"] = flag_perm_buffer;
     phi::RecordOpInfoSupplement("graph_sample_neighbors", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("graph_sample_neighbors infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GraphSampleNeighborsInferMeta(MakeMetaTensor(*input_row), MakeMetaTensor(*input_colptr), MakeMetaTensor(*input_x), MakeMetaTensor(input_eids), MakeMetaTensor(input_perm_buffer), sample_size, return_eids, flag_perm_buffer, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("graph_sample_neighbors compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_row, *input_colptr, *input_x, input_eids, input_perm_buffer, sample_size, return_eids, flag_perm_buffer, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor grid_sample(const Tensor& x, const Tensor& grid, const std::string& mode, const std::string& padding_mode, bool align_corners) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, grid);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "grid_sample API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "grid_sample", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("grid_sample", kernel_data_type);
  }
  VLOG(6) << "grid_sample kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grid = PrepareData(grid, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grid", {
     (*input_grid).dims()}}};
     phi::AttributeMap attrs;
     attrs["mode"] = mode;
     attrs["padding_mode"] = padding_mode;
     attrs["align_corners"] = align_corners;
     phi::RecordOpInfoSupplement("grid_sample", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("grid_sample infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GridSampleBaseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_grid), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("grid_sample compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grid, mode, padding_mode, align_corners, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor group_norm(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, float epsilon, int groups, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "group_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "group_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("group_norm", kernel_data_type);
  }
  VLOG(6) << "group_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("group_norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("group_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GroupNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), epsilon, groups, data_format, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("group_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, epsilon, groups, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor gru(const Tensor& input, const paddle::optional<Tensor>& h0, const Tensor& weight, const paddle::optional<Tensor>& bias, const std::string& activation, const std::string& gate_activation, bool is_reverse, bool origin_mode, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, h0, weight, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gru API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gru", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gru", kernel_data_type);
  }
  VLOG(6) << "gru kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_h0 = PrepareData(h0, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> h0_record_shapes;
     if(input_h0){
       h0_record_shapes.push_back((*input_h0).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"h0", h0_record_shapes},
     {"weight", {
     (*input_weight).dims()}},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["activation"] = activation;
     attrs["gate_activation"] = gate_activation;
     attrs["is_reverse"] = is_reverse;
     attrs["origin_mode"] = origin_mode;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("gru", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gru infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::GruInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(input_h0), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), activation, gate_activation, is_reverse, origin_mode, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::string&, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gru compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, input_h0, *input_weight, input_bias, activation, gate_activation, is_reverse, origin_mode, is_test, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<3>(api_output);
}

PADDLE_API Tensor gru_unit(const Tensor& input, const Tensor& hidden_prev, const Tensor& weight, const paddle::optional<Tensor>& bias, int activation, int gate_activation, bool origin_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, hidden_prev, weight, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gru_unit API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gru_unit", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gru_unit", kernel_data_type);
  }
  VLOG(6) << "gru_unit kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_hidden_prev = PrepareData(hidden_prev, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"hidden_prev", {
     (*input_hidden_prev).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["activation"] = activation;
     attrs["gate_activation"] = gate_activation;
     attrs["origin_mode"] = origin_mode;
     phi::RecordOpInfoSupplement("gru_unit", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gru_unit infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GruUnitInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_hidden_prev), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), activation, gate_activation, origin_mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gru_unit compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_hidden_prev, *input_weight, input_bias, activation, gate_activation, origin_mode, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<2>(api_output);
}

PADDLE_API Tensor gumbel_softmax(const Tensor& x, float temperature, bool hard, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gumbel_softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gumbel_softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gumbel_softmax", kernel_data_type);
  }
  VLOG(6) << "gumbel_softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["temperature"] = temperature;
     attrs["hard"] = hard;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("gumbel_softmax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gumbel_softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GumbelSoftmaxInferMeta(MakeMetaTensor(*input_x), temperature, hard, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gumbel_softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, temperature, hard, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor hardshrink(const Tensor& x, float threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hardshrink API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hard_shrink", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardshrink", kernel_data_type);
  }
  VLOG(6) << "hard_shrink kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("hardshrink", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardshrink infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardshrink compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor hardsigmoid(const Tensor& x, float slope, float offset) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hardsigmoid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardsigmoid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardsigmoid", kernel_data_type);
  }
  VLOG(6) << "hardsigmoid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["slope"] = slope;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("hardsigmoid", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardsigmoid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardsigmoid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, slope, offset, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor hardtanh(const Tensor& x, float t_min, float t_max) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hardtanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardtanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardtanh", kernel_data_type);
  }
  VLOG(6) << "hardtanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["t_min"] = t_min;
     attrs["t_max"] = t_max;
     phi::RecordOpInfoSupplement("hardtanh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardtanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardtanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, t_min, t_max, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& hardtanh_(Tensor& x, float t_min, float t_max) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hardtanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardtanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardtanh", kernel_data_type);
  }
  VLOG(6) << "hardtanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["t_min"] = t_min;
     attrs["t_max"] = t_max;
     phi::RecordOpInfoSupplement("hardtanh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardtanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardtanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, t_min, t_max, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor heaviside(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "heaviside API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "heaviside", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("heaviside", kernel_data_type);
  }
  VLOG(6) << "heaviside kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("heaviside", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("heaviside infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("heaviside compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor hinge_loss(const Tensor& logits, const Tensor& labels) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(logits);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(logits, labels);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hinge_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hinge_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hinge_loss", kernel_data_type);
  }
  VLOG(6) << "hinge_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_logits = PrepareData(logits, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_labels = PrepareData(labels, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"logits", {
     (*input_logits).dims()}},
     {"labels", {
     (*input_labels).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("hinge_loss", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hinge_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::HingeLossInferMeta(MakeMetaTensor(*input_logits), MakeMetaTensor(*input_labels), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hinge_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_logits, *input_labels, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor histogram(const Tensor& input, const paddle::optional<Tensor>& weight, int64_t bins, int min, int max, bool density) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "histogram API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "histogram", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("histogram", kernel_data_type);
  }
  VLOG(6) << "histogram kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> weight_record_shapes;
     if(input_weight){
       weight_record_shapes.push_back((*input_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"weight",
     weight_record_shapes}};
     phi::AttributeMap attrs;
     attrs["bins"] = bins;
     attrs["min"] = min;
     attrs["max"] = max;
     attrs["density"] = density;
     phi::RecordOpInfoSupplement("histogram", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("histogram infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::HistogramInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(input_weight), bins, min, max, density, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int64_t, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("histogram compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, input_weight, bins, min, max, density, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> hsigmoid_loss(const Tensor& x, const Tensor& label, const Tensor& w, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& path, const paddle::optional<Tensor>& code, int num_classes, bool is_sparse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, w, bias, path, code);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hsigmoid_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hsigmoid_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hsigmoid_loss", kernel_data_type);
  }
  VLOG(6) << "hsigmoid_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_path = PrepareData(path, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_code = PrepareData(code, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> path_record_shapes;
     if(input_path){
       path_record_shapes.push_back((*input_path).dims());
     }
     std::vector<phi::DDim> code_record_shapes;
     if(input_code){
       code_record_shapes.push_back((*input_code).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"w", {
     (*input_w).dims()}},
     {"bias", bias_record_shapes},
     {"path", path_record_shapes},
     {"code",
     code_record_shapes}};
     phi::AttributeMap attrs;
     attrs["num_classes"] = num_classes;
     attrs["is_sparse"] = is_sparse;
     phi::RecordOpInfoSupplement("hsigmoid_loss", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hsigmoid_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::HSigmoidLossInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_label), MakeMetaTensor(*input_w), MakeMetaTensor(input_bias), MakeMetaTensor(input_path), MakeMetaTensor(input_code), num_classes, is_sparse, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hsigmoid_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, *input_w, input_bias, input_path, input_code, num_classes, is_sparse, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor huber_loss(const Tensor& input, const Tensor& label, float delta) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "huber_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "huber_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("huber_loss", kernel_data_type);
  }
  VLOG(6) << "huber_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["delta"] = delta;
     phi::RecordOpInfoSupplement("huber_loss", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("huber_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::HuberLossInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), delta, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("huber_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, delta, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor i0(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "i0 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i0", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i0", kernel_data_type);
  }
  VLOG(6) << "i0 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i0", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i0 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i0 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& i0_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "i0 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i0", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i0", kernel_data_type);
  }
  VLOG(6) << "i0 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i0", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i0 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i0 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor i0e(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "i0e API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i0e", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i0e", kernel_data_type);
  }
  VLOG(6) << "i0e kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i0e", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i0e infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i0e compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor i1(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "i1 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i1", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i1", kernel_data_type);
  }
  VLOG(6) << "i1 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i1", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i1 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i1 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor i1e(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "i1e API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i1e", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i1e", kernel_data_type);
  }
  VLOG(6) << "i1e kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i1e", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i1e infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i1e compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor identity_loss(const Tensor& x, int reduction) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "identity_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "identity_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("identity_loss", kernel_data_type);
  }
  VLOG(6) << "identity_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("identity_loss", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("identity_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IdentityLossInferMeta(MakeMetaTensor(*input_x), reduction, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("identity_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, reduction, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& identity_loss_(Tensor& x, int reduction) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "identity_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "identity_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("identity_loss", kernel_data_type);
  }
  VLOG(6) << "identity_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("identity_loss", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("identity_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IdentityLossInferMeta(MakeMetaTensor(origin_input_x), reduction, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("identity_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, reduction, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor im2sequence(const Tensor& x, const paddle::optional<Tensor>& y, const std::vector<int>& kernels, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& out_stride) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "im2sequence API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "im2sequence", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("im2sequence", kernel_data_type);
  }
  VLOG(6) << "im2sequence kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> y_record_shapes;
     if(input_y){
       y_record_shapes.push_back((*input_y).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y",
     y_record_shapes}};
     phi::AttributeMap attrs;
     attrs["kernels"] = kernels;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["out_stride"] = out_stride;
     phi::RecordOpInfoSupplement("im2sequence", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("im2sequence infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Im2sequenceInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_y), kernels, strides, paddings, out_stride, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("im2sequence compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_y, kernels, strides, paddings, out_stride, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor imag(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "imag API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "imag", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("imag", kernel_data_type);
  }
  VLOG(6) << "imag kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("imag", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("imag infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("imag compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor increment(const Tensor& x, float value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "increment API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "increment", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("increment", kernel_data_type);
  }
  VLOG(6) << "increment kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["value"] = value;
     phi::RecordOpInfoSupplement("increment", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("increment infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IncrementInferMeta(MakeMetaTensor(*input_x), value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("increment compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& increment_(Tensor& x, float value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "increment API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "increment", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("increment", kernel_data_type);
  }
  VLOG(6) << "increment kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["value"] = value;
     phi::RecordOpInfoSupplement("increment", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("increment infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IncrementInferMeta(MakeMetaTensor(origin_input_x), value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("increment compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor index_add(const Tensor& x, const Tensor& index, const Tensor& add_value, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, add_value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_add", kernel_data_type);
  }
  VLOG(6) << "index_add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_add_value = PrepareData(add_value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"add_value", {
     (*input_add_value).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_add", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexAddInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_add_value), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_add_value, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& index_add_(Tensor& x, const Tensor& index, const Tensor& add_value, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, add_value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_add", kernel_data_type);
  }
  VLOG(6) << "index_add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_add_value = PrepareData(add_value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"add_value", {
     (*input_add_value).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_add", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexAddInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_add_value), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_index, *input_add_value, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor index_put(const Tensor& x, const std::vector<Tensor>& indices, const Tensor& value, bool accumulate) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_put API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_put", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_put", kernel_data_type);
  }
  VLOG(6) << "index_put kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices_vec = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_indices(input_indices_vec->size());
  for (size_t i = 0; i < input_indices.size(); ++i) {
    input_indices[i] = &input_indices_vec->at(i);
  }
  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"value", {
     (*input_value).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_indices.size());
     for (size_t i = 0; i < input_indices.size(); ++i) {
       ddims_vec.emplace_back((*input_indices[i]).dims());
     }
     input_shapes.emplace_back("indices", ddims_vec);
     phi::AttributeMap attrs;
     attrs["accumulate"] = accumulate;
     phi::RecordOpInfoSupplement("index_put", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_put infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto indices_meta_vec = MakeMetaTensor(input_indices);
  std::vector<const phi::MetaTensor*> indices_metas(indices_meta_vec.size());
  for (size_t i = 0; i < indices_meta_vec.size(); ++i) {
    indices_metas[i] = &indices_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexPutInferMeta(MakeMetaTensor(*input_x), indices_metas, MakeMetaTensor(*input_value), accumulate, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_put compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_indices, *input_value, accumulate, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& index_put_(Tensor& x, const std::vector<Tensor>& indices, const Tensor& value, bool accumulate) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_put API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_put", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_put", kernel_data_type);
  }
  VLOG(6) << "index_put kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices_vec = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_indices(input_indices_vec->size());
  for (size_t i = 0; i < input_indices.size(); ++i) {
    input_indices[i] = &input_indices_vec->at(i);
  }
  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"value", {
     (*input_value).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_indices.size());
     for (size_t i = 0; i < input_indices.size(); ++i) {
       ddims_vec.emplace_back((*input_indices[i]).dims());
     }
     input_shapes.emplace_back("indices", ddims_vec);
     phi::AttributeMap attrs;
     attrs["accumulate"] = accumulate;
     phi::RecordOpInfoSupplement("index_put", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_put infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;

  auto indices_meta_vec = MakeMetaTensor(input_indices);
  std::vector<const phi::MetaTensor*> indices_metas(indices_meta_vec.size());
  for (size_t i = 0; i < indices_meta_vec.size(); ++i) {
    indices_metas[i] = &indices_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexPutInferMeta(MakeMetaTensor(origin_input_x), indices_metas, MakeMetaTensor(*input_value), accumulate, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_put compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, input_indices, *input_value, accumulate, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor index_sample(const Tensor& x, const Tensor& index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_sample API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_sample", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_sample", kernel_data_type);
  }
  VLOG(6) << "index_sample kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("index_sample", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_sample infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexSampleInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_sample compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor index_select(const Tensor& x, const Tensor& index, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_select API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_select", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_select", kernel_data_type);
  }
  VLOG(6) << "index_select kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_select", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_select infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexSelectInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_select compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor index_select_strided(const Tensor& x, int64_t index, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_select_strided API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_select_strided", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_select_strided", kernel_data_type);
  }
  VLOG(6) << "index_select_strided kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["index"] = index;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_select_strided", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_select_strided infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IndexSelectStridedInferMeta(MakeMetaTensor(*input_x), index, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int64_t, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_select_strided compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, index, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor instance_norm(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, float epsilon) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "instance_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "instance_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("instance_norm", kernel_data_type);
  }
  VLOG(6) << "instance_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("instance_norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("instance_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::InstanceNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), epsilon, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("instance_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, epsilon, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor inverse(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "inverse API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "inverse", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("inverse", kernel_data_type);
  }
  VLOG(6) << "inverse kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("inverse", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("inverse infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InverseInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("inverse compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor is_empty(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "is_empty API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "is_empty", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("is_empty", kernel_data_type);
  }
  VLOG(6) << "is_empty kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("is_empty", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("is_empty infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IsEmptyInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("is_empty compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor isclose(const Tensor& x, const Tensor& y, const Scalar& rtol, const Scalar& atol, bool equal_nan) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "isclose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "isclose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isclose", kernel_data_type);
  }
  VLOG(6) << "isclose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
    switch (rtol.dtype()) {
      case DataType::FLOAT32:
          attrs["rtol"] = static_cast<float>(rtol.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["rtol"] = static_cast<double>(rtol.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["rtol"] = static_cast<float>(rtol.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["rtol"] = static_cast<float>(rtol.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["rtol"] = static_cast<int32_t>(rtol.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["rtol"] = static_cast<int64_t>(rtol.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["rtol"] = static_cast<int16_t>(rtol.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["rtol"] = static_cast<int8_t>(rtol.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["rtol"] = static_cast<uint16_t>(rtol.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["rtol"] = static_cast<uint8_t>(rtol.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["rtol"] = static_cast<bool>(rtol.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["rtol"] = static_cast<float>(rtol.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["rtol"] = static_cast<double>(rtol.to<complex128>());
          break;
      default:
          attrs["rtol"] = "";
          break;
    }
    switch (atol.dtype()) {
      case DataType::FLOAT32:
          attrs["atol"] = static_cast<float>(atol.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["atol"] = static_cast<double>(atol.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["atol"] = static_cast<float>(atol.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["atol"] = static_cast<float>(atol.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["atol"] = static_cast<int32_t>(atol.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["atol"] = static_cast<int64_t>(atol.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["atol"] = static_cast<int16_t>(atol.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["atol"] = static_cast<int8_t>(atol.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["atol"] = static_cast<uint16_t>(atol.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["atol"] = static_cast<uint8_t>(atol.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["atol"] = static_cast<bool>(atol.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["atol"] = static_cast<float>(atol.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["atol"] = static_cast<double>(atol.to<complex128>());
          break;
      default:
          attrs["atol"] = "";
          break;
    }
     attrs["equal_nan"] = equal_nan;
     phi::RecordOpInfoSupplement("isclose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("isclose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ValueCompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("isclose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, phi::Scalar(rtol), phi::Scalar(atol), equal_nan, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor isfinite(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "isfinite API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isfinite", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isfinite", kernel_data_type);
    }
    VLOG(6) << "isfinite kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isfinite", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isfinite infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isfinite compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "isfinite API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isfinite_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isfinite", kernel_data_type);
    }
    VLOG(6) << "isfinite_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isfinite", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isfinite infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isfinite compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (isfinite) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor isinf(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "isinf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isinf", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isinf", kernel_data_type);
    }
    VLOG(6) << "isinf kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isinf", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isinf infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isinf compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "isinf API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isinf_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isinf", kernel_data_type);
    }
    VLOG(6) << "isinf_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isinf", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isinf infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isinf compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (isinf) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor isnan(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "isnan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isnan", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isnan", kernel_data_type);
    }
    VLOG(6) << "isnan kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isnan", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isnan infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isnan compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "isnan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "isnan_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("isnan", kernel_data_type);
    }
    VLOG(6) << "isnan_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("isnan", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("isnan infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("isnan compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (isnan) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor kldiv_loss(const Tensor& x, const Tensor& label, const std::string& reduction, bool log_target) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kldiv_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kldiv_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kldiv_loss", kernel_data_type);
  }
  VLOG(6) << "kldiv_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduction"] = reduction;
     attrs["log_target"] = log_target;
     phi::RecordOpInfoSupplement("kldiv_loss", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kldiv_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KLDivInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_label), reduction, log_target, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kldiv_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, reduction, log_target, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor kron(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kron API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kron", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kron", kernel_data_type);
  }
  VLOG(6) << "kron kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("kron", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kron infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KronInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kron compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> kthvalue(const Tensor& x, int k, int axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kthvalue API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kthvalue", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kthvalue", kernel_data_type);
  }
  VLOG(6) << "kthvalue kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["k"] = k;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("kthvalue", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kthvalue infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::KthvalueInferMeta(MakeMetaTensor(*input_x), k, axis, keepdim, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kthvalue compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, k, axis, keepdim, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor l1_norm(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "l1_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "l1_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("l1_norm", kernel_data_type);
  }
  VLOG(6) << "l1_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("l1_norm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("l1_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::L1NormInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("l1_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& l1_norm_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "l1_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "l1_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("l1_norm", kernel_data_type);
  }
  VLOG(6) << "l1_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("l1_norm", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("l1_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::L1NormInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("l1_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor label_smooth(const Tensor& label, const paddle::optional<Tensor>& prior_dist, float epsilon) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(label);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(label, prior_dist);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "label_smooth API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "label_smooth", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("label_smooth", kernel_data_type);
  }
  VLOG(6) << "label_smooth kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_prior_dist = PrepareData(prior_dist, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> prior_dist_record_shapes;
     if(input_prior_dist){
       prior_dist_record_shapes.push_back((*input_prior_dist).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"label", {
     (*input_label).dims()}},
     {"prior_dist",
     prior_dist_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("label_smooth", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("label_smooth infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_label), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("label_smooth compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_label, input_prior_dist, epsilon, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> lamb_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& moment1, Tensor& moment2, Tensor& beta1_pow, Tensor& beta2_pow, paddle::optional<Tensor>& master_param, const paddle::optional<Tensor>& skip_update, float weight_decay, float beta1, float beta2, float epsilon, bool always_adapt, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param, skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && grad.is_dense_tensor() && learning_rate.is_dense_tensor() && moment1.is_dense_tensor() && moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() && beta2_pow.is_dense_tensor() && (!master_param || master_param->is_dense_tensor()) && (!skip_update || skip_update->is_dense_tensor())) {

    VLOG(6) << "lamb_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "lamb", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lamb_", kernel_data_type);
    }
    VLOG(6) << "lamb kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<phi::DDim> skip_update_record_shapes;
       if(input_skip_update){
         skip_update_record_shapes.push_back((*input_skip_update).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"moment1", {
       (*input_moment1).dims()}},
       {"moment2", {
       (*input_moment2).dims()}},
       {"beta1_pow", {
       (*input_beta1_pow).dims()}},
       {"beta2_pow", {
       (*input_beta2_pow).dims()}},
       {"master_param", master_param_record_shapes},
       {"skip_update",
       skip_update_record_shapes}};
       phi::AttributeMap attrs;
       attrs["weight_decay"] = weight_decay;
       attrs["beta1"] = beta1;
       attrs["beta2"] = beta2;
       attrs["epsilon"] = epsilon;
       attrs["always_adapt"] = always_adapt;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("lamb_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);
    auto backup5 = ProcessStrideBackup(&kernel_out_5);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("lamb_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment1 = *input_moment1;

    auto origin_input_moment2 = *input_moment2;

    auto origin_input_beta1_pow = *input_beta1_pow;

    auto origin_input_beta2_pow = *input_beta2_pow;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

    phi::LambInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(input_master_param), MakeMetaTensor(input_skip_update), weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, float, float, float, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("lamb_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment1, origin_input_moment2, origin_input_beta1_pow, origin_input_beta2_pow, input_master_param, input_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);
    TransStride(dev_ctx, kernel_out_5, backup5);

    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() && learning_rate.is_dense_tensor() && moment1.is_dense_tensor() && moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() && beta2_pow.is_dense_tensor() && (!master_param || master_param->is_dense_tensor()) && (!skip_update || skip_update->is_dense_tensor())) {

    VLOG(6) << "lamb_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "lamb_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lamb_", kernel_data_type);
    }
    VLOG(6) << "lamb_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {});

    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<phi::DDim> skip_update_record_shapes;
       if(input_skip_update){
         skip_update_record_shapes.push_back((*input_skip_update).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"moment1", {
       (*input_moment1).dims()}},
       {"moment2", {
       (*input_moment2).dims()}},
       {"beta1_pow", {
       (*input_beta1_pow).dims()}},
       {"beta2_pow", {
       (*input_beta2_pow).dims()}},
       {"master_param", master_param_record_shapes},
       {"skip_update",
       skip_update_record_shapes}};
       phi::AttributeMap attrs;
       attrs["weight_decay"] = weight_decay;
       attrs["beta1"] = beta1;
       attrs["beta2"] = beta2;
       attrs["epsilon"] = epsilon;
       attrs["always_adapt"] = always_adapt;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("lamb_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);
    auto backup5 = ProcessStrideBackup(&kernel_out_5);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("lamb_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_moment1 = *input_moment1;

    auto origin_input_moment2 = *input_moment2;

    auto origin_input_beta1_pow = *input_beta1_pow;

    auto origin_input_beta2_pow = *input_beta2_pow;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

    phi::LambInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(input_master_param), MakeMetaTensor(input_skip_update), weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, float, float, float, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("lamb_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_moment1, origin_input_moment2, origin_input_beta1_pow, origin_input_beta2_pow, input_master_param, input_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);
    TransStride(dev_ctx, kernel_out_5, backup5);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (lamb_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor layer_norm(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, float epsilon, int begin_norm_axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "layer_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "layer_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("layer_norm", kernel_data_type);
  }
  VLOG(6) << "layer_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["begin_norm_axis"] = begin_norm_axis;
     phi::RecordOpInfoSupplement("layer_norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("layer_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::LayerNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), epsilon, begin_norm_axis, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("layer_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, epsilon, begin_norm_axis, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor leaky_relu(const Tensor& x, float negative_slope) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "leaky_relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "leaky_relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("leaky_relu", kernel_data_type);
  }
  VLOG(6) << "leaky_relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["negative_slope"] = negative_slope;
     phi::RecordOpInfoSupplement("leaky_relu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("leaky_relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("leaky_relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, negative_slope, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& leaky_relu_(Tensor& x, float negative_slope) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "leaky_relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "leaky_relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("leaky_relu", kernel_data_type);
  }
  VLOG(6) << "leaky_relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["negative_slope"] = negative_slope;
     phi::RecordOpInfoSupplement("leaky_relu", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("leaky_relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("leaky_relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, negative_slope, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor lerp(const Tensor& x, const Tensor& y, const Tensor& weight) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lerp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lerp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lerp", kernel_data_type);
  }
  VLOG(6) << "lerp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"weight", {
     (*input_weight).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lerp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lerp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LerpInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_weight), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lerp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& lerp_(Tensor& x, const Tensor& y, const Tensor& weight) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lerp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lerp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lerp", kernel_data_type);
  }
  VLOG(6) << "lerp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"weight", {
     (*input_weight).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lerp", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lerp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LerpInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_weight), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lerp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, *input_weight, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor lgamma(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lgamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lgamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lgamma", kernel_data_type);
  }
  VLOG(6) << "lgamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lgamma", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lgamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lgamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& lgamma_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lgamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lgamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lgamma", kernel_data_type);
  }
  VLOG(6) << "lgamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lgamma", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lgamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lgamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor limit_by_capacity(const Tensor& expert_count, const Tensor& capacity, int n_worker) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(expert_count);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(expert_count, capacity);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "limit_by_capacity API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "limit_by_capacity", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("limit_by_capacity", kernel_data_type);
  }
  VLOG(6) << "limit_by_capacity kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_expert_count = PrepareData(expert_count, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_capacity = PrepareData(capacity, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"expert_count", {
     (*input_expert_count).dims()}},
     {"capacity", {
     (*input_capacity).dims()}}};
     phi::AttributeMap attrs;
     attrs["n_worker"] = n_worker;
     phi::RecordOpInfoSupplement("limit_by_capacity", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("limit_by_capacity infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LimitByCapacityInferMeta(MakeMetaTensor(*input_expert_count), MakeMetaTensor(*input_capacity), n_worker, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("limit_by_capacity compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_expert_count, *input_capacity, n_worker, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor linear_interp(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "linear_interp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "linear_interp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("linear_interp", kernel_data_type);
  }
  VLOG(6) << "linear_interp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor",
     scale_tensor_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("linear_interp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("linear_interp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_out_size), size_tensor_metas, MakeMetaTensor(input_scale_tensor), data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("linear_interp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor linspace(const Tensor& start, const Tensor& stop, const Tensor& number, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(start, stop, number);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "linspace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "linspace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("linspace", kernel_data_type);
  }
  VLOG(6) << "linspace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_start = PrepareData(start, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_stop = PrepareData(stop, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_number = PrepareData(number, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"start", {
     (*input_start).dims()}},
     {"stop", {
     (*input_stop).dims()}},
     {"number", {
     (*input_number).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("linspace", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("linspace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LinspaceInferMeta(MakeMetaTensor(*input_start), MakeMetaTensor(*input_stop), MakeMetaTensor(*input_number), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("linspace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_start, *input_stop, *input_number, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor llm_int8_linear(const Tensor& x, const Tensor& weight, const paddle::optional<Tensor>& bias, const Tensor& weight_scale, float threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, weight, bias, weight_scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "llm_int8_linear API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "llm_int8_linear", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("llm_int8_linear", kernel_data_type);
  }
  VLOG(6) << "llm_int8_linear kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight_scale = PrepareData(weight_scale, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"bias", bias_record_shapes},
     {"weight_scale", {
     (*input_weight_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("llm_int8_linear", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("llm_int8_linear infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LLMInt8LinearInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), MakeMetaTensor(*input_weight_scale), threshold, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("llm_int8_linear compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_weight, input_bias, *input_weight_scale, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor log(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log", kernel_data_type);
  }
  VLOG(6) << "log kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& log_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log", kernel_data_type);
  }
  VLOG(6) << "log kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor log10(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log10 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log10", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log10", kernel_data_type);
  }
  VLOG(6) << "log10 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log10", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log10 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log10 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& log10_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log10 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log10", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log10", kernel_data_type);
  }
  VLOG(6) << "log10 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log10", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log10 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log10 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor log1p(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log1p API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log1p", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log1p", kernel_data_type);
  }
  VLOG(6) << "log1p kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log1p", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log1p infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log1p compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& log1p_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log1p API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log1p", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log1p", kernel_data_type);
  }
  VLOG(6) << "log1p kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log1p", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log1p infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log1p compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor log2(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log2 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log2", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log2", kernel_data_type);
  }
  VLOG(6) << "log2 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log2", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log2 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log2 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& log2_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log2 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log2", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log2", kernel_data_type);
  }
  VLOG(6) << "log2 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log2", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log2 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log2 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor log_loss(const Tensor& input, const Tensor& label, float epsilon) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_loss", kernel_data_type);
  }
  VLOG(6) << "log_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("log_loss", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogLossInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), epsilon, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, epsilon, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor log_softmax(const Tensor& x, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log_softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_softmax", kernel_data_type);
  }
  VLOG(6) << "log_softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("log_softmax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMetaCheckAxis(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor logcumsumexp(const Tensor& x, int axis, bool flatten, bool exclusive, bool reverse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logcumsumexp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logcumsumexp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logcumsumexp", kernel_data_type);
  }
  VLOG(6) << "logcumsumexp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["flatten"] = flatten;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("logcumsumexp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logcumsumexp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CumInferMeta(MakeMetaTensor(*input_x), axis, flatten, exclusive, reverse, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logcumsumexp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, flatten, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor logical_and(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_and API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_and", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_and", kernel_data_type);
  }
  VLOG(6) << "logical_and kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_and", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_and infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_and compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& logical_and_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_and API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_and", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_and", kernel_data_type);
  }
  VLOG(6) << "logical_and kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_and", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_and infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_and compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor logical_not(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_not API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_not", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_not", kernel_data_type);
  }
  VLOG(6) << "logical_not kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_not", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_not infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalNotInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_not compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& logical_not_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_not API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_not", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_not", kernel_data_type);
  }
  VLOG(6) << "logical_not kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_not", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_not infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalNotInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_not compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor logical_or(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_or API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_or", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_or", kernel_data_type);
  }
  VLOG(6) << "logical_or kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_or", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_or infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_or compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& logical_or_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_or API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_or", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_or", kernel_data_type);
  }
  VLOG(6) << "logical_or kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_or", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_or infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_or compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor logical_xor(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_xor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_xor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_xor", kernel_data_type);
  }
  VLOG(6) << "logical_xor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_xor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_xor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_xor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& logical_xor_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_xor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_xor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logical_xor", kernel_data_type);
  }
  VLOG(6) << "logical_xor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logical_xor", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logical_xor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogicalBinaryInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logical_xor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor logit(const Tensor& x, float eps) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logit API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logit", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logit", kernel_data_type);
  }
  VLOG(6) << "logit kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["eps"] = eps;
     phi::RecordOpInfoSupplement("logit", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logit infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logit compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, eps, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& logit_(Tensor& x, float eps) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logit API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logit", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logit", kernel_data_type);
  }
  VLOG(6) << "logit kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["eps"] = eps;
     phi::RecordOpInfoSupplement("logit", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logit infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logit compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, eps, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor logsigmoid(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logsigmoid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsigmoid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logsigmoid", kernel_data_type);
  }
  VLOG(6) << "logsigmoid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logsigmoid", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logsigmoid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logsigmoid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor logspace(const Tensor& start, const Tensor& stop, const Tensor& num, const Tensor& base, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(start, stop, num, base);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logspace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logspace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logspace", kernel_data_type);
  }
  VLOG(6) << "logspace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_start = PrepareData(start, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_stop = PrepareData(stop, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_num = PrepareData(num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_base = PrepareData(base, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"start", {
     (*input_start).dims()}},
     {"stop", {
     (*input_stop).dims()}},
     {"num", {
     (*input_num).dims()}},
     {"base", {
     (*input_base).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logspace", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logspace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogspaceInferMeta(MakeMetaTensor(*input_start), MakeMetaTensor(*input_stop), MakeMetaTensor(*input_num), MakeMetaTensor(*input_base), dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logspace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_start, *input_stop, *input_num, *input_base, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor logsumexp(const Tensor& x, const std::vector<int>& axis, bool keepdim, bool reduce_all) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logsumexp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsumexp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logsumexp", kernel_data_type);
  }
  VLOG(6) << "logsumexp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("logsumexp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logsumexp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LogsumexpInferMeta(MakeMetaTensor(*input_x), axis, keepdim, reduce_all, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logsumexp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor lookup_table_dequant(const Tensor& w, const Tensor& ids, int64_t padding_idx) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(w);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(w, ids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lookup_table_dequant API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lookup_table_dequant", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lookup_table_dequant", kernel_data_type);
  }
  VLOG(6) << "lookup_table_dequant kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_ids = PrepareData(ids, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"w", {
     (*input_w).dims()}},
     {"ids", {
     (*input_ids).dims()}}};
     phi::AttributeMap attrs;
     attrs["padding_idx"] = padding_idx;
     phi::RecordOpInfoSupplement("lookup_table_dequant", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lookup_table_dequant infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LookupTableDequantInferMeta(MakeMetaTensor(*input_w), MakeMetaTensor(*input_ids), padding_idx, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lookup_table_dequant compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_w, *input_ids, padding_idx, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor lp_pool2d(const Tensor& x, const IntArray& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm, float norm_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lp_pool2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lp_pool2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lp_pool2d", kernel_data_type);
  }
  VLOG(6) << "lp_pool2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size.GetData();
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["norm_type"] = norm_type;
     phi::RecordOpInfoSupplement("lp_pool2d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lp_pool2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Pool2DInferMeta(MakeMetaTensor(*input_x), kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lp_pool2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(kernel_size), strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, norm_type, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> lstm(const Tensor& input, const paddle::optional<Tensor>& h0, const paddle::optional<Tensor>& c0, const Tensor& weight, const Tensor& bias, bool use_peepholes, bool is_reverse, bool is_test, const std::string& gate_activation, const std::string& cell_activation, const std::string& candidate_activation) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, h0, c0, weight, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lstm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lstm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lstm", kernel_data_type);
  }
  VLOG(6) << "lstm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_h0 = PrepareData(h0, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_c0 = PrepareData(c0, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> h0_record_shapes;
     if(input_h0){
       h0_record_shapes.push_back((*input_h0).dims());
     }
     std::vector<phi::DDim> c0_record_shapes;
     if(input_c0){
       c0_record_shapes.push_back((*input_c0).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"h0", h0_record_shapes},
     {"c0", c0_record_shapes},
     {"weight", {
     (*input_weight).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["use_peepholes"] = use_peepholes;
     attrs["is_reverse"] = is_reverse;
     attrs["is_test"] = is_test;
     attrs["gate_activation"] = gate_activation;
     attrs["cell_activation"] = cell_activation;
     attrs["candidate_activation"] = candidate_activation;
     phi::RecordOpInfoSupplement("lstm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lstm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::LSTMInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(input_h0), MakeMetaTensor(input_c0), MakeMetaTensor(*input_weight), MakeMetaTensor(*input_bias), use_peepholes, is_reverse, is_test, gate_activation, cell_activation, candidate_activation, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, const std::string&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lstm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, input_h0, input_c0, *input_weight, *input_bias, use_peepholes, is_reverse, is_test, gate_activation, cell_activation, candidate_activation, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> lstsq(const Tensor& x, const Tensor& y, const Scalar& rcond, const std::string& driver) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lstsq API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lstsq", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lstsq", kernel_data_type);
  }
  VLOG(6) << "lstsq kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
    switch (rcond.dtype()) {
      case DataType::FLOAT32:
          attrs["rcond"] = static_cast<float>(rcond.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["rcond"] = static_cast<double>(rcond.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["rcond"] = static_cast<float>(rcond.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["rcond"] = static_cast<float>(rcond.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["rcond"] = static_cast<int32_t>(rcond.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["rcond"] = static_cast<int64_t>(rcond.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["rcond"] = static_cast<int16_t>(rcond.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["rcond"] = static_cast<int8_t>(rcond.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["rcond"] = static_cast<uint16_t>(rcond.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["rcond"] = static_cast<uint8_t>(rcond.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["rcond"] = static_cast<bool>(rcond.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["rcond"] = static_cast<float>(rcond.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["rcond"] = static_cast<double>(rcond.to<complex128>());
          break;
      default:
          attrs["rcond"] = "";
          break;
    }
     attrs["driver"] = driver;
     phi::RecordOpInfoSupplement("lstsq", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lstsq infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::LstsqInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), rcond, driver, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lstsq compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, phi::Scalar(rcond), driver, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> lu(const Tensor& x, bool pivot) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lu", kernel_data_type);
  }
  VLOG(6) << "lu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["pivot"] = pivot;
     phi::RecordOpInfoSupplement("lu", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::LUInferMeta(MakeMetaTensor(*input_x), pivot, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, pivot, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor, Tensor> lu_(Tensor& x, bool pivot) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lu", kernel_data_type);
  }
  VLOG(6) << "lu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["pivot"] = pivot;
     phi::RecordOpInfoSupplement("lu", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor, Tensor> api_output{x, Tensor(), Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::LUInferMeta(MakeMetaTensor(origin_input_x), pivot, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, pivot, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> lu_unpack(const Tensor& x, const Tensor& y, bool unpack_ludata, bool unpack_pivots) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lu_unpack API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu_unpack", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lu_unpack", kernel_data_type);
  }
  VLOG(6) << "lu_unpack kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["unpack_ludata"] = unpack_ludata;
     attrs["unpack_pivots"] = unpack_pivots;
     phi::RecordOpInfoSupplement("lu_unpack", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lu_unpack infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::LUUnpackInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), unpack_ludata, unpack_pivots, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lu_unpack compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, unpack_ludata, unpack_pivots, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> margin_cross_entropy(const Tensor& logits, const Tensor& label, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(logits);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(logits, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "margin_cross_entropy API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "margin_cross_entropy", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("margin_cross_entropy", kernel_data_type);
  }
  VLOG(6) << "margin_cross_entropy kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_logits = PrepareData(logits, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"logits", {
     (*input_logits).dims()}},
     {"label", {
     (*input_label).dims()}}};
     phi::AttributeMap attrs;
     attrs["return_softmax"] = return_softmax;
     attrs["ring_id"] = ring_id;
     attrs["rank"] = rank;
     attrs["nranks"] = nranks;
     attrs["margin1"] = margin1;
     attrs["margin2"] = margin2;
     attrs["margin3"] = margin3;
     attrs["scale"] = scale;
     phi::RecordOpInfoSupplement("margin_cross_entropy", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("margin_cross_entropy infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::MarginCrossEntropyInferMeta(MakeMetaTensor(*input_logits), MakeMetaTensor(*input_label), return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, int, int, int, float, float, float, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("margin_cross_entropy compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_logits, *input_label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, paddle::optional<Tensor>&> masked_multihead_attention_(const Tensor& x, Tensor& cache_kv, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& src_mask, const paddle::optional<Tensor>& cum_offsets, const paddle::optional<Tensor>& sequence_lengths, const paddle::optional<Tensor>& rotary_tensor, paddle::optional<Tensor>& beam_cache_offset, const paddle::optional<Tensor>& qkv_out_scale, const paddle::optional<Tensor>& out_shift, const paddle::optional<Tensor>& out_smooth, int seq_len, int rotary_emb_dims, bool use_neox_rotary_style, const std::string& compute_dtype, float out_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, cache_kv, bias, src_mask, cum_offsets, sequence_lengths, rotary_tensor, beam_cache_offset, qkv_out_scale, out_shift, out_smooth);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "masked_multihead_attention_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "masked_multihead_attention", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("masked_multihead_attention_", kernel_data_type);
  }
  VLOG(6) << "masked_multihead_attention kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cache_kv = PrepareData(cache_kv, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_mask = PrepareData(src_mask, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cum_offsets = PrepareData(cum_offsets, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_sequence_lengths = PrepareData(sequence_lengths, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rotary_tensor = PrepareData(rotary_tensor, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beam_cache_offset = PrepareData(beam_cache_offset, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_qkv_out_scale = PrepareData(qkv_out_scale, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_shift = PrepareData(out_shift, GetKernelInputArgDef(kernel.InputAt(9), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_smooth = PrepareData(out_smooth, GetKernelInputArgDef(kernel.InputAt(10), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> src_mask_record_shapes;
     if(input_src_mask){
       src_mask_record_shapes.push_back((*input_src_mask).dims());
     }
     std::vector<phi::DDim> cum_offsets_record_shapes;
     if(input_cum_offsets){
       cum_offsets_record_shapes.push_back((*input_cum_offsets).dims());
     }
     std::vector<phi::DDim> sequence_lengths_record_shapes;
     if(input_sequence_lengths){
       sequence_lengths_record_shapes.push_back((*input_sequence_lengths).dims());
     }
     std::vector<phi::DDim> rotary_tensor_record_shapes;
     if(input_rotary_tensor){
       rotary_tensor_record_shapes.push_back((*input_rotary_tensor).dims());
     }
     std::vector<phi::DDim> beam_cache_offset_record_shapes;
     if(input_beam_cache_offset){
       beam_cache_offset_record_shapes.push_back((*input_beam_cache_offset).dims());
     }
     std::vector<phi::DDim> qkv_out_scale_record_shapes;
     if(input_qkv_out_scale){
       qkv_out_scale_record_shapes.push_back((*input_qkv_out_scale).dims());
     }
     std::vector<phi::DDim> out_shift_record_shapes;
     if(input_out_shift){
       out_shift_record_shapes.push_back((*input_out_shift).dims());
     }
     std::vector<phi::DDim> out_smooth_record_shapes;
     if(input_out_smooth){
       out_smooth_record_shapes.push_back((*input_out_smooth).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"cache_kv", {
     (*input_cache_kv).dims()}},
     {"bias", bias_record_shapes},
     {"src_mask", src_mask_record_shapes},
     {"cum_offsets", cum_offsets_record_shapes},
     {"sequence_lengths", sequence_lengths_record_shapes},
     {"rotary_tensor", rotary_tensor_record_shapes},
     {"beam_cache_offset", beam_cache_offset_record_shapes},
     {"qkv_out_scale", qkv_out_scale_record_shapes},
     {"out_shift", out_shift_record_shapes},
     {"out_smooth",
     out_smooth_record_shapes}};
     phi::AttributeMap attrs;
     attrs["seq_len"] = seq_len;
     attrs["rotary_emb_dims"] = rotary_emb_dims;
     attrs["use_neox_rotary_style"] = use_neox_rotary_style;
     attrs["compute_dtype"] = compute_dtype;
     attrs["out_scale"] = out_scale;
     attrs["quant_round_type"] = quant_round_type;
     attrs["quant_max_bound"] = quant_max_bound;
     attrs["quant_min_bound"] = quant_min_bound;
     phi::RecordOpInfoSupplement("masked_multihead_attention_", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor&, paddle::optional<Tensor>&> api_output{Tensor(), cache_kv, beam_cache_offset};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("masked_multihead_attention_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_cache_kv = *input_cache_kv;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::MaskedMultiheadAttentionInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(origin_input_cache_kv), MakeMetaTensor(input_bias), MakeMetaTensor(input_src_mask), MakeMetaTensor(input_cum_offsets), MakeMetaTensor(input_sequence_lengths), MakeMetaTensor(input_rotary_tensor), MakeMetaTensor(input_beam_cache_offset), MakeMetaTensor(input_qkv_out_scale), MakeMetaTensor(input_out_shift), MakeMetaTensor(input_out_smooth), seq_len, rotary_emb_dims, use_neox_rotary_style, compute_dtype, out_scale, quant_round_type, quant_max_bound, quant_min_bound, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, int, bool, const std::string&, float, int, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("masked_multihead_attention_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, origin_input_cache_kv, input_bias, input_src_mask, input_cum_offsets, input_sequence_lengths, input_rotary_tensor, input_beam_cache_offset, input_qkv_out_scale, input_out_shift, input_out_smooth, seq_len, rotary_emb_dims, use_neox_rotary_style, compute_dtype, out_scale, quant_round_type, quant_max_bound, quant_min_bound, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);

  return api_output;
}

PADDLE_API Tensor masked_select(const Tensor& x, const Tensor& mask) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "masked_select API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "masked_select", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("masked_select", kernel_data_type);
  }
  VLOG(6) << "masked_select kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("masked_select", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("masked_select infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MaskedSelectInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("masked_select compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> match_matrix_tensor(const Tensor& x, const Tensor& y, const Tensor& w, int dim_t) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, w);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "match_matrix_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "match_matrix_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("match_matrix_tensor", kernel_data_type);
  }
  VLOG(6) << "match_matrix_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"w", {
     (*input_w).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim_t"] = dim_t;
     phi::RecordOpInfoSupplement("match_matrix_tensor", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("match_matrix_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::MatchMatrixTensorInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_w), dim_t, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("match_matrix_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_w, dim_t, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> matrix_nms(const Tensor& bboxes, const Tensor& scores, float score_threshold, int nms_top_k, int keep_top_k, float post_threshold, bool use_gaussian, float gaussian_sigma, int background_label, bool normalized) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(bboxes, scores);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_nms API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_nms", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matrix_nms", kernel_data_type);
  }
  VLOG(6) << "matrix_nms kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_bboxes = PrepareData(bboxes, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scores = PrepareData(scores, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"bboxes", {
     (*input_bboxes).dims()}},
     {"scores", {
     (*input_scores).dims()}}};
     phi::AttributeMap attrs;
     attrs["score_threshold"] = score_threshold;
     attrs["nms_top_k"] = nms_top_k;
     attrs["keep_top_k"] = keep_top_k;
     attrs["post_threshold"] = post_threshold;
     attrs["use_gaussian"] = use_gaussian;
     attrs["gaussian_sigma"] = gaussian_sigma;
     attrs["background_label"] = background_label;
     attrs["normalized"] = normalized;
     phi::RecordOpInfoSupplement("matrix_nms", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matrix_nms infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::MatrixNMSInferMeta(MakeMetaTensor(*input_bboxes), MakeMetaTensor(*input_scores), score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, int, float, bool, float, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matrix_nms compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_bboxes, *input_scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor matrix_power(const Tensor& x, int n) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_power API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_power", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matrix_power", kernel_data_type);
  }
  VLOG(6) << "matrix_power kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("matrix_power", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matrix_power infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MatrixPowerInferMeta(MakeMetaTensor(*input_x), n, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matrix_power compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, n, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor matrix_rank(const Tensor& x, float tol, bool use_default_tol, bool hermitian) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_rank API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_rank", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matrix_rank", kernel_data_type);
  }
  VLOG(6) << "matrix_rank kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["tol"] = tol;
     attrs["use_default_tol"] = use_default_tol;
     attrs["hermitian"] = hermitian;
     phi::RecordOpInfoSupplement("matrix_rank", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matrix_rank infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MatrixRankInferMeta(MakeMetaTensor(*input_x), use_default_tol, hermitian, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matrix_rank compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, tol, use_default_tol, hermitian, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor matrix_rank_tol(const Tensor& x, const Tensor& atol_tensor, bool use_default_tol, bool hermitian) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, atol_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_rank_tol API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_rank_tol", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matrix_rank_tol", kernel_data_type);
  }
  VLOG(6) << "matrix_rank_tol kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_atol_tensor = PrepareData(atol_tensor, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"atol_tensor", {
     (*input_atol_tensor).dims()}}};
     phi::AttributeMap attrs;
     attrs["use_default_tol"] = use_default_tol;
     attrs["hermitian"] = hermitian;
     phi::RecordOpInfoSupplement("matrix_rank_tol", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matrix_rank_tol infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MatrixRankTolInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_atol_tensor), use_default_tol, hermitian, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matrix_rank_tol compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_atol_tensor, use_default_tol, hermitian, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor max(const Tensor& x, const IntArray& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max", kernel_data_type);
  }
  VLOG(6) << "max kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("max", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceIntArrayAxisInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> max_pool2d_with_index(const Tensor& x, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive, bool ceil_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max_pool2d_with_index API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool2d_with_index", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max_pool2d_with_index", kernel_data_type);
  }
  VLOG(6) << "max_pool2d_with_index kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["ceil_mode"] = ceil_mode;
     phi::RecordOpInfoSupplement("max_pool2d_with_index", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max_pool2d_with_index infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::MaxPoolWithIndexInferMeta(MakeMetaTensor(*input_x), kernel_size, strides, paddings, global_pooling, adaptive, ceil_mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max_pool2d_with_index compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_size, strides, paddings, global_pooling, adaptive, ceil_mode, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> max_pool3d_with_index(const Tensor& x, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive, bool ceil_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max_pool3d_with_index API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool3d_with_index", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max_pool3d_with_index", kernel_data_type);
  }
  VLOG(6) << "max_pool3d_with_index kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["ceil_mode"] = ceil_mode;
     phi::RecordOpInfoSupplement("max_pool3d_with_index", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max_pool3d_with_index infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::MaxPoolWithIndexInferMeta(MakeMetaTensor(*input_x), kernel_size, strides, paddings, global_pooling, adaptive, ceil_mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max_pool3d_with_index compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_size, strides, paddings, global_pooling, adaptive, ceil_mode, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor maxout(const Tensor& x, int groups, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "maxout API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maxout", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("maxout", kernel_data_type);
  }
  VLOG(6) << "maxout kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["groups"] = groups;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("maxout", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("maxout infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MaxOutInferMeta(MakeMetaTensor(*input_x), groups, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("maxout compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, groups, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor mean(const Tensor& x, const IntArray& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mean API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mean", kernel_data_type);
  }
  VLOG(6) << "mean kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("mean", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mean infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceIntArrayAxisInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mean compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor mean_all(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mean_all API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean_all", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mean_all", kernel_data_type);
  }
  VLOG(6) << "mean_all kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("mean_all", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mean_all infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MeanAllInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mean_all compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor memcpy_d2h(const Tensor& x, int dst_place_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "memcpy_d2h API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "memcpy_d2h", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("memcpy_d2h", kernel_data_type);
  }
  VLOG(6) << "memcpy_d2h kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["dst_place_type"] = dst_place_type;
     phi::RecordOpInfoSupplement("memcpy_d2h", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("memcpy_d2h infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("memcpy_d2h compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, dst_place_type, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor memcpy_h2d(const Tensor& x, int dst_place_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "memcpy_h2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "memcpy_h2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("memcpy_h2d", kernel_data_type);
  }
  VLOG(6) << "memcpy_h2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["dst_place_type"] = dst_place_type;
     phi::RecordOpInfoSupplement("memcpy_h2d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("memcpy_h2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("memcpy_h2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, dst_place_type, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> memory_efficient_attention(const Tensor& query, const Tensor& key, const Tensor& value, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& cu_seqlens_q, const paddle::optional<Tensor>& cu_seqlens_k, const paddle::optional<Tensor>& causal_diagonal, const paddle::optional<Tensor>& seqlen_k, const Scalar& max_seqlen_q, const Scalar& max_seqlen_k, bool causal, double dropout_p, float scale, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(query);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(query, key, value, bias, cu_seqlens_q, cu_seqlens_k, causal_diagonal, seqlen_k);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "memory_efficient_attention API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "memory_efficient_attention", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("memory_efficient_attention", kernel_data_type);
  }
  VLOG(6) << "memory_efficient_attention kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_query = PrepareData(query, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_key = PrepareData(key, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_q = PrepareData(cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_k = PrepareData(cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_causal_diagonal = PrepareData(causal_diagonal, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seqlen_k = PrepareData(seqlen_k, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> cu_seqlens_q_record_shapes;
     if(input_cu_seqlens_q){
       cu_seqlens_q_record_shapes.push_back((*input_cu_seqlens_q).dims());
     }
     std::vector<phi::DDim> cu_seqlens_k_record_shapes;
     if(input_cu_seqlens_k){
       cu_seqlens_k_record_shapes.push_back((*input_cu_seqlens_k).dims());
     }
     std::vector<phi::DDim> causal_diagonal_record_shapes;
     if(input_causal_diagonal){
       causal_diagonal_record_shapes.push_back((*input_causal_diagonal).dims());
     }
     std::vector<phi::DDim> seqlen_k_record_shapes;
     if(input_seqlen_k){
       seqlen_k_record_shapes.push_back((*input_seqlen_k).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"query", {
     (*input_query).dims()}},
     {"key", {
     (*input_key).dims()}},
     {"value", {
     (*input_value).dims()}},
     {"bias", bias_record_shapes},
     {"cu_seqlens_q", cu_seqlens_q_record_shapes},
     {"cu_seqlens_k", cu_seqlens_k_record_shapes},
     {"causal_diagonal", causal_diagonal_record_shapes},
     {"seqlen_k",
     seqlen_k_record_shapes}};
     phi::AttributeMap attrs;
    switch (max_seqlen_q.dtype()) {
      case DataType::FLOAT32:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max_seqlen_q"] = static_cast<int32_t>(max_seqlen_q.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max_seqlen_q"] = static_cast<int64_t>(max_seqlen_q.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max_seqlen_q"] = static_cast<int16_t>(max_seqlen_q.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max_seqlen_q"] = static_cast<int8_t>(max_seqlen_q.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max_seqlen_q"] = static_cast<uint16_t>(max_seqlen_q.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max_seqlen_q"] = static_cast<uint8_t>(max_seqlen_q.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max_seqlen_q"] = static_cast<bool>(max_seqlen_q.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<complex128>());
          break;
      default:
          attrs["max_seqlen_q"] = "";
          break;
    }
    switch (max_seqlen_k.dtype()) {
      case DataType::FLOAT32:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max_seqlen_k"] = static_cast<int32_t>(max_seqlen_k.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max_seqlen_k"] = static_cast<int64_t>(max_seqlen_k.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max_seqlen_k"] = static_cast<int16_t>(max_seqlen_k.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max_seqlen_k"] = static_cast<int8_t>(max_seqlen_k.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max_seqlen_k"] = static_cast<uint16_t>(max_seqlen_k.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max_seqlen_k"] = static_cast<uint8_t>(max_seqlen_k.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max_seqlen_k"] = static_cast<bool>(max_seqlen_k.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<complex128>());
          break;
      default:
          attrs["max_seqlen_k"] = "";
          break;
    }
     attrs["causal"] = causal;
     attrs["dropout_p"] = dropout_p;
     attrs["scale"] = scale;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("memory_efficient_attention", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("memory_efficient_attention infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::MemoryEfficientAttentionInferMeta(MakeMetaTensor(*input_query), MakeMetaTensor(*input_key), MakeMetaTensor(*input_value), MakeMetaTensor(input_bias), MakeMetaTensor(input_cu_seqlens_q), MakeMetaTensor(input_cu_seqlens_k), MakeMetaTensor(input_causal_diagonal), MakeMetaTensor(input_seqlen_k), max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, const phi::Scalar&, bool, double, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("memory_efficient_attention compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_query, *input_key, *input_value, input_bias, input_cu_seqlens_q, input_cu_seqlens_k, input_causal_diagonal, input_seqlen_k, phi::Scalar(max_seqlen_q), phi::Scalar(max_seqlen_k), causal, dropout_p, scale, is_test, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor merge_selected_rows(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "merge_selected_rows API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "merge_selected_rows", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("merge_selected_rows", kernel_data_type);
  }
  VLOG(6) << "merge_selected_rows kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("merge_selected_rows", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("merge_selected_rows infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("merge_selected_rows compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> merged_adam_(std::vector<Tensor>& param, const std::vector<Tensor>& grad, const std::vector<Tensor>& learning_rate, std::vector<Tensor>& moment1, std::vector<Tensor>& moment2, std::vector<Tensor>& beta1_pow, std::vector<Tensor>& beta2_pow, paddle::optional<std::vector<Tensor>>& master_param, const Scalar& beta1, const Scalar& beta2, const Scalar& epsilon, bool multi_precision, bool use_global_beta_pow) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, moment1, moment2, beta1_pow, beta2_pow, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "merged_adam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "merged_adam", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("merged_adam_", kernel_data_type);
  }
  VLOG(6) << "merged_adam kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_param;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_param_vec;
  if (kernel_result.has_fallback_cpu) {
    input_param_vec = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_param.resize(input_param_vec->size());
    for (size_t i = 0; i < input_param.size(); ++i) {
      input_param[i] = &input_param_vec->at(i);
    }
  }
  else {
    input_param = TensorToConstDenseTensorPtr(param);
  }
  auto input_grad_vec = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_grad(input_grad_vec->size());
  for (size_t i = 0; i < input_grad.size(); ++i) {
    input_grad[i] = &input_grad_vec->at(i);
  }
  auto input_learning_rate_vec = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_learning_rate(input_learning_rate_vec->size());
  for (size_t i = 0; i < input_learning_rate.size(); ++i) {
    input_learning_rate[i] = &input_learning_rate_vec->at(i);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_moment1;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_moment1_vec;
  if (kernel_result.has_fallback_cpu) {
    input_moment1_vec = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_moment1.resize(input_moment1_vec->size());
    for (size_t i = 0; i < input_moment1.size(); ++i) {
      input_moment1[i] = &input_moment1_vec->at(i);
    }
  }
  else {
    input_moment1 = TensorToConstDenseTensorPtr(moment1);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_moment2;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_moment2_vec;
  if (kernel_result.has_fallback_cpu) {
    input_moment2_vec = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_moment2.resize(input_moment2_vec->size());
    for (size_t i = 0; i < input_moment2.size(); ++i) {
      input_moment2[i] = &input_moment2_vec->at(i);
    }
  }
  else {
    input_moment2 = TensorToConstDenseTensorPtr(moment2);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_beta1_pow;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_beta1_pow_vec;
  if (kernel_result.has_fallback_cpu) {
    input_beta1_pow_vec = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_beta1_pow.resize(input_beta1_pow_vec->size());
    for (size_t i = 0; i < input_beta1_pow.size(); ++i) {
      input_beta1_pow[i] = &input_beta1_pow_vec->at(i);
    }
  }
  else {
    input_beta1_pow = TensorToConstDenseTensorPtr(beta1_pow);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_beta2_pow;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_beta2_pow_vec;
  if (kernel_result.has_fallback_cpu) {
    input_beta2_pow_vec = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_beta2_pow.resize(input_beta2_pow_vec->size());
    for (size_t i = 0; i < input_beta2_pow.size(); ++i) {
      input_beta2_pow[i] = &input_beta2_pow_vec->at(i);
    }
  }
  else {
    input_beta2_pow = TensorToConstDenseTensorPtr(beta2_pow);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  paddle::optional<std::vector<const phi::DenseTensor*>> input_master_param;
  paddle::optional<std::vector<phi::DenseTensor>> input_master_param_vec;
  if (kernel_result.has_fallback_cpu) {
    input_master_param_vec = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if (input_master_param_vec){
      input_master_param = paddle::optional<std::vector<const phi::DenseTensor*>>(input_master_param_vec->size());
      for (size_t i = 0; i < input_master_param_vec->size(); ++i) {
        input_master_param->at(i) = &input_master_param_vec->at(i);
      }
    }
  }
  else {
    input_master_param = TensorToConstDenseTensorPtr(master_param);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_param.size());
     for (size_t i = 0; i < input_param.size(); ++i) {
       ddims_vec.emplace_back((*input_param[i]).dims());
     }
     input_shapes.emplace_back("param", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_grad.size());
     for (size_t i = 0; i < input_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_grad[i]).dims());
     }
     input_shapes.emplace_back("grad", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_learning_rate.size());
     for (size_t i = 0; i < input_learning_rate.size(); ++i) {
       ddims_vec.emplace_back((*input_learning_rate[i]).dims());
     }
     input_shapes.emplace_back("learning_rate", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_moment1.size());
     for (size_t i = 0; i < input_moment1.size(); ++i) {
       ddims_vec.emplace_back((*input_moment1[i]).dims());
     }
     input_shapes.emplace_back("moment1", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_moment2.size());
     for (size_t i = 0; i < input_moment2.size(); ++i) {
       ddims_vec.emplace_back((*input_moment2[i]).dims());
     }
     input_shapes.emplace_back("moment2", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_beta1_pow.size());
     for (size_t i = 0; i < input_beta1_pow.size(); ++i) {
       ddims_vec.emplace_back((*input_beta1_pow[i]).dims());
     }
     input_shapes.emplace_back("beta1_pow", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_beta2_pow.size());
     for (size_t i = 0; i < input_beta2_pow.size(); ++i) {
       ddims_vec.emplace_back((*input_beta2_pow[i]).dims());
     }
     input_shapes.emplace_back("beta2_pow", ddims_vec);
     ddims_vec.clear();
     if (input_master_param){
       ddims_vec.reserve(input_master_param->size());
       for (size_t i = 0; i < input_master_param->size(); ++i) {
         ddims_vec.emplace_back((*input_master_param->at(i)).dims());
       }
     }
     input_shapes.emplace_back("master_param", ddims_vec);
     phi::AttributeMap attrs;
    switch (beta1.dtype()) {
      case DataType::FLOAT32:
          attrs["beta1"] = static_cast<float>(beta1.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta1"] = static_cast<double>(beta1.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta1"] = static_cast<int32_t>(beta1.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta1"] = static_cast<int64_t>(beta1.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta1"] = static_cast<int16_t>(beta1.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta1"] = static_cast<int8_t>(beta1.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta1"] = static_cast<uint16_t>(beta1.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta1"] = static_cast<uint8_t>(beta1.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta1"] = static_cast<bool>(beta1.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta1"] = static_cast<float>(beta1.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta1"] = static_cast<double>(beta1.to<complex128>());
          break;
      default:
          attrs["beta1"] = "";
          break;
    }
    switch (beta2.dtype()) {
      case DataType::FLOAT32:
          attrs["beta2"] = static_cast<float>(beta2.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta2"] = static_cast<double>(beta2.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta2"] = static_cast<int32_t>(beta2.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta2"] = static_cast<int64_t>(beta2.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta2"] = static_cast<int16_t>(beta2.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta2"] = static_cast<int8_t>(beta2.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta2"] = static_cast<uint16_t>(beta2.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta2"] = static_cast<uint8_t>(beta2.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta2"] = static_cast<bool>(beta2.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta2"] = static_cast<float>(beta2.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta2"] = static_cast<double>(beta2.to<complex128>());
          break;
      default:
          attrs["beta2"] = "";
          break;
    }
    switch (epsilon.dtype()) {
      case DataType::FLOAT32:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["epsilon"] = static_cast<double>(epsilon.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["epsilon"] = static_cast<int32_t>(epsilon.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["epsilon"] = static_cast<int64_t>(epsilon.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["epsilon"] = static_cast<int16_t>(epsilon.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["epsilon"] = static_cast<int8_t>(epsilon.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["epsilon"] = static_cast<uint16_t>(epsilon.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["epsilon"] = static_cast<uint8_t>(epsilon.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["epsilon"] = static_cast<bool>(epsilon.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["epsilon"] = static_cast<float>(epsilon.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["epsilon"] = static_cast<double>(epsilon.to<complex128>());
          break;
      default:
          attrs["epsilon"] = "";
          break;
    }
     attrs["multi_precision"] = multi_precision;
     attrs["use_global_beta_pow"] = use_global_beta_pow;
     phi::RecordOpInfoSupplement("merged_adam_", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
  auto kernel_out_0 = SetInplaceVectorKernelOutput(param.size(), &std::get<0>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_0.size(); ++i) {
      kernel_out_0[i] = const_cast<phi::DenseTensor*>(input_param[i]);
    }
  }
  auto kernel_out_1 = SetInplaceVectorKernelOutput(param.size(), &std::get<1>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_1.size(); ++i) {
      kernel_out_1[i] = const_cast<phi::DenseTensor*>(input_moment1[i]);
    }
  }
  auto kernel_out_2 = SetInplaceVectorKernelOutput(param.size(), &std::get<2>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_2.size(); ++i) {
      kernel_out_2[i] = const_cast<phi::DenseTensor*>(input_moment2[i]);
    }
  }
  auto kernel_out_3 = SetInplaceVectorKernelOutput(param.size(), &std::get<3>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_3.size(); ++i) {
      kernel_out_3[i] = const_cast<phi::DenseTensor*>(input_beta1_pow[i]);
    }
  }
  auto kernel_out_4 = SetInplaceVectorKernelOutput(param.size(), &std::get<4>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_4.size(); ++i) {
      kernel_out_4[i] = const_cast<phi::DenseTensor*>(input_beta2_pow[i]);
    }
  }
  auto kernel_out_5 = SetInplaceOptionalVectorKernelOutput(param.size(), std::get<5>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_5.size(); ++i) {
      kernel_out_5[i] = const_cast<phi::DenseTensor*>(input_master_param->at(i));
    }
  }
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("merged_adam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto param_meta_vec = MakeMetaTensor(input_param);
  std::vector<const phi::MetaTensor*> param_metas(param_meta_vec.size());
  for (size_t i = 0; i < param_meta_vec.size(); ++i) {
    param_metas[i] = &param_meta_vec[i];
  }

  auto grad_meta_vec = MakeMetaTensor(input_grad);
  std::vector<const phi::MetaTensor*> grad_metas(grad_meta_vec.size());
  for (size_t i = 0; i < grad_meta_vec.size(); ++i) {
    grad_metas[i] = &grad_meta_vec[i];
  }

  auto learning_rate_meta_vec = MakeMetaTensor(input_learning_rate);
  std::vector<const phi::MetaTensor*> learning_rate_metas(learning_rate_meta_vec.size());
  for (size_t i = 0; i < learning_rate_meta_vec.size(); ++i) {
    learning_rate_metas[i] = &learning_rate_meta_vec[i];
  }

  auto moment1_meta_vec = MakeMetaTensor(input_moment1);
  std::vector<const phi::MetaTensor*> moment1_metas(moment1_meta_vec.size());
  for (size_t i = 0; i < moment1_meta_vec.size(); ++i) {
    moment1_metas[i] = &moment1_meta_vec[i];
  }

  auto moment2_meta_vec = MakeMetaTensor(input_moment2);
  std::vector<const phi::MetaTensor*> moment2_metas(moment2_meta_vec.size());
  for (size_t i = 0; i < moment2_meta_vec.size(); ++i) {
    moment2_metas[i] = &moment2_meta_vec[i];
  }

  auto beta1_pow_meta_vec = MakeMetaTensor(input_beta1_pow);
  std::vector<const phi::MetaTensor*> beta1_pow_metas(beta1_pow_meta_vec.size());
  for (size_t i = 0; i < beta1_pow_meta_vec.size(); ++i) {
    beta1_pow_metas[i] = &beta1_pow_meta_vec[i];
  }

  auto beta2_pow_meta_vec = MakeMetaTensor(input_beta2_pow);
  std::vector<const phi::MetaTensor*> beta2_pow_metas(beta2_pow_meta_vec.size());
  for (size_t i = 0; i < beta2_pow_meta_vec.size(); ++i) {
    beta2_pow_metas[i] = &beta2_pow_meta_vec[i];
  }

  auto master_param_meta_vec = MakeMetaTensor(input_master_param);
  paddle::optional<std::vector<const phi::MetaTensor*>> master_param_metas(master_param_meta_vec.size());
  for (size_t i = 0; i < master_param_meta_vec.size(); ++i) {
    master_param_metas->at(i) = &master_param_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  auto kernel_out_3_meta_vec = MakeMetaTensor(kernel_out_3);
  std::vector<phi::MetaTensor*> kernel_out_3_metas(kernel_out_3_meta_vec.size());
  for (size_t i = 0; i < kernel_out_3_meta_vec.size(); ++i) {
    kernel_out_3_metas[i] = kernel_out_3[i] ? &kernel_out_3_meta_vec[i] : nullptr;
  }
  auto kernel_out_4_meta_vec = MakeMetaTensor(kernel_out_4);
  std::vector<phi::MetaTensor*> kernel_out_4_metas(kernel_out_4_meta_vec.size());
  for (size_t i = 0; i < kernel_out_4_meta_vec.size(); ++i) {
    kernel_out_4_metas[i] = kernel_out_4[i] ? &kernel_out_4_meta_vec[i] : nullptr;
  }
  auto kernel_out_5_meta_vec = MakeMetaTensor(kernel_out_5);
  std::vector<phi::MetaTensor*> kernel_out_5_metas(kernel_out_5_meta_vec.size());
  for (size_t i = 0; i < kernel_out_5_meta_vec.size(); ++i) {
    kernel_out_5_metas[i] = kernel_out_5[i] ? &kernel_out_5_meta_vec[i] : nullptr;
  }
  phi::MergedAdamInferMeta(param_metas, grad_metas, learning_rate_metas, moment1_metas, moment2_metas, beta1_pow_metas, beta2_pow_metas, master_param_metas, beta1, beta2, epsilon, multi_precision, use_global_beta_pow, kernel_out_0_metas, kernel_out_1_metas, kernel_out_2_metas, kernel_out_3_metas, kernel_out_4_metas, kernel_out_5_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const phi::Scalar&, const phi::Scalar&, const phi::Scalar&, bool, bool, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("merged_adam_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_param, input_grad, input_learning_rate, input_moment1, input_moment2, input_beta1_pow, input_beta2_pow, input_master_param, phi::Scalar(beta1), phi::Scalar(beta2), phi::Scalar(epsilon), multi_precision, use_global_beta_pow, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    for (size_t i = 0; i < param.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(param.at(i).impl().get());
      *target_ptr = *kernel_out_0.at(i);
    }
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    for (size_t i = 0; i < moment1.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(moment1.at(i).impl().get());
      *target_ptr = *kernel_out_1.at(i);
    }
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    for (size_t i = 0; i < moment2.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(moment2.at(i).impl().get());
      *target_ptr = *kernel_out_2.at(i);
    }
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    for (size_t i = 0; i < beta1_pow.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(beta1_pow.at(i).impl().get());
      *target_ptr = *kernel_out_3.at(i);
    }
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    for (size_t i = 0; i < beta2_pow.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(beta2_pow.at(i).impl().get());
      *target_ptr = *kernel_out_4.at(i);
    }
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    if (master_param) {
      for (size_t i = 0; i < master_param->size(); ++i) {
        auto target_ptr = static_cast<phi::DenseTensor*>(master_param->at(i).impl().get());
        *target_ptr = *kernel_out_5.at(i);
      }
    }

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> merged_momentum_(std::vector<Tensor>& param, const std::vector<Tensor>& grad, std::vector<Tensor>& velocity, const std::vector<Tensor>& learning_rate, paddle::optional<std::vector<Tensor>>& master_param, float mu, bool use_nesterov, const std::vector<std::string>& regularization_method, const std::vector<float>& regularization_coeff, bool multi_precision, float rescale_grad) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, velocity, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "merged_momentum_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "merged_momentum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("merged_momentum_", kernel_data_type);
  }
  VLOG(6) << "merged_momentum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_param;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_param_vec;
  if (kernel_result.has_fallback_cpu) {
    input_param_vec = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_param.resize(input_param_vec->size());
    for (size_t i = 0; i < input_param.size(); ++i) {
      input_param[i] = &input_param_vec->at(i);
    }
  }
  else {
    input_param = TensorToConstDenseTensorPtr(param);
  }
  auto input_grad_vec = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_grad(input_grad_vec->size());
  for (size_t i = 0; i < input_grad.size(); ++i) {
    input_grad[i] = &input_grad_vec->at(i);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_velocity;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_velocity_vec;
  if (kernel_result.has_fallback_cpu) {
    input_velocity_vec = PrepareData(velocity, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_velocity.resize(input_velocity_vec->size());
    for (size_t i = 0; i < input_velocity.size(); ++i) {
      input_velocity[i] = &input_velocity_vec->at(i);
    }
  }
  else {
    input_velocity = TensorToConstDenseTensorPtr(velocity);
  }
  auto input_learning_rate_vec = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_learning_rate(input_learning_rate_vec->size());
  for (size_t i = 0; i < input_learning_rate.size(); ++i) {
    input_learning_rate[i] = &input_learning_rate_vec->at(i);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  paddle::optional<std::vector<const phi::DenseTensor*>> input_master_param;
  paddle::optional<std::vector<phi::DenseTensor>> input_master_param_vec;
  if (kernel_result.has_fallback_cpu) {
    input_master_param_vec = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if (input_master_param_vec){
      input_master_param = paddle::optional<std::vector<const phi::DenseTensor*>>(input_master_param_vec->size());
      for (size_t i = 0; i < input_master_param_vec->size(); ++i) {
        input_master_param->at(i) = &input_master_param_vec->at(i);
      }
    }
  }
  else {
    input_master_param = TensorToConstDenseTensorPtr(master_param);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_param.size());
     for (size_t i = 0; i < input_param.size(); ++i) {
       ddims_vec.emplace_back((*input_param[i]).dims());
     }
     input_shapes.emplace_back("param", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_grad.size());
     for (size_t i = 0; i < input_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_grad[i]).dims());
     }
     input_shapes.emplace_back("grad", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_velocity.size());
     for (size_t i = 0; i < input_velocity.size(); ++i) {
       ddims_vec.emplace_back((*input_velocity[i]).dims());
     }
     input_shapes.emplace_back("velocity", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_learning_rate.size());
     for (size_t i = 0; i < input_learning_rate.size(); ++i) {
       ddims_vec.emplace_back((*input_learning_rate[i]).dims());
     }
     input_shapes.emplace_back("learning_rate", ddims_vec);
     ddims_vec.clear();
     if (input_master_param){
       ddims_vec.reserve(input_master_param->size());
       for (size_t i = 0; i < input_master_param->size(); ++i) {
         ddims_vec.emplace_back((*input_master_param->at(i)).dims());
       }
     }
     input_shapes.emplace_back("master_param", ddims_vec);
     phi::AttributeMap attrs;
     attrs["mu"] = mu;
     attrs["use_nesterov"] = use_nesterov;
     attrs["regularization_method"] = regularization_method;
     attrs["regularization_coeff"] = regularization_coeff;
     attrs["multi_precision"] = multi_precision;
     attrs["rescale_grad"] = rescale_grad;
     phi::RecordOpInfoSupplement("merged_momentum_", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> api_output{param, velocity, master_param};
  auto kernel_out_0 = SetInplaceVectorKernelOutput(param.size(), &std::get<0>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_0.size(); ++i) {
      kernel_out_0[i] = const_cast<phi::DenseTensor*>(input_param[i]);
    }
  }
  auto kernel_out_1 = SetInplaceVectorKernelOutput(param.size(), &std::get<1>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_1.size(); ++i) {
      kernel_out_1[i] = const_cast<phi::DenseTensor*>(input_velocity[i]);
    }
  }
  auto kernel_out_2 = SetInplaceOptionalVectorKernelOutput(param.size(), std::get<2>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_2.size(); ++i) {
      kernel_out_2[i] = const_cast<phi::DenseTensor*>(input_master_param->at(i));
    }
  }
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("merged_momentum_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto param_meta_vec = MakeMetaTensor(input_param);
  std::vector<const phi::MetaTensor*> param_metas(param_meta_vec.size());
  for (size_t i = 0; i < param_meta_vec.size(); ++i) {
    param_metas[i] = &param_meta_vec[i];
  }

  auto grad_meta_vec = MakeMetaTensor(input_grad);
  std::vector<const phi::MetaTensor*> grad_metas(grad_meta_vec.size());
  for (size_t i = 0; i < grad_meta_vec.size(); ++i) {
    grad_metas[i] = &grad_meta_vec[i];
  }

  auto velocity_meta_vec = MakeMetaTensor(input_velocity);
  std::vector<const phi::MetaTensor*> velocity_metas(velocity_meta_vec.size());
  for (size_t i = 0; i < velocity_meta_vec.size(); ++i) {
    velocity_metas[i] = &velocity_meta_vec[i];
  }

  auto learning_rate_meta_vec = MakeMetaTensor(input_learning_rate);
  std::vector<const phi::MetaTensor*> learning_rate_metas(learning_rate_meta_vec.size());
  for (size_t i = 0; i < learning_rate_meta_vec.size(); ++i) {
    learning_rate_metas[i] = &learning_rate_meta_vec[i];
  }

  auto master_param_meta_vec = MakeMetaTensor(input_master_param);
  paddle::optional<std::vector<const phi::MetaTensor*>> master_param_metas(master_param_meta_vec.size());
  for (size_t i = 0; i < master_param_meta_vec.size(); ++i) {
    master_param_metas->at(i) = &master_param_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  phi::MergedMomentumInferMeta(param_metas, grad_metas, velocity_metas, learning_rate_metas, master_param_metas, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0_metas, kernel_out_1_metas, kernel_out_2_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, float, bool, const std::vector<std::string>&, const std::vector<float>&, bool, float, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("merged_momentum_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_param, input_grad, input_velocity, input_learning_rate, input_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    for (size_t i = 0; i < param.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(param.at(i).impl().get());
      *target_ptr = *kernel_out_0.at(i);
    }
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    for (size_t i = 0; i < velocity.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(velocity.at(i).impl().get());
      *target_ptr = *kernel_out_1.at(i);
    }
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    if (master_param) {
      for (size_t i = 0; i < master_param->size(); ++i) {
        auto target_ptr = static_cast<phi::DenseTensor*>(master_param->at(i).impl().get());
        *target_ptr = *kernel_out_2.at(i);
      }
    }

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);

  return api_output;
}

PADDLE_API std::vector<Tensor> meshgrid(const std::vector<Tensor>& inputs) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(inputs);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(inputs);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "meshgrid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "meshgrid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("meshgrid", kernel_data_type);
  }
  VLOG(6) << "meshgrid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_inputs_vec = PrepareData(inputs, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_inputs(input_inputs_vec->size());
  for (size_t i = 0; i < input_inputs.size(); ++i) {
    input_inputs[i] = &input_inputs_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_inputs.size());
     for (size_t i = 0; i < input_inputs.size(); ++i) {
       ddims_vec.emplace_back((*input_inputs[i]).dims());
     }
     input_shapes.emplace_back("inputs", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("meshgrid", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(inputs.size(), &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("meshgrid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto inputs_meta_vec = MakeMetaTensor(input_inputs);
  std::vector<const phi::MetaTensor*> inputs_metas(inputs_meta_vec.size());
  for (size_t i = 0; i < inputs_meta_vec.size(); ++i) {
    inputs_metas[i] = &inputs_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::MeshgridInferMeta(inputs_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("meshgrid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_inputs, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor mish(const Tensor& x, float lambda) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mish API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mish", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mish", kernel_data_type);
  }
  VLOG(6) << "mish kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["lambda"] = lambda;
     phi::RecordOpInfoSupplement("mish", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mish infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mish compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, lambda, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> mode(const Tensor& x, int axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mode API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mode", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mode", kernel_data_type);
  }
  VLOG(6) << "mode kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("mode", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mode infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ModeInferMeta(MakeMetaTensor(*input_x), axis, keepdim, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mode compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> momentum_(Tensor& param, const Tensor& grad, Tensor& velocity, const Tensor& learning_rate, paddle::optional<Tensor>& master_param, float mu, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, velocity, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && grad.is_dense_tensor() && velocity.is_dense_tensor() && learning_rate.is_dense_tensor() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "momentum_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "momentum", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("momentum_", kernel_data_type);
    }
    VLOG(6) << "momentum kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_velocity = PrepareData(velocity, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"velocity", {
       (*input_velocity).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["mu"] = mu;
       attrs["use_nesterov"] = use_nesterov;
       attrs["regularization_method"] = regularization_method;
       attrs["regularization_coeff"] = regularization_coeff;
       attrs["multi_precision"] = multi_precision;
       attrs["rescale_grad"] = rescale_grad;
       phi::RecordOpInfoSupplement("momentum_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, velocity, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("momentum_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_velocity = *input_velocity;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::MomentumInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_velocity), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, bool, const std::string&, float, bool, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("momentum_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_velocity, *input_learning_rate, input_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);

    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() && velocity.is_dense_tensor() && learning_rate.is_dense_tensor() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "momentum_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "momentum_dense_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("momentum_", kernel_data_type);
    }
    VLOG(6) << "momentum_dense_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {});

    auto input_velocity = PrepareData(velocity, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"velocity", {
       (*input_velocity).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["mu"] = mu;
       attrs["use_nesterov"] = use_nesterov;
       attrs["regularization_method"] = regularization_method;
       attrs["regularization_coeff"] = regularization_coeff;
       attrs["multi_precision"] = multi_precision;
       attrs["rescale_grad"] = rescale_grad;
       phi::RecordOpInfoSupplement("momentum_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, velocity, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("momentum_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_velocity = *input_velocity;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

    phi::MomentumInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_velocity), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_master_param), mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, bool, const std::string&, float, bool, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("momentum_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_velocity, *input_learning_rate, input_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, kernel_out_0, kernel_out_1, kernel_out_2);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (momentum_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor multi_dot(const std::vector<Tensor>& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multi_dot API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multi_dot", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multi_dot", kernel_data_type);
  }
  VLOG(6) << "multi_dot kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("multi_dot", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multi_dot infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MultiDotInferMeta(x_metas, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multi_dot compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> multiclass_nms3(const Tensor& bboxes, const Tensor& scores, const paddle::optional<Tensor>& rois_num, float score_threshold, int nms_top_k, int keep_top_k, float nms_threshold, bool normalized, float nms_eta, int background_label) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(scores);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(bboxes, scores, rois_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multiclass_nms3 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiclass_nms3", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiclass_nms3", kernel_data_type);
  }
  VLOG(6) << "multiclass_nms3 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_bboxes = PrepareData(bboxes, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scores = PrepareData(scores, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rois_num = PrepareData(rois_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> rois_num_record_shapes;
     if(input_rois_num){
       rois_num_record_shapes.push_back((*input_rois_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"bboxes", {
     (*input_bboxes).dims()}},
     {"scores", {
     (*input_scores).dims()}},
     {"rois_num",
     rois_num_record_shapes}};
     phi::AttributeMap attrs;
     attrs["score_threshold"] = score_threshold;
     attrs["nms_top_k"] = nms_top_k;
     attrs["keep_top_k"] = keep_top_k;
     attrs["nms_threshold"] = nms_threshold;
     attrs["normalized"] = normalized;
     attrs["nms_eta"] = nms_eta;
     attrs["background_label"] = background_label;
     phi::RecordOpInfoSupplement("multiclass_nms3", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiclass_nms3 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::MultiClassNMSInferMeta(MakeMetaTensor(*input_bboxes), MakeMetaTensor(*input_scores), MakeMetaTensor(input_rois_num), score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, int, int, float, bool, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiclass_nms3 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_bboxes, *input_scores, input_rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor multinomial(const Tensor& x, const Scalar& num_samples, bool replacement) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multinomial API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multinomial", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multinomial", kernel_data_type);
  }
  VLOG(6) << "multinomial kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (num_samples.dtype()) {
      case DataType::FLOAT32:
          attrs["num_samples"] = static_cast<float>(num_samples.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["num_samples"] = static_cast<double>(num_samples.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["num_samples"] = static_cast<float>(num_samples.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["num_samples"] = static_cast<float>(num_samples.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["num_samples"] = static_cast<int32_t>(num_samples.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["num_samples"] = static_cast<int64_t>(num_samples.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["num_samples"] = static_cast<int16_t>(num_samples.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["num_samples"] = static_cast<int8_t>(num_samples.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["num_samples"] = static_cast<uint16_t>(num_samples.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["num_samples"] = static_cast<uint8_t>(num_samples.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["num_samples"] = static_cast<bool>(num_samples.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["num_samples"] = static_cast<float>(num_samples.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["num_samples"] = static_cast<double>(num_samples.to<complex128>());
          break;
      default:
          attrs["num_samples"] = "";
          break;
    }
     attrs["replacement"] = replacement;
     phi::RecordOpInfoSupplement("multinomial", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multinomial infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MultinomialInferMeta(MakeMetaTensor(*input_x), num_samples, replacement, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multinomial compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(num_samples), replacement, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor multiplex(const std::vector<Tensor>& inputs, const Tensor& index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(inputs);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(inputs, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multiplex API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiplex", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiplex", kernel_data_type);
  }
  VLOG(6) << "multiplex kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_inputs_vec = PrepareData(inputs, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_inputs(input_inputs_vec->size());
  for (size_t i = 0; i < input_inputs.size(); ++i) {
    input_inputs[i] = &input_inputs_vec->at(i);
  }
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"index", {
     (*input_index).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_inputs.size());
     for (size_t i = 0; i < input_inputs.size(); ++i) {
       ddims_vec.emplace_back((*input_inputs[i]).dims());
     }
     input_shapes.emplace_back("inputs", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("multiplex", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiplex infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto inputs_meta_vec = MakeMetaTensor(input_inputs);
  std::vector<const phi::MetaTensor*> inputs_metas(inputs_meta_vec.size());
  for (size_t i = 0; i < inputs_meta_vec.size(); ++i) {
    inputs_metas[i] = &inputs_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MultiplexInferMeta(inputs_metas, MakeMetaTensor(*input_index), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiplex compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_inputs, *input_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor mv(const Tensor& x, const Tensor& vec) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, vec);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mv", kernel_data_type);
  }
  VLOG(6) << "mv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_vec = PrepareData(vec, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"vec", {
     (*input_vec).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("mv", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MvInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_vec), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_vec, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> nadam_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& momentum_decay_pow, Tensor& beta2_pow, Tensor& mu_product, Tensor& moment1, Tensor& moment2, paddle::optional<Tensor>& master_param, float beta1, float beta2, float epsilon, float momentum_decay, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, momentum_decay_pow, beta2_pow, mu_product, moment1, moment2, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nadam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nadam", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nadam_", kernel_data_type);
  }
  VLOG(6) << "nadam kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_momentum_decay_pow = PrepareData(momentum_decay_pow, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mu_product = PrepareData(mu_product, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"momentum_decay_pow", {
     (*input_momentum_decay_pow).dims()}},
     {"beta2_pow", {
     (*input_beta2_pow).dims()}},
     {"mu_product", {
     (*input_mu_product).dims()}},
     {"moment1", {
     (*input_moment1).dims()}},
     {"moment2", {
     (*input_moment2).dims()}},
     {"master_param",
     master_param_record_shapes}};
     phi::AttributeMap attrs;
     attrs["beta1"] = beta1;
     attrs["beta2"] = beta2;
     attrs["epsilon"] = epsilon;
     attrs["momentum_decay"] = momentum_decay;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("nadam_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, momentum_decay_pow, beta2_pow, mu_product, moment1, moment2, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  auto kernel_out_6 = SetKernelOutput(std::get<6>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);
  auto backup6 = ProcessStrideBackup(&kernel_out_6);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nadam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_momentum_decay_pow = *input_momentum_decay_pow;

  auto origin_input_beta2_pow = *input_beta2_pow;

  auto origin_input_mu_product = *input_mu_product;

  auto origin_input_moment1 = *input_moment1;

  auto origin_input_moment2 = *input_moment2;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_6(kernel_out_6, kernel_result.is_stride_kernel);

  phi::NAdamInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_momentum_decay_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(origin_input_mu_product), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(input_master_param), beta1, beta2, epsilon, momentum_decay, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr, kernel_out_6 ? &meta_out_6 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, float, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nadam_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_momentum_decay_pow, origin_input_beta2_pow, origin_input_mu_product, origin_input_moment1, origin_input_moment2, input_master_param, beta1, beta2, epsilon, momentum_decay, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5, kernel_out_6);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    TransDataBackend(kernel_out_6, kernel_backend, kernel_out_6);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);
  TransStride(dev_ctx, kernel_out_6, backup6);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> nanmedian(const Tensor& x, const IntArray& axis, bool keepdim, const std::string& mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nanmedian API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nanmedian", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nanmedian", kernel_data_type);
  }
  VLOG(6) << "nanmedian kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("nanmedian", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nanmedian infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::NanmedianInferMeta(MakeMetaTensor(*input_x), axis, keepdim, mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nanmedian compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keepdim, mode, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor nearest_interp(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nearest_interp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nearest_interp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nearest_interp", kernel_data_type);
  }
  VLOG(6) << "nearest_interp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor",
     scale_tensor_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("nearest_interp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nearest_interp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_out_size), size_tensor_metas, MakeMetaTensor(input_scale_tensor), data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nearest_interp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor nextafter(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nextafter API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nextafter", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nextafter", kernel_data_type);
  }
  VLOG(6) << "nextafter kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("nextafter", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nextafter infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nextafter compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> nll_loss(const Tensor& input, const Tensor& label, const paddle::optional<Tensor>& weight, int64_t ignore_index, const std::string& reduction) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nll_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nll_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nll_loss", kernel_data_type);
  }
  VLOG(6) << "nll_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> weight_record_shapes;
     if(input_weight){
       weight_record_shapes.push_back((*input_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"weight",
     weight_record_shapes}};
     phi::AttributeMap attrs;
     attrs["ignore_index"] = ignore_index;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("nll_loss", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nll_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::NllLossRawInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), MakeMetaTensor(input_weight), ignore_index, reduction, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int64_t, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nll_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, input_weight, ignore_index, reduction, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor nms(const Tensor& x, float threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nms API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nms", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nms", kernel_data_type);
  }
  VLOG(6) << "nms kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("nms", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nms infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NMSInferMeta(MakeMetaTensor(*input_x), threshold, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nms compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor nonzero(const Tensor& condition) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(condition);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(condition);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nonzero API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nonzero", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nonzero", kernel_data_type);
  }
  VLOG(6) << "nonzero kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_condition = PrepareData(condition, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"condition", {
     (*input_condition).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("nonzero", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nonzero infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NonZeroInferMeta(MakeMetaTensor(*input_condition), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nonzero compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_condition, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> norm(const Tensor& x, int axis, float epsilon, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("norm", kernel_data_type);
  }
  VLOG(6) << "norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["epsilon"] = epsilon;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::NormInferMeta(MakeMetaTensor(*input_x), axis, epsilon, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, float, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, epsilon, is_test, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor npu_identity(const Tensor& x, int format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "npu_identity API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "npu_identity", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("npu_identity", kernel_data_type);
  }
  VLOG(6) << "npu_identity kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["format"] = format;
     phi::RecordOpInfoSupplement("npu_identity", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("npu_identity infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("npu_identity compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor numel(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "numel API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "numel", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("numel", kernel_data_type);
  }
  VLOG(6) << "numel kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("numel", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("numel infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NumelInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("numel compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor one_hot(const Tensor& x, const Scalar& num_classes) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "one_hot API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "one_hot", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("one_hot", kernel_data_type);
  }
  VLOG(6) << "one_hot kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (num_classes.dtype()) {
      case DataType::FLOAT32:
          attrs["num_classes"] = static_cast<float>(num_classes.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["num_classes"] = static_cast<double>(num_classes.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["num_classes"] = static_cast<float>(num_classes.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["num_classes"] = static_cast<float>(num_classes.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["num_classes"] = static_cast<int32_t>(num_classes.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["num_classes"] = static_cast<int64_t>(num_classes.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["num_classes"] = static_cast<int16_t>(num_classes.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["num_classes"] = static_cast<int8_t>(num_classes.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["num_classes"] = static_cast<uint16_t>(num_classes.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["num_classes"] = static_cast<uint8_t>(num_classes.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["num_classes"] = static_cast<bool>(num_classes.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["num_classes"] = static_cast<float>(num_classes.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["num_classes"] = static_cast<double>(num_classes.to<complex128>());
          break;
      default:
          attrs["num_classes"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("one_hot", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("one_hot infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::OneHotInferMeta(MakeMetaTensor(*input_x), num_classes, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("one_hot compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(num_classes), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor ones(const IntArray& shape, DataType dtype, const Place& place) {
  return full(shape, 1, dtype, place);
}
PADDLE_API Tensor ones_like(const Tensor& x, DataType dtype, const Place& place) {
  return full_like(x, 1, dtype, place);
}
PADDLE_API Tensor overlap_add(const Tensor& x, int hop_length, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "overlap_add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "overlap_add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("overlap_add", kernel_data_type);
  }
  VLOG(6) << "overlap_add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["hop_length"] = hop_length;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("overlap_add", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("overlap_add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::OverlapAddInferMeta(MakeMetaTensor(*input_x), hop_length, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("overlap_add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, hop_length, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor p_norm(const Tensor& x, float porder, int axis, float epsilon, bool keepdim, bool asvector) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "p_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "p_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("p_norm", kernel_data_type);
  }
  VLOG(6) << "p_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["porder"] = porder;
     attrs["axis"] = axis;
     attrs["epsilon"] = epsilon;
     attrs["keepdim"] = keepdim;
     attrs["asvector"] = asvector;
     phi::RecordOpInfoSupplement("p_norm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("p_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PNormInferMeta(MakeMetaTensor(*input_x), porder, axis, epsilon, keepdim, asvector, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, float, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("p_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, porder, axis, epsilon, keepdim, asvector, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pad(const Tensor& x, const std::vector<int>& paddings, const Scalar& pad_value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad", kernel_data_type);
  }
  VLOG(6) << "pad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings;
    switch (pad_value.dtype()) {
      case DataType::FLOAT32:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["pad_value"] = static_cast<double>(pad_value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["pad_value"] = static_cast<int32_t>(pad_value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["pad_value"] = static_cast<int64_t>(pad_value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["pad_value"] = static_cast<int16_t>(pad_value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["pad_value"] = static_cast<int8_t>(pad_value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["pad_value"] = static_cast<uint16_t>(pad_value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["pad_value"] = static_cast<uint8_t>(pad_value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["pad_value"] = static_cast<bool>(pad_value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["pad_value"] = static_cast<float>(pad_value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["pad_value"] = static_cast<double>(pad_value.to<complex128>());
          break;
      default:
          attrs["pad_value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pad", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PadInferMeta(MakeMetaTensor(*input_x), paddings, pad_value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, paddings, phi::Scalar(pad_value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pad3d(const Tensor& x, const IntArray& paddings, const std::string& mode, float pad_value, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pad3d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad3d", kernel_data_type);
  }
  VLOG(6) << "pad3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings.GetData();
     attrs["mode"] = mode;
     attrs["pad_value"] = pad_value;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pad3d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad3d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Pad3dInferMeta(MakeMetaTensor(*input_x), paddings, mode, pad_value, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::string&, float, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad3d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(paddings), mode, pad_value, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor partial_concat(const std::vector<Tensor>& x, int start_index, int length) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "partial_concat API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "partial_concat", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("partial_concat", kernel_data_type);
  }
  VLOG(6) << "partial_concat kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     attrs["start_index"] = start_index;
     attrs["length"] = length;
     phi::RecordOpInfoSupplement("partial_concat", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("partial_concat infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PartialConcatInferMeta(x_metas, start_index, length, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("partial_concat compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, start_index, length, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor partial_sum(const std::vector<Tensor>& x, int start_index, int length) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "partial_sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "partial_sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("partial_sum", kernel_data_type);
  }
  VLOG(6) << "partial_sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     attrs["start_index"] = start_index;
     attrs["length"] = length;
     phi::RecordOpInfoSupplement("partial_sum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("partial_sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PartialSumInferMeta(x_metas, start_index, length, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("partial_sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, start_index, length, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pixel_shuffle(const Tensor& x, int upscale_factor, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pixel_shuffle API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pixel_shuffle", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pixel_shuffle", kernel_data_type);
  }
  VLOG(6) << "pixel_shuffle kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["upscale_factor"] = upscale_factor;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pixel_shuffle", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pixel_shuffle infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PixelShuffleInferMeta(MakeMetaTensor(*input_x), upscale_factor, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pixel_shuffle compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, upscale_factor, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pixel_unshuffle(const Tensor& x, int downscale_factor, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pixel_unshuffle API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pixel_unshuffle", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pixel_unshuffle", kernel_data_type);
  }
  VLOG(6) << "pixel_unshuffle kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["downscale_factor"] = downscale_factor;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pixel_unshuffle", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pixel_unshuffle infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PixelUnshuffleInferMeta(MakeMetaTensor(*input_x), downscale_factor, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pixel_unshuffle compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, downscale_factor, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor poisson(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "poisson API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "poisson", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("poisson", kernel_data_type);
  }
  VLOG(6) << "poisson kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("poisson", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("poisson infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("poisson compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor polygamma(const Tensor& x, int n) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "polygamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "polygamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("polygamma", kernel_data_type);
  }
  VLOG(6) << "polygamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("polygamma", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("polygamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("polygamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, n, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& polygamma_(Tensor& x, int n) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "polygamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "polygamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("polygamma", kernel_data_type);
  }
  VLOG(6) << "polygamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("polygamma", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("polygamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("polygamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, n, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor pool2d(const Tensor& x, const IntArray& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pool2d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pool2d", kernel_data_type);
  }
  VLOG(6) << "pool2d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size.GetData();
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     phi::RecordOpInfoSupplement("pool2d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pool2d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Pool2DInferMeta(MakeMetaTensor(*input_x), kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pool2d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(kernel_size), strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pool3d(const Tensor& x, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pool3d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pool3d", kernel_data_type);
  }
  VLOG(6) << "pool3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     phi::RecordOpInfoSupplement("pool3d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pool3d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PoolInferMeta(MakeMetaTensor(*input_x), kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pool3d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor pow(const Tensor& x, const Scalar& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pow API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pow", kernel_data_type);
  }
  VLOG(6) << "pow kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (y.dtype()) {
      case DataType::FLOAT32:
          attrs["y"] = static_cast<float>(y.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["y"] = static_cast<double>(y.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["y"] = static_cast<float>(y.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["y"] = static_cast<float>(y.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["y"] = static_cast<bool>(y.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["y"] = static_cast<float>(y.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["y"] = static_cast<double>(y.to<complex128>());
          break;
      default:
          attrs["y"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pow", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pow infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pow compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(y), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& pow_(Tensor& x, const Scalar& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pow API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pow", kernel_data_type);
  }
  VLOG(6) << "pow kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (y.dtype()) {
      case DataType::FLOAT32:
          attrs["y"] = static_cast<float>(y.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["y"] = static_cast<double>(y.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["y"] = static_cast<float>(y.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["y"] = static_cast<float>(y.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["y"] = static_cast<bool>(y.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["y"] = static_cast<float>(y.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["y"] = static_cast<double>(y.to<complex128>());
          break;
      default:
          attrs["y"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pow", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pow infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pow compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(y), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor prelu(const Tensor& x, const Tensor& alpha, const std::string& data_format, const std::string& mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, alpha);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prelu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prelu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prelu", kernel_data_type);
  }
  VLOG(6) << "prelu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_alpha = PrepareData(alpha, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"alpha", {
     (*input_alpha).dims()}}};
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("prelu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prelu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PReluInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_alpha), data_format, mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prelu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_alpha, data_format, mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> prior_box(const Tensor& input, const Tensor& image, const std::vector<float>& min_sizes, const std::vector<float>& max_sizes, const std::vector<float>& aspect_ratios, const std::vector<float>& variances, bool flip, bool clip, float step_w, float step_h, float offset, bool min_max_aspect_ratios_order) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, image);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prior_box API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prior_box", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prior_box", kernel_data_type);
  }
  VLOG(6) << "prior_box kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_image = PrepareData(image, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"image", {
     (*input_image).dims()}}};
     phi::AttributeMap attrs;
     attrs["min_sizes"] = min_sizes;
     attrs["max_sizes"] = max_sizes;
     attrs["aspect_ratios"] = aspect_ratios;
     attrs["variances"] = variances;
     attrs["flip"] = flip;
     attrs["clip"] = clip;
     attrs["step_w"] = step_w;
     attrs["step_h"] = step_h;
     attrs["offset"] = offset;
     attrs["min_max_aspect_ratios_order"] = min_max_aspect_ratios_order;
     phi::RecordOpInfoSupplement("prior_box", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prior_box infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::PriorBoxInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_image), min_sizes, max_sizes, aspect_ratios, variances, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<float>&, const std::vector<float>&, const std::vector<float>&, const std::vector<float>&, bool, bool, float, float, float, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prior_box compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_image, min_sizes, max_sizes, aspect_ratios, variances, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor prod(const Tensor& x, const IntArray& axis, bool keepdim, bool reduce_all) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prod API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prod", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prod", kernel_data_type);
  }
  VLOG(6) << "prod kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("prod", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prod infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceIntArrayAxisInferMetaBase(MakeMetaTensor(*input_x), axis, keepdim, reduce_all, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prod compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor prune_gate_by_capacity(const Tensor& gate_idx, const Tensor& expert_count, int64_t n_expert, int64_t n_worker) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(gate_idx);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(gate_idx, expert_count);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prune_gate_by_capacity API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prune_gate_by_capacity", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prune_gate_by_capacity", kernel_data_type);
  }
  VLOG(6) << "prune_gate_by_capacity kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_gate_idx = PrepareData(gate_idx, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_expert_count = PrepareData(expert_count, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"gate_idx", {
     (*input_gate_idx).dims()}},
     {"expert_count", {
     (*input_expert_count).dims()}}};
     phi::AttributeMap attrs;
     attrs["n_expert"] = n_expert;
     attrs["n_worker"] = n_worker;
     phi::RecordOpInfoSupplement("prune_gate_by_capacity", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prune_gate_by_capacity infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PruneGateByCapacityInferMeta(MakeMetaTensor(*input_gate_idx), MakeMetaTensor(*input_expert_count), n_expert, n_worker, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prune_gate_by_capacity compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_gate_idx, *input_expert_count, n_expert, n_worker, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor psroi_pool(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, int pooled_height, int pooled_width, int output_channels, float spatial_scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "psroi_pool API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "psroi_pool", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("psroi_pool", kernel_data_type);
  }
  VLOG(6) << "psroi_pool kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num",
     boxes_num_record_shapes}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["output_channels"] = output_channels;
     attrs["spatial_scale"] = spatial_scale;
     phi::RecordOpInfoSupplement("psroi_pool", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("psroi_pool infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PsroiPoolInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_boxes), MakeMetaTensor(input_boxes_num), pooled_height, pooled_width, output_channels, spatial_scale, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("psroi_pool compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, pooled_height, pooled_width, output_channels, spatial_scale, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor put_along_axis(const Tensor& arr, const Tensor& indices, const Tensor& values, int axis, const std::string& reduce, bool include_self) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(arr);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(arr, indices, values);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "put_along_axis API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "put_along_axis", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("put_along_axis", kernel_data_type);
  }
  VLOG(6) << "put_along_axis kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_arr = PrepareData(arr, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"arr", {
     (*input_arr).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"values", {
     (*input_values).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["reduce"] = reduce;
     attrs["include_self"] = include_self;
     phi::RecordOpInfoSupplement("put_along_axis", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("put_along_axis infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_arr), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("put_along_axis compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, *input_values, axis, reduce, include_self, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& put_along_axis_(Tensor& arr, const Tensor& indices, const Tensor& values, int axis, const std::string& reduce, bool include_self) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(arr);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(arr, indices, values);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "put_along_axis API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "put_along_axis", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("put_along_axis", kernel_data_type);
  }
  VLOG(6) << "put_along_axis kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_arr = PrepareData(arr, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"arr", {
     (*input_arr).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"values", {
     (*input_values).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["reduce"] = reduce;
     attrs["include_self"] = include_self;
     phi::RecordOpInfoSupplement("put_along_axis", input_shapes, attrs);
  }

  Tensor& api_output = arr;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("put_along_axis infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_arr = *input_arr;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_arr), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("put_along_axis compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_arr, *input_indices, *input_values, axis, reduce, include_self, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> pyramid_hash(const Tensor& x, const Tensor& w, const Tensor& white_list, const Tensor& black_list, int num_emb, int space_len, int pyramid_layer, int rand_len, float drop_out_percent, int is_training, bool use_filter, int white_list_len, int black_list_len, int seed, float lr, const std::string& distribute_update_vars) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(w);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, w, white_list, black_list);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pyramid_hash API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pyramid_hash", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pyramid_hash", kernel_data_type);
  }
  VLOG(6) << "pyramid_hash kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_white_list = PrepareData(white_list, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_black_list = PrepareData(black_list, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"w", {
     (*input_w).dims()}},
     {"white_list", {
     (*input_white_list).dims()}},
     {"black_list", {
     (*input_black_list).dims()}}};
     phi::AttributeMap attrs;
     attrs["num_emb"] = num_emb;
     attrs["space_len"] = space_len;
     attrs["pyramid_layer"] = pyramid_layer;
     attrs["rand_len"] = rand_len;
     attrs["drop_out_percent"] = drop_out_percent;
     attrs["is_training"] = is_training;
     attrs["use_filter"] = use_filter;
     attrs["white_list_len"] = white_list_len;
     attrs["black_list_len"] = black_list_len;
     attrs["seed"] = seed;
     attrs["lr"] = lr;
     attrs["distribute_update_vars"] = distribute_update_vars;
     phi::RecordOpInfoSupplement("pyramid_hash", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pyramid_hash infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::PyramidHashInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_w), MakeMetaTensor(*input_white_list), MakeMetaTensor(*input_black_list), num_emb, space_len, pyramid_layer, rand_len, drop_out_percent, is_training, use_filter, white_list_len, black_list_len, seed, lr, distribute_update_vars, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, int, float, int, bool, int, int, int, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pyramid_hash compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_w, *input_white_list, *input_black_list, num_emb, space_len, pyramid_layer, rand_len, drop_out_percent, is_training, use_filter, white_list_len, black_list_len, seed, lr, distribute_update_vars, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor, Tensor> qr(const Tensor& x, const std::string& mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "qr API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "qr", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("qr", kernel_data_type);
  }
  VLOG(6) << "qr kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("qr", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("qr infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::QrInferMeta(MakeMetaTensor(*input_x), mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("qr compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, mode, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> radam_(Tensor& param, const Tensor& grad, const Tensor& learning_rate, Tensor& beta1_pow, Tensor& beta2_pow, Tensor& rho, Tensor& moment1, Tensor& moment2, paddle::optional<Tensor>& master_param, float beta1, float beta2, float epsilon, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, learning_rate, beta1_pow, beta2_pow, rho, moment1, moment2, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "radam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "radam", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("radam_", kernel_data_type);
  }
  VLOG(6) << "radam kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta1_pow = PrepareData(beta1_pow, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_beta2_pow = PrepareData(beta2_pow, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rho = PrepareData(rho, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment1 = PrepareData(moment1, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_moment2 = PrepareData(moment2, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"beta1_pow", {
     (*input_beta1_pow).dims()}},
     {"beta2_pow", {
     (*input_beta2_pow).dims()}},
     {"rho", {
     (*input_rho).dims()}},
     {"moment1", {
     (*input_moment1).dims()}},
     {"moment2", {
     (*input_moment2).dims()}},
     {"master_param",
     master_param_record_shapes}};
     phi::AttributeMap attrs;
     attrs["beta1"] = beta1;
     attrs["beta2"] = beta2;
     attrs["epsilon"] = epsilon;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("radam_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, beta1_pow, beta2_pow, rho, moment1, moment2, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  auto kernel_out_6 = SetKernelOutput(std::get<6>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);
  auto backup6 = ProcessStrideBackup(&kernel_out_6);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("radam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_beta1_pow = *input_beta1_pow;

  auto origin_input_beta2_pow = *input_beta2_pow;

  auto origin_input_rho = *input_rho;

  auto origin_input_moment1 = *input_moment1;

  auto origin_input_moment2 = *input_moment2;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_6(kernel_out_6, kernel_result.is_stride_kernel);

  phi::RAdamInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(origin_input_beta1_pow), MakeMetaTensor(origin_input_beta2_pow), MakeMetaTensor(origin_input_rho), MakeMetaTensor(origin_input_moment1), MakeMetaTensor(origin_input_moment2), MakeMetaTensor(input_master_param), beta1, beta2, epsilon, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr, kernel_out_6 ? &meta_out_6 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("radam_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, *input_learning_rate, origin_input_beta1_pow, origin_input_beta2_pow, origin_input_rho, origin_input_moment1, origin_input_moment2, input_master_param, beta1, beta2, epsilon, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5, kernel_out_6);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    TransDataBackend(kernel_out_6, kernel_backend, kernel_out_6);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);
  TransStride(dev_ctx, kernel_out_6, backup6);

  return api_output;
}

PADDLE_API Tensor randint(int low, int high, const IntArray& shape, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "randint API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "randint", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("randint", kernel_data_type);
  }
  VLOG(6) << "randint kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["low"] = low;
     attrs["high"] = high;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("randint", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("randint infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RandintInferMeta(low, high, shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, int, int, const phi::IntArray&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("randint compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, low, high, phi::IntArray(shape), dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor random_routing(const Tensor& prob, const Tensor& topk_value, const Tensor& topk_idx) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(prob);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(prob, topk_value, topk_idx);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "random_routing API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "random_routing", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("random_routing", kernel_data_type);
  }
  VLOG(6) << "random_routing kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_prob = PrepareData(prob, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_topk_value = PrepareData(topk_value, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_topk_idx = PrepareData(topk_idx, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"prob", {
     (*input_prob).dims()}},
     {"topk_value", {
     (*input_topk_value).dims()}},
     {"topk_idx", {
     (*input_topk_idx).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("random_routing", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("random_routing infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RandomRoutingInferMeta(MakeMetaTensor(*input_prob), MakeMetaTensor(*input_topk_value), MakeMetaTensor(*input_topk_idx), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("random_routing compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_prob, *input_topk_value, *input_topk_idx, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& random_routing_(const Tensor& prob, const Tensor& topk_value, Tensor& topk_idx) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(prob);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(prob, topk_value, topk_idx);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "random_routing API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "random_routing", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("random_routing", kernel_data_type);
  }
  VLOG(6) << "random_routing kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_prob = PrepareData(prob, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_topk_value = PrepareData(topk_value, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_topk_idx = PrepareData(topk_idx, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"prob", {
     (*input_prob).dims()}},
     {"topk_value", {
     (*input_topk_value).dims()}},
     {"topk_idx", {
     (*input_topk_idx).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("random_routing", input_shapes, attrs);
  }

  Tensor& api_output = topk_idx;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("random_routing infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_topk_idx = *input_topk_idx;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RandomRoutingInferMeta(MakeMetaTensor(*input_prob), MakeMetaTensor(*input_topk_value), MakeMetaTensor(origin_input_topk_idx), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("random_routing compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_prob, *input_topk_value, origin_input_topk_idx, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor randperm(int n, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "randperm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "randperm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("randperm", kernel_data_type);
  }
  VLOG(6) << "randperm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("randperm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("randperm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RandpermInferMeta(n, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("randperm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, n, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> rank_attention(const Tensor& x, const Tensor& rank_offset, const Tensor& rank_param, int max_rank, int max_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, rank_offset, rank_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rank_attention API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rank_attention", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rank_attention", kernel_data_type);
  }
  VLOG(6) << "rank_attention kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rank_offset = PrepareData(rank_offset, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rank_param = PrepareData(rank_param, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"rank_offset", {
     (*input_rank_offset).dims()}},
     {"rank_param", {
     (*input_rank_param).dims()}}};
     phi::AttributeMap attrs;
     attrs["max_rank"] = max_rank;
     attrs["max_size"] = max_size;
     phi::RecordOpInfoSupplement("rank_attention", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rank_attention infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::RankAttentionInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_rank_offset), MakeMetaTensor(*input_rank_param), max_rank, max_size, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rank_attention compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_rank_offset, *input_rank_param, max_rank, max_size, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor read_file(const std::string& filename, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "read_file API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "read_file", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("read_file", kernel_data_type);
  }
  VLOG(6) << "read_file kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["filename"] = filename;
     phi::RecordOpInfoSupplement("read_file", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("read_file infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReadFileInferMeta(filename, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("read_file compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, filename, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor real(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "real API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "real", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("real", kernel_data_type);
  }
  VLOG(6) << "real kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("real", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("real infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("real compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor reciprocal(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reciprocal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reciprocal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reciprocal", kernel_data_type);
  }
  VLOG(6) << "reciprocal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reciprocal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reciprocal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reciprocal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& reciprocal_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reciprocal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reciprocal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reciprocal", kernel_data_type);
  }
  VLOG(6) << "reciprocal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reciprocal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reciprocal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reciprocal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor reduce_as(const Tensor& x, const Tensor& target) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, target);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reduce_as API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reduce_as", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reduce_as", kernel_data_type);
  }
  VLOG(6) << "reduce_as kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_target = PrepareData(target, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"target", {
     (*input_target).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reduce_as", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reduce_as infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceAsInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_target), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reduce_as compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_target, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor reduce_scatter(const Tensor& x, int ring_id, int nranks) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reduce_scatter API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reduce_scatter", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reduce_scatter", kernel_data_type);
  }
  VLOG(6) << "reduce_scatter kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["ring_id"] = ring_id;
     attrs["nranks"] = nranks;
     phi::RecordOpInfoSupplement("reduce_scatter", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reduce_scatter infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceScatterInferMeta(MakeMetaTensor(*input_x), nranks, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reduce_scatter compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, nranks, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> reindex_graph(const Tensor& x, const Tensor& neighbors, const Tensor& count, const paddle::optional<Tensor>& hashtable_value, const paddle::optional<Tensor>& hashtable_index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, neighbors, count, hashtable_value, hashtable_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reindex_graph API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_reindex", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reindex_graph", kernel_data_type);
  }
  VLOG(6) << "graph_reindex kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_neighbors = PrepareData(neighbors, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_count = PrepareData(count, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_hashtable_value = PrepareData(hashtable_value, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_hashtable_index = PrepareData(hashtable_index, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> hashtable_value_record_shapes;
     if(input_hashtable_value){
       hashtable_value_record_shapes.push_back((*input_hashtable_value).dims());
     }
     std::vector<phi::DDim> hashtable_index_record_shapes;
     if(input_hashtable_index){
       hashtable_index_record_shapes.push_back((*input_hashtable_index).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"neighbors", {
     (*input_neighbors).dims()}},
     {"count", {
     (*input_count).dims()}},
     {"hashtable_value", hashtable_value_record_shapes},
     {"hashtable_index",
     hashtable_index_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reindex_graph", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reindex_graph infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GraphReindexInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_neighbors), MakeMetaTensor(*input_count), MakeMetaTensor(input_hashtable_value), MakeMetaTensor(input_hashtable_index), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reindex_graph compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_neighbors, *input_count, input_hashtable_value, input_hashtable_index, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor relu(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu", kernel_data_type);
  }
  VLOG(6) << "relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& relu_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu", kernel_data_type);
  }
  VLOG(6) << "relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor relu6(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu6 API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu6", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu6", kernel_data_type);
  }
  VLOG(6) << "relu6 kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu6", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu6 infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu6 compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor renorm(const Tensor& x, float p, int axis, float max_norm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "renorm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "renorm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("renorm", kernel_data_type);
  }
  VLOG(6) << "renorm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["p"] = p;
     attrs["axis"] = axis;
     attrs["max_norm"] = max_norm;
     phi::RecordOpInfoSupplement("renorm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("renorm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("renorm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, p, axis, max_norm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& renorm_(Tensor& x, float p, int axis, float max_norm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "renorm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "renorm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("renorm", kernel_data_type);
  }
  VLOG(6) << "renorm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["p"] = p;
     attrs["axis"] = axis;
     attrs["max_norm"] = max_norm;
     phi::RecordOpInfoSupplement("renorm", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("renorm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("renorm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, p, axis, max_norm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor repeat_interleave(const Tensor& x, int repeats, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "repeat_interleave API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("repeat_interleave", kernel_data_type);
  }
  VLOG(6) << "repeat_interleave kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["repeats"] = repeats;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("repeat_interleave", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("repeat_interleave infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RepeatInterleaveInferMeta(MakeMetaTensor(*input_x), repeats, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("repeat_interleave compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, repeats, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor repeat_interleave_with_tensor_index(const Tensor& x, const Tensor& repeats, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, repeats);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "repeat_interleave_with_tensor_index API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave_with_tensor_index", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("repeat_interleave_with_tensor_index", kernel_data_type);
  }
  VLOG(6) << "repeat_interleave_with_tensor_index kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_repeats = PrepareData(repeats, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"repeats", {
     (*input_repeats).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("repeat_interleave_with_tensor_index", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("repeat_interleave_with_tensor_index infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RepeatInterleaveWithTensorIndexInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_repeats), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("repeat_interleave_with_tensor_index compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_repeats, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor reshape(const Tensor& x, const IntArray& shape) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reshape API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reshape", kernel_data_type);
  }
  VLOG(6) << "reshape kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("reshape", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  kernel_out->ShareBufferWith(*input_x);
  kernel_out->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reshape infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReshapeInferMeta(MakeMetaTensor(*input_x), shape, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reshape compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shape), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

    phi::DenseTensor * x_remap = static_cast<phi::DenseTensor*>(x.impl().get());
    x_remap->ShareBufferWith(*kernel_out);
    kernel_out->ShareInplaceVersionCounterWith(*x_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& reshape_(Tensor& x, const IntArray& shape) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reshape API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reshape", kernel_data_type);
  }
  VLOG(6) << "reshape kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("reshape", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reshape infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReshapeInferMeta(MakeMetaTensor(origin_input_x), shape, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reshape compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::IntArray(shape), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor reverse(const Tensor& x, const IntArray& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reverse API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reverse", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reverse", kernel_data_type);
  }
  VLOG(6) << "reverse kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("reverse", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reverse infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReverseInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reverse compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> rms_norm(const Tensor& x, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& residual, const Tensor& norm_weight, const paddle::optional<Tensor>& norm_bias, float epsilon, int begin_norm_axis, float quant_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, bias, residual, norm_weight, norm_bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rms_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rms_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rms_norm", kernel_data_type);
  }
  VLOG(6) << "rms_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_residual = PrepareData(residual, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_norm_weight = PrepareData(norm_weight, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_norm_bias = PrepareData(norm_bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> residual_record_shapes;
     if(input_residual){
       residual_record_shapes.push_back((*input_residual).dims());
     }
     std::vector<phi::DDim> norm_bias_record_shapes;
     if(input_norm_bias){
       norm_bias_record_shapes.push_back((*input_norm_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"bias", bias_record_shapes},
     {"residual", residual_record_shapes},
     {"norm_weight", {
     (*input_norm_weight).dims()}},
     {"norm_bias",
     norm_bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["begin_norm_axis"] = begin_norm_axis;
     attrs["quant_scale"] = quant_scale;
     attrs["quant_round_type"] = quant_round_type;
     attrs["quant_max_bound"] = quant_max_bound;
     attrs["quant_min_bound"] = quant_min_bound;
     phi::RecordOpInfoSupplement("rms_norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rms_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::RmsNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_bias), MakeMetaTensor(input_residual), MakeMetaTensor(*input_norm_weight), MakeMetaTensor(input_norm_bias), epsilon, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, float, int, float, int, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rms_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_bias, input_residual, *input_norm_weight, input_norm_bias, epsilon, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output));
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&, paddle::optional<Tensor>&> rmsprop_(Tensor& param, Tensor& mean_square, const Tensor& grad, Tensor& moment, const Tensor& learning_rate, paddle::optional<Tensor>& mean_grad, paddle::optional<Tensor>& master_param, float epsilon, float decay, float momentum, bool centered, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, mean_square, grad, moment, learning_rate, mean_grad, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && mean_square.is_dense_tensor() && grad.is_dense_tensor() && moment.is_dense_tensor() && learning_rate.is_dense_tensor() && (!mean_grad || mean_grad->is_dense_tensor()) && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "rmsprop_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "rmsprop", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rmsprop_", kernel_data_type);
    }
    VLOG(6) << "rmsprop kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_mean_square = PrepareData(mean_square, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_mean_grad = PrepareData(mean_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> mean_grad_record_shapes;
       if(input_mean_grad){
         mean_grad_record_shapes.push_back((*input_mean_grad).dims());
       }
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"mean_square", {
       (*input_mean_square).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"moment", {
       (*input_moment).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"mean_grad", mean_grad_record_shapes},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["epsilon"] = epsilon;
       attrs["decay"] = decay;
       attrs["momentum"] = momentum;
       attrs["centered"] = centered;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("rmsprop_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&, paddle::optional<Tensor>&> api_output{param, moment, mean_square, mean_grad, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
    auto kernel_out_4 = SetKernelOutput(std::get<4>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("rmsprop_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_mean_square = *input_mean_square;

    auto origin_input_moment = *input_moment;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);

    phi::RmspropInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(origin_input_mean_square), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_moment), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_mean_grad), MakeMetaTensor(input_master_param), epsilon, decay, momentum, centered, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, float, float, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("rmsprop_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, origin_input_mean_square, *input_grad, origin_input_moment, *input_learning_rate, input_mean_grad, input_master_param, epsilon, decay, momentum, centered, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);

    return api_output;
  }

  if (param.is_dense_tensor() && mean_square.is_dense_tensor() && grad.is_selected_rows() && moment.is_dense_tensor() && learning_rate.is_dense_tensor() && (!mean_grad || mean_grad->is_dense_tensor()) && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "rmsprop_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "rmsprop_dense_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rmsprop_", kernel_data_type);
    }
    VLOG(6) << "rmsprop_dense_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_mean_square = PrepareData(mean_square, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {});

    auto input_moment = PrepareData(moment, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_mean_grad = PrepareData(mean_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> mean_grad_record_shapes;
       if(input_mean_grad){
         mean_grad_record_shapes.push_back((*input_mean_grad).dims());
       }
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"mean_square", {
       (*input_mean_square).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"moment", {
       (*input_moment).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"mean_grad", mean_grad_record_shapes},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["epsilon"] = epsilon;
       attrs["decay"] = decay;
       attrs["momentum"] = momentum;
       attrs["centered"] = centered;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("rmsprop_", input_shapes, attrs);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&, paddle::optional<Tensor>&> api_output{param, moment, mean_square, mean_grad, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
    auto kernel_out_4 = SetKernelOutput(std::get<4>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);
    auto backup2 = ProcessStrideBackup(&kernel_out_2);
    auto backup3 = ProcessStrideBackup(&kernel_out_3);
    auto backup4 = ProcessStrideBackup(&kernel_out_4);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("rmsprop_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;

    auto origin_input_mean_square = *input_mean_square;

    auto origin_input_moment = *input_moment;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);

    phi::RmspropInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(origin_input_mean_square), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_moment), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(input_mean_grad), MakeMetaTensor(input_master_param), epsilon, decay, momentum, centered, multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, float, float, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("rmsprop_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, origin_input_mean_square, *input_grad, origin_input_moment, *input_learning_rate, input_mean_grad, input_master_param, epsilon, decay, momentum, centered, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);
    TransStride(dev_ctx, kernel_out_2, backup2);
    TransStride(dev_ctx, kernel_out_3, backup3);
    TransStride(dev_ctx, kernel_out_4, backup4);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (rmsprop_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor, Tensor, std::vector<Tensor>> rnn(const Tensor& x, const std::vector<Tensor>& pre_state, const std::vector<Tensor>& weight_list, const paddle::optional<Tensor>& sequence_length, const Tensor& dropout_state_in, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, const std::string& mode, int seed, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, pre_state, weight_list, sequence_length, dropout_state_in);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rnn API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rnn", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rnn", kernel_data_type);
  }
  VLOG(6) << "rnn kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pre_state_vec = PrepareData(pre_state, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_pre_state(input_pre_state_vec->size());
  for (size_t i = 0; i < input_pre_state.size(); ++i) {
    input_pre_state[i] = &input_pre_state_vec->at(i);
  }
  auto input_weight_list_vec = PrepareData(weight_list, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_weight_list(input_weight_list_vec->size());
  for (size_t i = 0; i < input_weight_list.size(); ++i) {
    input_weight_list[i] = &input_weight_list_vec->at(i);
  }
  auto input_sequence_length = PrepareData(sequence_length, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dropout_state_in = PrepareData(dropout_state_in, kernel.InputAt(0), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> sequence_length_record_shapes;
     if(input_sequence_length){
       sequence_length_record_shapes.push_back((*input_sequence_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"sequence_length",
     sequence_length_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_pre_state.size());
     for (size_t i = 0; i < input_pre_state.size(); ++i) {
       ddims_vec.emplace_back((*input_pre_state[i]).dims());
     }
     input_shapes.emplace_back("pre_state", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_weight_list.size());
     for (size_t i = 0; i < input_weight_list.size(); ++i) {
       ddims_vec.emplace_back((*input_weight_list[i]).dims());
     }
     input_shapes.emplace_back("weight_list", ddims_vec);
     phi::AttributeMap attrs;
     attrs["dropout_prob"] = dropout_prob;
     attrs["is_bidirec"] = is_bidirec;
     attrs["input_size"] = input_size;
     attrs["hidden_size"] = hidden_size;
     attrs["num_layers"] = num_layers;
     attrs["mode"] = mode;
     attrs["seed"] = seed;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("rnn", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
      kernel_out_1->ShareBufferWith(*input_dropout_state_in);
      kernel_out_1->ShareInplaceVersionCounterWith(*input_dropout_state_in);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_2 = SetKernelOutput(pre_state.size(), &std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rnn infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto pre_state_meta_vec = MakeMetaTensor(input_pre_state);
  std::vector<const phi::MetaTensor*> pre_state_metas(pre_state_meta_vec.size());
  for (size_t i = 0; i < pre_state_meta_vec.size(); ++i) {
    pre_state_metas[i] = &pre_state_meta_vec[i];
  }

  auto weight_list_meta_vec = MakeMetaTensor(input_weight_list);
  std::vector<const phi::MetaTensor*> weight_list_metas(weight_list_meta_vec.size());
  for (size_t i = 0; i < weight_list_meta_vec.size(); ++i) {
    weight_list_metas[i] = &weight_list_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::RnnInferMeta(MakeMetaTensor(*input_x), pre_state_metas, weight_list_metas, MakeMetaTensor(input_sequence_length), dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2_metas, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<phi::DenseTensor>&, float, bool, int, int, int, const std::string&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*>, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rnn compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_pre_state, input_weight_list, input_sequence_length, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

    phi::DenseTensor * dropout_state_in_remap = static_cast<phi::DenseTensor*>(dropout_state_in.impl().get());
    dropout_state_in_remap->ShareBufferWith(*kernel_out_1);
    kernel_out_1->ShareInplaceVersionCounterWith(*dropout_state_in_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::make_tuple(std::get<0>(api_output), std::get<1>(api_output), std::get<2>(api_output));
}

PADDLE_API Tensor roi_align(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roi_align API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_align", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roi_align", kernel_data_type);
  }
  VLOG(6) << "roi_align kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num",
     boxes_num_record_shapes}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["spatial_scale"] = spatial_scale;
     attrs["sampling_ratio"] = sampling_ratio;
     attrs["aligned"] = aligned;
     phi::RecordOpInfoSupplement("roi_align", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roi_align infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RoiAlignInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_boxes), MakeMetaTensor(input_boxes_num), pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, float, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roi_align compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor roi_pool(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, int pooled_height, int pooled_width, float spatial_scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roi_pool API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_pool", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roi_pool", kernel_data_type);
  }
  VLOG(6) << "roi_pool kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num",
     boxes_num_record_shapes}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["spatial_scale"] = spatial_scale;
     phi::RecordOpInfoSupplement("roi_pool", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roi_pool infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::RoiPoolInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_boxes), MakeMetaTensor(input_boxes_num), pooled_height, pooled_width, spatial_scale, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roi_pool compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, pooled_height, pooled_width, spatial_scale, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor roll(const Tensor& x, const IntArray& shifts, const std::vector<int64_t>& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roll API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roll", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roll", kernel_data_type);
  }
  VLOG(6) << "roll kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["shifts"] = shifts.GetData();
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("roll", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roll infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RollInferMeta(MakeMetaTensor(*input_x), shifts, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roll compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shifts), axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor round(const Tensor& x, int decimals) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "round API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "round", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("round", kernel_data_type);
  }
  VLOG(6) << "round kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["decimals"] = decimals;
     phi::RecordOpInfoSupplement("round", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("round infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("round compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, decimals, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& round_(Tensor& x, int decimals) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "round API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "round", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("round", kernel_data_type);
  }
  VLOG(6) << "round kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["decimals"] = decimals;
     phi::RecordOpInfoSupplement("round", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("round infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("round compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, decimals, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> rprop_(Tensor& param, const Tensor& grad, Tensor& prev, Tensor& learning_rate, paddle::optional<Tensor>& master_param, const Tensor& learning_rate_range, const Tensor& etas, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, grad, prev, learning_rate, master_param, learning_rate_range, etas);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rprop_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rprop", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rprop_", kernel_data_type);
  }
  VLOG(6) << "rprop kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_prev = PrepareData(prev, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_learning_rate_range = PrepareData(learning_rate_range, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_etas = PrepareData(etas, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> master_param_record_shapes;
     if(input_master_param){
       master_param_record_shapes.push_back((*input_master_param).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"param", {
     (*input_param).dims()}},
     {"grad", {
     (*input_grad).dims()}},
     {"prev", {
     (*input_prev).dims()}},
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"master_param", master_param_record_shapes},
     {"learning_rate_range", {
     (*input_learning_rate_range).dims()}},
     {"etas", {
     (*input_etas).dims()}}};
     phi::AttributeMap attrs;
     attrs["multi_precision"] = multi_precision;
     phi::RecordOpInfoSupplement("rprop_", input_shapes, attrs);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{param, prev, learning_rate, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(std::get<3>(api_output).get_ptr());
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rprop_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_param = *input_param;

  auto origin_input_prev = *input_prev;

  auto origin_input_learning_rate = *input_learning_rate;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::RpropInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_grad), MakeMetaTensor(origin_input_prev), MakeMetaTensor(origin_input_learning_rate), MakeMetaTensor(input_master_param), MakeMetaTensor(*input_learning_rate_range), MakeMetaTensor(*input_etas), multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rprop_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_param, *input_grad, origin_input_prev, origin_input_learning_rate, input_master_param, *input_learning_rate_range, *input_etas, multi_precision, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API Tensor rrelu(const Tensor& x, float lower, float upper, bool is_test) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rrelu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rrelu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rrelu", kernel_data_type);
  }
  VLOG(6) << "rrelu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["lower"] = lower;
     attrs["upper"] = upper;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("rrelu", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rrelu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::RReluInferMeta(MakeMetaTensor(*input_x), lower, upper, is_test, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rrelu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, lower, upper, is_test, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor rsqrt(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rsqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rsqrt", kernel_data_type);
  }
  VLOG(6) << "rsqrt kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("rsqrt", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rsqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rsqrt compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& rsqrt_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rsqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rsqrt", kernel_data_type);
  }
  VLOG(6) << "rsqrt kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("rsqrt", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rsqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rsqrt compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor scale(const Tensor& x, const Scalar& scale, const Scalar& bias, bool bias_after_scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "scale", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scale", kernel_data_type);
    }
    VLOG(6) << "scale kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
      switch (scale.dtype()) {
        case DataType::FLOAT32:
            attrs["scale"] = static_cast<float>(scale.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["scale"] = static_cast<double>(scale.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["scale"] = static_cast<int32_t>(scale.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["scale"] = static_cast<int64_t>(scale.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["scale"] = static_cast<int16_t>(scale.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["scale"] = static_cast<int8_t>(scale.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["scale"] = static_cast<uint16_t>(scale.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["scale"] = static_cast<uint8_t>(scale.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["scale"] = static_cast<bool>(scale.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["scale"] = static_cast<float>(scale.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["scale"] = static_cast<double>(scale.to<complex128>());
            break;
        default:
            attrs["scale"] = "";
            break;
      }
      switch (bias.dtype()) {
        case DataType::FLOAT32:
            attrs["bias"] = static_cast<float>(bias.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["bias"] = static_cast<double>(bias.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["bias"] = static_cast<int32_t>(bias.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["bias"] = static_cast<int64_t>(bias.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["bias"] = static_cast<int16_t>(bias.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["bias"] = static_cast<int8_t>(bias.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["bias"] = static_cast<uint16_t>(bias.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["bias"] = static_cast<uint8_t>(bias.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["bias"] = static_cast<bool>(bias.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["bias"] = static_cast<float>(bias.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["bias"] = static_cast<double>(bias.to<complex128>());
            break;
        default:
            attrs["bias"] = "";
            break;
      }
       attrs["bias_after_scale"] = bias_after_scale;
       phi::RecordOpInfoSupplement("scale", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("scale infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("scale compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(scale), phi::Scalar(bias), bias_after_scale, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "scale_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scale", kernel_data_type);
    }
    VLOG(6) << "scale_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
      switch (scale.dtype()) {
        case DataType::FLOAT32:
            attrs["scale"] = static_cast<float>(scale.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["scale"] = static_cast<double>(scale.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["scale"] = static_cast<int32_t>(scale.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["scale"] = static_cast<int64_t>(scale.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["scale"] = static_cast<int16_t>(scale.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["scale"] = static_cast<int8_t>(scale.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["scale"] = static_cast<uint16_t>(scale.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["scale"] = static_cast<uint8_t>(scale.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["scale"] = static_cast<bool>(scale.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["scale"] = static_cast<float>(scale.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["scale"] = static_cast<double>(scale.to<complex128>());
            break;
        default:
            attrs["scale"] = "";
            break;
      }
      switch (bias.dtype()) {
        case DataType::FLOAT32:
            attrs["bias"] = static_cast<float>(bias.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["bias"] = static_cast<double>(bias.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["bias"] = static_cast<int32_t>(bias.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["bias"] = static_cast<int64_t>(bias.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["bias"] = static_cast<int16_t>(bias.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["bias"] = static_cast<int8_t>(bias.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["bias"] = static_cast<uint16_t>(bias.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["bias"] = static_cast<uint8_t>(bias.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["bias"] = static_cast<bool>(bias.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["bias"] = static_cast<float>(bias.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["bias"] = static_cast<double>(bias.to<complex128>());
            break;
        default:
            attrs["bias"] = "";
            break;
      }
       attrs["bias_after_scale"] = bias_after_scale;
       phi::RecordOpInfoSupplement("scale", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("scale infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::Scalar&, const phi::Scalar&, bool, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("scale compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(scale), phi::Scalar(bias), bias_after_scale, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (scale) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor& scale_(Tensor& x, const Scalar& scale, const Scalar& bias, bool bias_after_scale) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "scale", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scale", kernel_data_type);
    }
    VLOG(6) << "scale kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
      switch (scale.dtype()) {
        case DataType::FLOAT32:
            attrs["scale"] = static_cast<float>(scale.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["scale"] = static_cast<double>(scale.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["scale"] = static_cast<int32_t>(scale.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["scale"] = static_cast<int64_t>(scale.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["scale"] = static_cast<int16_t>(scale.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["scale"] = static_cast<int8_t>(scale.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["scale"] = static_cast<uint16_t>(scale.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["scale"] = static_cast<uint8_t>(scale.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["scale"] = static_cast<bool>(scale.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["scale"] = static_cast<float>(scale.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["scale"] = static_cast<double>(scale.to<complex128>());
            break;
        default:
            attrs["scale"] = "";
            break;
      }
      switch (bias.dtype()) {
        case DataType::FLOAT32:
            attrs["bias"] = static_cast<float>(bias.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["bias"] = static_cast<double>(bias.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["bias"] = static_cast<int32_t>(bias.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["bias"] = static_cast<int64_t>(bias.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["bias"] = static_cast<int16_t>(bias.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["bias"] = static_cast<int8_t>(bias.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["bias"] = static_cast<uint16_t>(bias.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["bias"] = static_cast<uint8_t>(bias.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["bias"] = static_cast<bool>(bias.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["bias"] = static_cast<float>(bias.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["bias"] = static_cast<double>(bias.to<complex128>());
            break;
        default:
            attrs["bias"] = "";
            break;
      }
       attrs["bias_after_scale"] = bias_after_scale;
       phi::RecordOpInfoSupplement("scale", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("scale infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("scale compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(scale), phi::Scalar(bias), bias_after_scale, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "scale_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scale", kernel_data_type);
    }
    VLOG(6) << "scale_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
      switch (scale.dtype()) {
        case DataType::FLOAT32:
            attrs["scale"] = static_cast<float>(scale.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["scale"] = static_cast<double>(scale.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["scale"] = static_cast<float>(scale.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["scale"] = static_cast<int32_t>(scale.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["scale"] = static_cast<int64_t>(scale.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["scale"] = static_cast<int16_t>(scale.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["scale"] = static_cast<int8_t>(scale.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["scale"] = static_cast<uint16_t>(scale.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["scale"] = static_cast<uint8_t>(scale.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["scale"] = static_cast<bool>(scale.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["scale"] = static_cast<float>(scale.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["scale"] = static_cast<double>(scale.to<complex128>());
            break;
        default:
            attrs["scale"] = "";
            break;
      }
      switch (bias.dtype()) {
        case DataType::FLOAT32:
            attrs["bias"] = static_cast<float>(bias.to<float>());
            break;
        case DataType::FLOAT64:
            attrs["bias"] = static_cast<double>(bias.to<double>());
            break;
        case DataType::FLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<float16>());
            break;
        case DataType::BFLOAT16:
            attrs["bias"] = static_cast<float>(bias.to<bfloat16>());
            break;
        case DataType::INT32:
            attrs["bias"] = static_cast<int32_t>(bias.to<int32_t>());
            break;
        case DataType::INT64:
            attrs["bias"] = static_cast<int64_t>(bias.to<int64_t>());
            break;
        case DataType::INT16:
            attrs["bias"] = static_cast<int16_t>(bias.to<int16_t>());
            break;
        case DataType::INT8:
            attrs["bias"] = static_cast<int8_t>(bias.to<int8_t>());
            break;
        case DataType::UINT16:
            attrs["bias"] = static_cast<uint16_t>(bias.to<uint16_t>());
            break;
        case DataType::UINT8:
            attrs["bias"] = static_cast<uint8_t>(bias.to<uint8_t>());
            break;
        case DataType::BOOL:
            attrs["bias"] = static_cast<bool>(bias.to<bool>());
            break;
        case DataType::COMPLEX64:
            attrs["bias"] = static_cast<float>(bias.to<complex64>());
            break;
        case DataType::COMPLEX128:
            attrs["bias"] = static_cast<double>(bias.to<complex128>());
            break;
        default:
            attrs["bias"] = "";
            break;
      }
       attrs["bias_after_scale"] = bias_after_scale;
       phi::RecordOpInfoSupplement("scale", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("scale infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::Scalar&, const phi::Scalar&, bool, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("scale compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, phi::Scalar(scale), phi::Scalar(bias), bias_after_scale, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (scale) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor scatter(const Tensor& x, const Tensor& index, const Tensor& updates, bool overwrite) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scatter", kernel_data_type);
  }
  VLOG(6) << "scatter kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_updates = PrepareData(updates, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"updates", {
     (*input_updates).dims()}}};
     phi::AttributeMap attrs;
     attrs["overwrite"] = overwrite;
     phi::RecordOpInfoSupplement("scatter", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("scatter infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ScatterInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), overwrite, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("scatter compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_updates, overwrite, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& scatter_(Tensor& x, const Tensor& index, const Tensor& updates, bool overwrite) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scatter", kernel_data_type);
  }
  VLOG(6) << "scatter kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_updates = PrepareData(updates, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"updates", {
     (*input_updates).dims()}}};
     phi::AttributeMap attrs;
     attrs["overwrite"] = overwrite;
     phi::RecordOpInfoSupplement("scatter", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("scatter infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ScatterInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), overwrite, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("scatter compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_index, *input_updates, overwrite, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor scatter_nd_add(const Tensor& x, const Tensor& index, const Tensor& updates) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter_nd_add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter_nd_add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scatter_nd_add", kernel_data_type);
  }
  VLOG(6) << "scatter_nd_add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_updates = PrepareData(updates, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"updates", {
     (*input_updates).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("scatter_nd_add", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("scatter_nd_add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ScatterNdAddInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("scatter_nd_add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_updates, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor searchsorted(const Tensor& sorted_sequence, const Tensor& values, bool out_int32, bool right) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(sorted_sequence);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(sorted_sequence, values);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "searchsorted API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "searchsorted", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("searchsorted", kernel_data_type);
  }
  VLOG(6) << "searchsorted kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_sorted_sequence = PrepareData(sorted_sequence, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"sorted_sequence", {
     (*input_sorted_sequence).dims()}},
     {"values", {
     (*input_values).dims()}}};
     phi::AttributeMap attrs;
     attrs["out_int32"] = out_int32;
     attrs["right"] = right;
     phi::RecordOpInfoSupplement("searchsorted", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("searchsorted infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SearchsortedInferMeta(MakeMetaTensor(*input_sorted_sequence), MakeMetaTensor(*input_values), out_int32, right, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("searchsorted compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_sorted_sequence, *input_values, out_int32, right, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor segment_pool(const Tensor& x, const Tensor& segment_ids, const std::string& pooltype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, segment_ids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "segment_pool API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "segment_pool", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("segment_pool", kernel_data_type);
  }
  VLOG(6) << "segment_pool kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_segment_ids = PrepareData(segment_ids, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"segment_ids", {
     (*input_segment_ids).dims()}}};
     phi::AttributeMap attrs;
     attrs["pooltype"] = pooltype;
     phi::RecordOpInfoSupplement("segment_pool", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("segment_pool infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SegmentPoolInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_segment_ids), pooltype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("segment_pool compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_segment_ids, pooltype, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor selu(const Tensor& x, float scale, float alpha) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "selu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "selu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("selu", kernel_data_type);
  }
  VLOG(6) << "selu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["scale"] = scale;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("selu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("selu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("selu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, scale, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor send_u_recv(const Tensor& x, const Tensor& src_index, const Tensor& dst_index, const std::string& reduce_op, const IntArray& out_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "send_u_recv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_u_recv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_u_recv", kernel_data_type);
  }
  VLOG(6) << "send_u_recv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduce_op"] = reduce_op;
     attrs["out_size"] = out_size.GetData();
     phi::RecordOpInfoSupplement("send_u_recv", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_u_recv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SendURecvInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_src_index), MakeMetaTensor(*input_dst_index), reduce_op, out_size, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const phi::IntArray&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_u_recv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_src_index, *input_dst_index, reduce_op, phi::IntArray(out_size), kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor send_ue_recv(const Tensor& x, const Tensor& y, const Tensor& src_index, const Tensor& dst_index, const std::string& message_op, const std::string& reduce_op, const IntArray& out_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "send_ue_recv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_ue_recv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_ue_recv", kernel_data_type);
  }
  VLOG(6) << "send_ue_recv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}}};
     phi::AttributeMap attrs;
     attrs["message_op"] = message_op;
     attrs["reduce_op"] = reduce_op;
     attrs["out_size"] = out_size.GetData();
     phi::RecordOpInfoSupplement("send_ue_recv", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_ue_recv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SendUERecvInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_src_index), MakeMetaTensor(*input_dst_index), message_op, reduce_op, out_size, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, const phi::IntArray&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_ue_recv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, message_op, reduce_op, phi::IntArray(out_size), kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor send_uv(const Tensor& x, const Tensor& y, const Tensor& src_index, const Tensor& dst_index, const std::string& message_op) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "send_uv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_uv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_uv", kernel_data_type);
  }
  VLOG(6) << "send_uv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}}};
     phi::AttributeMap attrs;
     attrs["message_op"] = message_op;
     phi::RecordOpInfoSupplement("send_uv", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_uv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SendUVInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_src_index), MakeMetaTensor(*input_dst_index), message_op, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_uv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, message_op, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sequence_conv(const Tensor& x, const paddle::optional<Tensor>& padding_data, const Tensor& filter, int context_length, bool padding_trainable, int context_start, int context_stride) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, padding_data, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sequence_conv API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sequence_conv", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sequence_conv", kernel_data_type);
  }
  VLOG(6) << "sequence_conv kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_padding_data = PrepareData(padding_data, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> padding_data_record_shapes;
     if(input_padding_data){
       padding_data_record_shapes.push_back((*input_padding_data).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"padding_data", padding_data_record_shapes},
     {"filter", {
     (*input_filter).dims()}}};
     phi::AttributeMap attrs;
     attrs["context_length"] = context_length;
     attrs["padding_trainable"] = padding_trainable;
     attrs["context_start"] = context_start;
     attrs["context_stride"] = context_stride;
     phi::RecordOpInfoSupplement("sequence_conv", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sequence_conv infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SequenceConvInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_padding_data), MakeMetaTensor(*input_filter), context_length, padding_trainable, context_start, context_stride, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, bool, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sequence_conv compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_padding_data, *input_filter, context_length, padding_trainable, context_start, context_stride, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sequence_mask(const Tensor& x, const Scalar& max_len, DataType out_dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sequence_mask API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sequence_mask_scalar", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sequence_mask", kernel_data_type);
  }
  VLOG(6) << "sequence_mask_scalar kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (max_len.dtype()) {
      case DataType::FLOAT32:
          attrs["max_len"] = static_cast<float>(max_len.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max_len"] = static_cast<double>(max_len.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max_len"] = static_cast<float>(max_len.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max_len"] = static_cast<float>(max_len.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max_len"] = static_cast<int32_t>(max_len.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max_len"] = static_cast<int64_t>(max_len.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max_len"] = static_cast<int16_t>(max_len.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max_len"] = static_cast<int8_t>(max_len.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max_len"] = static_cast<uint16_t>(max_len.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max_len"] = static_cast<uint8_t>(max_len.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max_len"] = static_cast<bool>(max_len.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max_len"] = static_cast<float>(max_len.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max_len"] = static_cast<double>(max_len.to<complex128>());
          break;
      default:
          attrs["max_len"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("sequence_mask", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sequence_mask infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SequenceMaskScalarInferMeta(MakeMetaTensor(*input_x), max_len, out_dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sequence_mask compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(max_len), out_dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sequence_pool(const Tensor& x, bool is_test, const std::string& pooltype, float pad_value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sequence_pool API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sequence_pool", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sequence_pool", kernel_data_type);
  }
  VLOG(6) << "sequence_pool kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_test"] = is_test;
     attrs["pooltype"] = pooltype;
     attrs["pad_value"] = pad_value;
     phi::RecordOpInfoSupplement("sequence_pool", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sequence_pool infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SequencePoolInferMeta(MakeMetaTensor(*input_x), is_test, pooltype, pad_value, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, const std::string&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sequence_pool compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, is_test, pooltype, pad_value, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor set_value_with_tensor(const Tensor& x, const Tensor& values, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, values);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "set_value_with_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value_with_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value_with_tensor", kernel_data_type);
  }
  VLOG(6) << "set_value_with_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"values", {
     (*input_values).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     phi::RecordOpInfoSupplement("set_value_with_tensor", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value_with_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SetValueInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value_with_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_values, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& set_value_with_tensor_(Tensor& x, const Tensor& values, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, values);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "set_value_with_tensor API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value_with_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value_with_tensor", kernel_data_type);
  }
  VLOG(6) << "set_value_with_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"values", {
     (*input_values).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     phi::RecordOpInfoSupplement("set_value_with_tensor", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value_with_tensor infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SetValueInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value_with_tensor compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_values, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor&, paddle::optional<Tensor>&> sgd_(Tensor& param, const Tensor& learning_rate, const Tensor& grad, paddle::optional<Tensor>& master_param, bool multi_precision) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param, learning_rate, grad, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (param.is_dense_tensor() && learning_rate.is_dense_tensor() && grad.is_dense_tensor() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sgd", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sgd_", kernel_data_type);
    }
    VLOG(6) << "sgd kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareData(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("sgd_", input_shapes, attrs);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(std::get<1>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sgd_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

    phi::SgdInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(*input_grad), MakeMetaTensor(input_master_param), multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sgd_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_learning_rate, *input_grad, input_master_param, multi_precision, kernel_out_0, kernel_out_1);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);

    return api_output;
  }

  if (param.is_dense_tensor() && learning_rate.is_dense_tensor() && grad.is_selected_rows() && (!master_param || master_param->is_dense_tensor())) {

    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sgd_dense_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sgd_", kernel_data_type);
    }
    VLOG(6) << "sgd_dense_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareData(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {});

    auto input_master_param = PrepareData(master_param, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("sgd_", input_shapes, attrs);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(std::get<1>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sgd_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

    phi::SgdInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(*input_grad), MakeMetaTensor(input_master_param), multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::SelectedRows&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sgd_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_learning_rate, *input_grad, input_master_param, multi_precision, kernel_out_0, kernel_out_1);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);

    return api_output;
  }

  if (param.is_selected_rows() && learning_rate.is_dense_tensor() && grad.is_selected_rows() && (!master_param || master_param->is_selected_rows())) {

    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sgd_sparse_param_sparse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sgd_", kernel_data_type);
    }
    VLOG(6) << "sgd_sparse_param_sparse_grad kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_param = PrepareDataForSelectedRows(param, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
    auto input_grad = PrepareDataForSelectedRows(grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {});

    auto input_master_param = PrepareDataForSelectedRows(master_param, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<phi::DDim> master_param_record_shapes;
       if(input_master_param){
         master_param_record_shapes.push_back((*input_master_param).dims());
       }
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"param", {
       (*input_param).dims()}},
       {"learning_rate", {
       (*input_learning_rate).dims()}},
       {"grad", {
       (*input_grad).dims()}},
       {"master_param",
       master_param_record_shapes}};
       phi::AttributeMap attrs;
       attrs["multi_precision"] = multi_precision;
       phi::RecordOpInfoSupplement("sgd_", input_shapes, attrs);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param, master_param};
    auto kernel_out_0 = SetSelectedRowsKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetSelectedRowsKernelOutput(std::get<1>(api_output).get_ptr());
    auto backup0 = ProcessStrideBackup(&kernel_out_0);
    auto backup1 = ProcessStrideBackup(&kernel_out_1);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sgd_ infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_param = *input_param;
    phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
    phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

    phi::SgdInferMeta(MakeMetaTensor(origin_input_param), MakeMetaTensor(*input_learning_rate), MakeMetaTensor(*input_grad), MakeMetaTensor(input_master_param), multi_precision, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::DenseTensor&, const phi::SelectedRows&, const paddle::optional<phi::SelectedRows>&, bool, phi::SelectedRows*, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sgd_ compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_param, *input_learning_rate, *input_grad, input_master_param, multi_precision, kernel_out_0, kernel_out_1);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out_0, backup0);
    TransStride(dev_ctx, kernel_out_1, backup1);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (sgd_) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor shape(const Tensor& input) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (input.is_dense_tensor()) {

    VLOG(6) << "shape API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "shape", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("shape", kernel_data_type);
    }
    VLOG(6) << "shape kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"input", {
       (*input_input).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("shape", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("shape infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ShapeInferMeta(MakeMetaTensor(*input_input), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("shape compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_input, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (input.is_selected_rows()) {

    VLOG(6) << "shape API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "shape_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("shape", kernel_data_type);
    }
    VLOG(6) << "shape_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_input = PrepareDataForSelectedRows(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {true});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"input", {
       (*input_input).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("shape", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("shape infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ShapeInferMeta(MakeMetaTensor(*input_input), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("shape compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_input, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (shape) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor shard_index(const Tensor& input, int index_num, int nshards, int shard_id, int ignore_value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "shard_index API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "shard_index", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("shard_index", kernel_data_type);
  }
  VLOG(6) << "shard_index kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["index_num"] = index_num;
     attrs["nshards"] = nshards;
     attrs["shard_id"] = shard_id;
     attrs["ignore_value"] = ignore_value;
     phi::RecordOpInfoSupplement("shard_index", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("shard_index infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ShardIndexInferMeta(MakeMetaTensor(*input_input), index_num, nshards, shard_id, ignore_value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("shard_index compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, index_num, nshards, shard_id, ignore_value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor share_data(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "share_data API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "share_data", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("share_data", kernel_data_type);
    }
    VLOG(6) << "share_data kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("share_data", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("share_data infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ShareDataInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("share_data compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "share_data API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "share_data_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("share_data", kernel_data_type);
    }
    VLOG(6) << "share_data_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("share_data", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("share_data infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ShareDataInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("share_data compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (share_data) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> shuffle_batch(const Tensor& x, const Tensor& seed, int startup_seed) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, seed);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "shuffle_batch API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "shuffle_batch", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("shuffle_batch", kernel_data_type);
  }
  VLOG(6) << "shuffle_batch kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed = PrepareData(seed, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"seed", {
     (*input_seed).dims()}}};
     phi::AttributeMap attrs;
     attrs["startup_seed"] = startup_seed;
     phi::RecordOpInfoSupplement("shuffle_batch", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("shuffle_batch infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::ShuffleBatchInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_seed), startup_seed, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("shuffle_batch compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_seed, startup_seed, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor shuffle_channel(const Tensor& x, int group) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "shuffle_channel API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "shuffle_channel", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("shuffle_channel", kernel_data_type);
  }
  VLOG(6) << "shuffle_channel kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["group"] = group;
     phi::RecordOpInfoSupplement("shuffle_channel", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("shuffle_channel infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ShuffleChannelInferMeta(MakeMetaTensor(*input_x), group, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("shuffle_channel compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, group, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sigmoid(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid", kernel_data_type);
  }
  VLOG(6) << "sigmoid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sigmoid", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& sigmoid_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid", kernel_data_type);
  }
  VLOG(6) << "sigmoid kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sigmoid", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor sigmoid_cross_entropy_with_logits(const Tensor& x, const Tensor& label, const paddle::optional<Tensor>& pos_weight, bool normalize, int ignore_index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, pos_weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid_cross_entropy_with_logits API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_cross_entropy_with_logits", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_cross_entropy_with_logits", kernel_data_type);
  }
  VLOG(6) << "sigmoid_cross_entropy_with_logits kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pos_weight = PrepareData(pos_weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> pos_weight_record_shapes;
     if(input_pos_weight){
       pos_weight_record_shapes.push_back((*input_pos_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"pos_weight",
     pos_weight_record_shapes}};
     phi::AttributeMap attrs;
     attrs["normalize"] = normalize;
     attrs["ignore_index"] = ignore_index;
     phi::RecordOpInfoSupplement("sigmoid_cross_entropy_with_logits", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_label), MakeMetaTensor(input_pos_weight), normalize, ignore_index, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, input_pos_weight, normalize, ignore_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& sigmoid_cross_entropy_with_logits_(Tensor& x, const Tensor& label, const paddle::optional<Tensor>& pos_weight, bool normalize, int ignore_index) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, pos_weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid_cross_entropy_with_logits API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_cross_entropy_with_logits", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_cross_entropy_with_logits", kernel_data_type);
  }
  VLOG(6) << "sigmoid_cross_entropy_with_logits kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pos_weight = PrepareData(pos_weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> pos_weight_record_shapes;
     if(input_pos_weight){
       pos_weight_record_shapes.push_back((*input_pos_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"pos_weight",
     pos_weight_record_shapes}};
     phi::AttributeMap attrs;
     attrs["normalize"] = normalize;
     attrs["ignore_index"] = ignore_index;
     phi::RecordOpInfoSupplement("sigmoid_cross_entropy_with_logits", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_label), MakeMetaTensor(input_pos_weight), normalize, ignore_index, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_label, input_pos_weight, normalize, ignore_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor sign(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sign", kernel_data_type);
  }
  VLOG(6) << "sign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sign", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor silu(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "silu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "silu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("silu", kernel_data_type);
  }
  VLOG(6) << "silu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("silu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("silu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("silu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sin(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sin", kernel_data_type);
  }
  VLOG(6) << "sin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sin", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& sin_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sin API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sin", kernel_data_type);
  }
  VLOG(6) << "sin kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sin", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sin infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sin compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor sinh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sinh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sinh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sinh", kernel_data_type);
  }
  VLOG(6) << "sinh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sinh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sinh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sinh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& sinh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sinh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sinh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sinh", kernel_data_type);
  }
  VLOG(6) << "sinh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sinh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sinh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sinh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor slice(const Tensor& input, const std::vector<int64_t>& axes, const IntArray& starts, const IntArray& ends, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "slice API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slice", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("slice", kernel_data_type);
  }
  VLOG(6) << "slice kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["infer_flags"] = infer_flags;
     attrs["decrease_axis"] = decrease_axis;
     phi::RecordOpInfoSupplement("slice", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("slice infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SliceRawInferMeta(MakeMetaTensor(*input_input), axes, starts, ends, infer_flags, decrease_axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("slice compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, axes, phi::IntArray(starts), phi::IntArray(ends), infer_flags, decrease_axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor slogdet(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "slogdet API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slogdet", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("slogdet", kernel_data_type);
  }
  VLOG(6) << "slogdet kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("slogdet", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("slogdet infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("slogdet compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor softplus(const Tensor& x, float beta, float threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softplus API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softplus", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softplus", kernel_data_type);
  }
  VLOG(6) << "softplus kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["beta"] = beta;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("softplus", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softplus infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softplus compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, beta, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor softshrink(const Tensor& x, float threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softshrink API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softshrink", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softshrink", kernel_data_type);
  }
  VLOG(6) << "softshrink kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("softshrink", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softshrink infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softshrink compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor softsign(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softsign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softsign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softsign", kernel_data_type);
  }
  VLOG(6) << "softsign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("softsign", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softsign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softsign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor solve(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "solve API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "solve", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("solve", kernel_data_type);
  }
  VLOG(6) << "solve kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("solve", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("solve infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SolveInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("solve compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sparse_attention(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& offset, const Tensor& columns, const paddle::optional<Tensor>& key_padding_mask, const paddle::optional<Tensor>& attn_mask) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(q);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, offset, columns, key_padding_mask, attn_mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sparse_attention API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sparse_attention", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sparse_attention", kernel_data_type);
  }
  VLOG(6) << "sparse_attention kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_offset = PrepareData(offset, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_columns = PrepareData(columns, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_key_padding_mask = PrepareData(key_padding_mask, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> key_padding_mask_record_shapes;
     if(input_key_padding_mask){
       key_padding_mask_record_shapes.push_back((*input_key_padding_mask).dims());
     }
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"offset", {
     (*input_offset).dims()}},
     {"columns", {
     (*input_columns).dims()}},
     {"key_padding_mask", key_padding_mask_record_shapes},
     {"attn_mask",
     attn_mask_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sparse_attention", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sparse_attention infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::SparseAttentionInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), MakeMetaTensor(*input_offset), MakeMetaTensor(*input_columns), MakeMetaTensor(input_key_padding_mask), MakeMetaTensor(input_attn_mask), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sparse_attention compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_offset, *input_columns, input_key_padding_mask, input_attn_mask, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor spectral_norm(const Tensor& weight, const Tensor& u, const Tensor& v, int dim, int power_iters, float eps) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(weight);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(weight, u, v);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "spectral_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "spectral_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("spectral_norm", kernel_data_type);
  }
  VLOG(6) << "spectral_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u = PrepareData(u, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"weight", {
     (*input_weight).dims()}},
     {"u", {
     (*input_u).dims()}},
     {"v", {
     (*input_v).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim"] = dim;
     attrs["power_iters"] = power_iters;
     attrs["eps"] = eps;
     phi::RecordOpInfoSupplement("spectral_norm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("spectral_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SpectralNormInferMeta(MakeMetaTensor(*input_weight), MakeMetaTensor(*input_u), MakeMetaTensor(*input_v), dim, power_iters, eps, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("spectral_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_weight, *input_u, *input_v, dim, power_iters, eps, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::vector<Tensor> split(const Tensor& x, const IntArray& sections, const Scalar& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "split API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "split", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("split", kernel_data_type);
  }
  VLOG(6) << "split kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["sections"] = sections.GetData();
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("split", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(sections.size(), &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("split infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::SplitInferMeta(MakeMetaTensor(*input_x), sections, axis, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::Scalar&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("split compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(sections), phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::vector<Tensor> split_with_num(const Tensor& x, int num, const Scalar& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "split_with_num API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "split_with_num", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("split_with_num", kernel_data_type);
  }
  VLOG(6) << "split_with_num kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["num"] = num;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("split_with_num", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(num, &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("split_with_num infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::SplitWithNumInferMeta(MakeMetaTensor(*input_x), num, axis, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const phi::Scalar&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("split_with_num compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, num, phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sqrt(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sqrt", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt", kernel_data_type);
    }
    VLOG(6) << "sqrt kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("sqrt", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sqrt compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sqrt_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt", kernel_data_type);
    }
    VLOG(6) << "sqrt_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("sqrt", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sqrt compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (sqrt) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor& sqrt_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sqrt", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt", kernel_data_type);
    }
    VLOG(6) << "sqrt kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("sqrt", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sqrt compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sqrt_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt", kernel_data_type);
    }
    VLOG(6) << "sqrt_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("sqrt", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("sqrt infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("sqrt compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (sqrt) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor square(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor()) {

    VLOG(6) << "square API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "square", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("square", kernel_data_type);
    }
    VLOG(6) << "square kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("square", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("square infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("square compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows()) {

    VLOG(6) << "square API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "square_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("square", kernel_data_type);
    }
    VLOG(6) << "square_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("square", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("square infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("square compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (square) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor squared_l2_norm(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squared_l2_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squared_l2_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("squared_l2_norm", kernel_data_type);
  }
  VLOG(6) << "squared_l2_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("squared_l2_norm", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("squared_l2_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SquaredL2NormInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("squared_l2_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor squeeze(const Tensor& x, const IntArray& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squeeze API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squeeze", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("squeeze", kernel_data_type);
  }
  VLOG(6) << "squeeze kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("squeeze", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  kernel_out->ShareBufferWith(*input_x);
  kernel_out->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("squeeze infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SqueezeInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("squeeze compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

    phi::DenseTensor * x_remap = static_cast<phi::DenseTensor*>(x.impl().get());
    x_remap->ShareBufferWith(*kernel_out);
    kernel_out->ShareInplaceVersionCounterWith(*x_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& squeeze_(Tensor& x, const IntArray& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squeeze API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squeeze", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("squeeze", kernel_data_type);
  }
  VLOG(6) << "squeeze kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("squeeze", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("squeeze infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SqueezeInferMeta(MakeMetaTensor(origin_input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("squeeze compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor stack(const std::vector<Tensor>& x, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "stack API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stack", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("stack", kernel_data_type);
  }
  VLOG(6) << "stack kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("stack", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("stack infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StackInferMeta(x_metas, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("stack compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor standard_gamma(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "standard_gamma API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "standard_gamma", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("standard_gamma", kernel_data_type);
  }
  VLOG(6) << "standard_gamma kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("standard_gamma", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("standard_gamma infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("standard_gamma compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor stanh(const Tensor& x, float scale_a, float scale_b) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "stanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("stanh", kernel_data_type);
  }
  VLOG(6) << "stanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["scale_a"] = scale_a;
     attrs["scale_b"] = scale_b;
     phi::RecordOpInfoSupplement("stanh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("stanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("stanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, scale_a, scale_b, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor stft(const Tensor& x, const Tensor& window, int n_fft, int hop_length, bool normalized, bool onesided) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, window);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "stft API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stft", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("stft", kernel_data_type);
  }
  VLOG(6) << "stft kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_window = PrepareData(window, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"window", {
     (*input_window).dims()}}};
     phi::AttributeMap attrs;
     attrs["n_fft"] = n_fft;
     attrs["hop_length"] = hop_length;
     attrs["normalized"] = normalized;
     attrs["onesided"] = onesided;
     phi::RecordOpInfoSupplement("stft", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("stft infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StftInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_window), n_fft, hop_length, normalized, onesided, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("stft compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_window, n_fft, hop_length, normalized, onesided, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor strided_slice(const Tensor& x, const std::vector<int>& axes, const IntArray& starts, const IntArray& ends, const IntArray& strides) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "strided_slice API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "strided_slice", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("strided_slice", kernel_data_type);
  }
  VLOG(6) << "strided_slice kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["strides"] = strides.GetData();
     phi::RecordOpInfoSupplement("strided_slice", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("strided_slice infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedSliceInferMeta(MakeMetaTensor(*input_x), axes, starts, ends, strides, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("strided_slice compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axes, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(strides), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor sum(const Tensor& x, const IntArray& axis, DataType dtype, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sum", kernel_data_type);
  }
  VLOG(6) << "sum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("sum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SumInferMeta(MakeMetaTensor(*input_x), axis, dtype, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, DataType, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), dtype, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> svd(const Tensor& x, bool full_matrices) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "svd API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "svd", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("svd", kernel_data_type);
  }
  VLOG(6) << "svd kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["full_matrices"] = full_matrices;
     phi::RecordOpInfoSupplement("svd", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("svd infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::SvdInferMeta(MakeMetaTensor(*input_x), full_matrices, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("svd compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, full_matrices, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor swiglu(const Tensor& x, const paddle::optional<Tensor>& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "swiglu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "swiglu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("swiglu", kernel_data_type);
  }
  VLOG(6) << "swiglu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> y_record_shapes;
     if(input_y){
       y_record_shapes.push_back((*input_y).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y",
     y_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("swiglu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("swiglu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SwiGLUInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("swiglu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor swish(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "swish API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "swish", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("swish", kernel_data_type);
  }
  VLOG(6) << "swish kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("swish", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("swish infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("swish compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, Tensor&, Tensor, Tensor, Tensor> sync_batch_norm_(const Tensor& x, Tensor& mean, Tensor& variance, const Tensor& scale, const Tensor& bias, bool is_test, float momentum, float epsilon, const std::string& data_format, bool use_global_stats, bool trainable_statistics) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, mean, variance, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sync_batch_norm_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sync_batch_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sync_batch_norm_", kernel_data_type);
  }
  VLOG(6) << "sync_batch_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}}};
     phi::AttributeMap attrs;
     attrs["is_test"] = is_test;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["data_format"] = data_format;
     attrs["use_global_stats"] = use_global_stats;
     attrs["trainable_statistics"] = trainable_statistics;
     phi::RecordOpInfoSupplement("sync_batch_norm_", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor&, Tensor&, Tensor, Tensor, Tensor> api_output{Tensor(), mean, variance, Tensor(), Tensor(), Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sync_batch_norm_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_mean = *input_mean;

  auto origin_input_variance = *input_variance;
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::BatchNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(origin_input_mean), MakeMetaTensor(origin_input_variance), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), is_test, momentum, epsilon, data_format, use_global_stats, trainable_statistics, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, float, float, const std::string&, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sync_batch_norm_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, origin_input_mean, origin_input_variance, *input_scale, *input_bias, is_test, momentum, epsilon, data_format, use_global_stats, trainable_statistics, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);

  return api_output;
}

PADDLE_API Tensor sync_calc_stream(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sync_calc_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sync_calc_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sync_calc_stream", kernel_data_type);
  }
  VLOG(6) << "sync_calc_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sync_calc_stream", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sync_calc_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sync_calc_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& sync_calc_stream_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sync_calc_stream API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sync_calc_stream", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sync_calc_stream", kernel_data_type);
  }
  VLOG(6) << "sync_calc_stream kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sync_calc_stream", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sync_calc_stream infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sync_calc_stream compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor take_along_axis(const Tensor& arr, const Tensor& indices, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(arr);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(arr, indices);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "take_along_axis API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "take_along_axis", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("take_along_axis", kernel_data_type);
  }
  VLOG(6) << "take_along_axis kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_arr = PrepareData(arr, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"arr", {
     (*input_arr).dims()}},
     {"indices", {
     (*input_indices).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("take_along_axis", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("take_along_axis infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TakeAlongAxisInferMeta(MakeMetaTensor(*input_arr), MakeMetaTensor(*input_indices), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("take_along_axis compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor tan(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tan", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tan", kernel_data_type);
  }
  VLOG(6) << "tan kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tan", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tan infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tan compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& tan_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tan API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tan", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tan", kernel_data_type);
  }
  VLOG(6) << "tan kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tan", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tan infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tan compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor tanh(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh", kernel_data_type);
  }
  VLOG(6) << "tanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& tanh_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh", kernel_data_type);
  }
  VLOG(6) << "tanh kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor tanh_shrink(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh_shrink API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_shrink", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh_shrink", kernel_data_type);
  }
  VLOG(6) << "tanh_shrink kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh_shrink", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh_shrink infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh_shrink compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> tdm_child(const Tensor& x, const Tensor& tree_info, int child_nums, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, tree_info);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tdm_child API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tdm_child", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tdm_child", kernel_data_type);
  }
  VLOG(6) << "tdm_child kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_tree_info = PrepareData(tree_info, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"tree_info", {
     (*input_tree_info).dims()}}};
     phi::AttributeMap attrs;
     attrs["child_nums"] = child_nums;
     phi::RecordOpInfoSupplement("tdm_child", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tdm_child infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::TdmChildInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_tree_info), child_nums, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tdm_child compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_tree_info, child_nums, dtype, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> tdm_sampler(const Tensor& x, const Tensor& travel, const Tensor& layer, bool output_positive, const std::vector<int>& neg_samples_num_list, const std::vector<int>& layer_offset_lod, int seed, int dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, travel, layer);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tdm_sampler API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tdm_sampler", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tdm_sampler", kernel_data_type);
  }
  VLOG(6) << "tdm_sampler kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_travel = PrepareData(travel, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_layer = PrepareData(layer, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"travel", {
     (*input_travel).dims()}},
     {"layer", {
     (*input_layer).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_positive"] = output_positive;
     attrs["neg_samples_num_list"] = neg_samples_num_list;
     attrs["layer_offset_lod"] = layer_offset_lod;
     attrs["seed"] = seed;
     attrs["dtype"] = dtype;
     phi::RecordOpInfoSupplement("tdm_sampler", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tdm_sampler infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::TdmSamplerInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_travel), MakeMetaTensor(*input_layer), output_positive, neg_samples_num_list, layer_offset_lod, seed, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, const std::vector<int>&, const std::vector<int>&, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tdm_sampler compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_travel, *input_layer, output_positive, neg_samples_num_list, layer_offset_lod, seed, dtype, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor temporal_shift(const Tensor& x, int seg_num, float shift_ratio, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "temporal_shift API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "temporal_shift", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("temporal_shift", kernel_data_type);
  }
  VLOG(6) << "temporal_shift kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["seg_num"] = seg_num;
     attrs["shift_ratio"] = shift_ratio;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("temporal_shift", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("temporal_shift infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TemporalShiftInferMeta(MakeMetaTensor(*input_x), seg_num, shift_ratio, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, float, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("temporal_shift compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, seg_num, shift_ratio, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor tensor_unfold(const Tensor& input, int64_t axis, int64_t size, int64_t step) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tensor_unfold API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tensor_unfold", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tensor_unfold", kernel_data_type);
  }
  VLOG(6) << "tensor_unfold kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["size"] = size;
     attrs["step"] = step;
     phi::RecordOpInfoSupplement("tensor_unfold", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tensor_unfold infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int64_t, int64_t, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tensor_unfold compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, axis, size, step, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor thresholded_relu(const Tensor& x, float threshold, float value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "thresholded_relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "thresholded_relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("thresholded_relu", kernel_data_type);
  }
  VLOG(6) << "thresholded_relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     attrs["value"] = value;
     phi::RecordOpInfoSupplement("thresholded_relu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("thresholded_relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("thresholded_relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, threshold, value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& thresholded_relu_(Tensor& x, float threshold, float value) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "thresholded_relu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "thresholded_relu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("thresholded_relu", kernel_data_type);
  }
  VLOG(6) << "thresholded_relu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     attrs["value"] = value;
     phi::RecordOpInfoSupplement("thresholded_relu", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("thresholded_relu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("thresholded_relu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, threshold, value, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> top_p_sampling(const Tensor& x, const Tensor& ps, const paddle::optional<Tensor>& threshold, const paddle::optional<Tensor>& topp_seed, int seed, int k, const std::string& mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, ps, threshold, topp_seed);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "top_p_sampling API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "top_p_sampling", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("top_p_sampling", kernel_data_type);
  }
  VLOG(6) << "top_p_sampling kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_ps = PrepareData(ps, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_threshold = PrepareData(threshold, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_topp_seed = PrepareData(topp_seed, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> threshold_record_shapes;
     if(input_threshold){
       threshold_record_shapes.push_back((*input_threshold).dims());
     }
     std::vector<phi::DDim> topp_seed_record_shapes;
     if(input_topp_seed){
       topp_seed_record_shapes.push_back((*input_topp_seed).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"ps", {
     (*input_ps).dims()}},
     {"threshold", threshold_record_shapes},
     {"topp_seed",
     topp_seed_record_shapes}};
     phi::AttributeMap attrs;
     attrs["seed"] = seed;
     attrs["k"] = k;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("top_p_sampling", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("top_p_sampling infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::TopPSamplingInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_ps), MakeMetaTensor(input_threshold), MakeMetaTensor(input_topp_seed), seed, k, mode, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("top_p_sampling compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_ps, input_threshold, input_topp_seed, seed, k, mode, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> topk(const Tensor& x, const Scalar& k, int axis, bool largest, bool sorted) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "topk API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "topk", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("topk", kernel_data_type);
  }
  VLOG(6) << "topk kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
    switch (k.dtype()) {
      case DataType::FLOAT32:
          attrs["k"] = static_cast<float>(k.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["k"] = static_cast<double>(k.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["k"] = static_cast<float>(k.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["k"] = static_cast<float>(k.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["k"] = static_cast<int32_t>(k.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["k"] = static_cast<int64_t>(k.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["k"] = static_cast<int16_t>(k.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["k"] = static_cast<int8_t>(k.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["k"] = static_cast<uint16_t>(k.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["k"] = static_cast<uint8_t>(k.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["k"] = static_cast<bool>(k.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["k"] = static_cast<float>(k.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["k"] = static_cast<double>(k.to<complex128>());
          break;
      default:
          attrs["k"] = "";
          break;
    }
     attrs["axis"] = axis;
     attrs["largest"] = largest;
     attrs["sorted"] = sorted;
     phi::RecordOpInfoSupplement("topk", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("topk infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::TopKInferMeta(MakeMetaTensor(*input_x), k, axis, largest, sorted, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, int, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("topk compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(k), axis, largest, sorted, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor trace(const Tensor& x, int offset, int axis1, int axis2) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trace", kernel_data_type);
  }
  VLOG(6) << "trace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["axis1"] = axis1;
     attrs["axis2"] = axis2;
     phi::RecordOpInfoSupplement("trace", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TraceInferMeta(MakeMetaTensor(*input_x), offset, axis1, axis2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, offset, axis1, axis2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor trans_layout(const Tensor& x, const std::vector<int>& perm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trans_layout API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trans_layout", kernel_data_type);
  }
  VLOG(6) << "transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["perm"] = perm;
     phi::RecordOpInfoSupplement("trans_layout", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trans_layout infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TransposeInferMeta(MakeMetaTensor(*input_x), perm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trans_layout compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, perm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor transpose(const Tensor& x, const std::vector<int>& perm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "transpose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("transpose", kernel_data_type);
  }
  VLOG(6) << "transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["perm"] = perm;
     phi::RecordOpInfoSupplement("transpose", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("transpose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TransposeInferMeta(MakeMetaTensor(*input_x), perm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("transpose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, perm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& transpose_(Tensor& x, const std::vector<int>& perm) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "transpose API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "transpose", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("transpose", kernel_data_type);
  }
  VLOG(6) << "transpose kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["perm"] = perm;
     phi::RecordOpInfoSupplement("transpose", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("transpose infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TransposeInferMeta(MakeMetaTensor(origin_input_x), perm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("transpose compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, perm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor triangular_solve(const Tensor& x, const Tensor& y, bool upper, bool transpose, bool unitriangular) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "triangular_solve API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triangular_solve", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triangular_solve", kernel_data_type);
  }
  VLOG(6) << "triangular_solve kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     attrs["transpose"] = transpose;
     attrs["unitriangular"] = unitriangular;
     phi::RecordOpInfoSupplement("triangular_solve", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triangular_solve infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TriangularSolveInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), upper, transpose, unitriangular, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triangular_solve compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, upper, transpose, unitriangular, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor tril(const Tensor& x, int diagonal) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tril API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tril", kernel_data_type);
  }
  VLOG(6) << "tril kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("tril", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tril infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TrilInferMeta(MakeMetaTensor(*input_x), diagonal, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tril compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& tril_(Tensor& x, int diagonal) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tril API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tril", kernel_data_type);
  }
  VLOG(6) << "tril kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("tril", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tril infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TrilInferMeta(MakeMetaTensor(origin_input_x), diagonal, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tril compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor tril_indices(int rows, int cols, int offset, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "tril_indices API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril_indices", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tril_indices", kernel_data_type);
  }
  VLOG(6) << "tril_indices kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["rows"] = rows;
     attrs["cols"] = cols;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("tril_indices", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tril_indices infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TrilIndicesInferMeta(rows, cols, offset, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, int, int, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tril_indices compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, rows, cols, offset, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor trilinear_interp(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trilinear_interp API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trilinear_interp", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trilinear_interp", kernel_data_type);
  }
  VLOG(6) << "trilinear_interp kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor",
     scale_tensor_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("trilinear_interp", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trilinear_interp infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_out_size), size_tensor_metas, MakeMetaTensor(input_scale_tensor), data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trilinear_interp compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor triu(const Tensor& x, int diagonal) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "triu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triu", kernel_data_type);
  }
  VLOG(6) << "triu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("triu", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TriuInferMeta(MakeMetaTensor(*input_x), diagonal, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& triu_(Tensor& x, int diagonal) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "triu API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triu", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triu", kernel_data_type);
  }
  VLOG(6) << "triu kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("triu", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triu infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TriuInferMeta(MakeMetaTensor(origin_input_x), diagonal, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triu compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor triu_indices(int row, int col, int offset, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "triu_indices API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triu_indices", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triu_indices", kernel_data_type);
  }
  VLOG(6) << "triu_indices kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["row"] = row;
     attrs["col"] = col;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("triu_indices", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triu_indices infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TriuIndicesInferMeta(row, col, offset, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, int, int, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triu_indices compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, row, col, offset, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor trunc(const Tensor& input) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trunc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trunc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trunc", kernel_data_type);
  }
  VLOG(6) << "trunc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("trunc", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trunc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trunc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& trunc_(Tensor& input) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trunc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trunc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trunc", kernel_data_type);
  }
  VLOG(6) << "trunc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("trunc", input_shapes, attrs);
  }

  Tensor& api_output = input;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trunc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_input = *input_input;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trunc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_input, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor truncated_gaussian_random(const std::vector<int>& shape, float mean, float std, int seed, float a, float b, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "truncated_gaussian_random API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "truncated_gaussian_random", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("truncated_gaussian_random", kernel_data_type);
  }
  VLOG(6) << "truncated_gaussian_random kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape;
     attrs["mean"] = mean;
     attrs["std"] = std;
     attrs["seed"] = seed;
     attrs["a"] = a;
     attrs["b"] = b;
     phi::RecordOpInfoSupplement("truncated_gaussian_random", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("truncated_gaussian_random infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TruncatedGaussianRandomInferMeta(shape, mean, std, seed, a, b, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<int>&, float, float, int, float, float, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("truncated_gaussian_random compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, shape, mean, std, seed, a, b, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::vector<Tensor> unbind(const Tensor& input, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unbind API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unbind", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unbind", kernel_data_type);
  }
  VLOG(6) << "unbind kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("unbind", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis], &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unbind infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnbindInferMeta(MakeMetaTensor(*input_input), axis, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unbind compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor unfold(const Tensor& x, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unfold API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unfold", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unfold", kernel_data_type);
  }
  VLOG(6) << "unfold kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_sizes"] = kernel_sizes;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     phi::RecordOpInfoSupplement("unfold", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unfold infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnfoldInferMeta(MakeMetaTensor(*input_x), kernel_sizes, strides, paddings, dilations, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unfold compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_sizes, strides, paddings, dilations, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor uniform(const IntArray& shape, DataType dtype, const Scalar& min, const Scalar& max, int seed, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);


  VLOG(6) << "uniform API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform", kernel_data_type);
  }
  VLOG(6) << "uniform kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
    switch (min.dtype()) {
      case DataType::FLOAT32:
          attrs["min"] = static_cast<float>(min.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["min"] = static_cast<double>(min.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["min"] = static_cast<float>(min.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["min"] = static_cast<float>(min.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["min"] = static_cast<bool>(min.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["min"] = static_cast<float>(min.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["min"] = static_cast<double>(min.to<complex128>());
          break;
      default:
          attrs["min"] = "";
          break;
    }
    switch (max.dtype()) {
      case DataType::FLOAT32:
          attrs["max"] = static_cast<float>(max.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max"] = static_cast<double>(max.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max"] = static_cast<float>(max.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max"] = static_cast<float>(max.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max"] = static_cast<bool>(max.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max"] = static_cast<float>(max.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max"] = static_cast<double>(max.to<complex128>());
          break;
      default:
          attrs["max"] = "";
          break;
    }
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("uniform", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("uniform infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UniformRandomInferMeta(shape, dtype, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::IntArray&, DataType, const phi::Scalar&, const phi::Scalar&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("uniform compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, phi::IntArray(shape), dtype, phi::Scalar(min), phi::Scalar(max), seed, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor uniform_inplace(const Tensor& x, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "uniform_inplace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_inplace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform_inplace", kernel_data_type);
  }
  VLOG(6) << "uniform_inplace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["min"] = min;
     attrs["max"] = max;
     attrs["seed"] = seed;
     attrs["diag_num"] = diag_num;
     attrs["diag_step"] = diag_step;
     attrs["diag_val"] = diag_val;
     phi::RecordOpInfoSupplement("uniform_inplace", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("uniform_inplace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UniformRandomInplaceInferMeta(MakeMetaTensor(*input_x), min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("uniform_inplace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, min, max, seed, diag_num, diag_step, diag_val, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& uniform_inplace_(Tensor& x, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "uniform_inplace API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_inplace", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform_inplace", kernel_data_type);
  }
  VLOG(6) << "uniform_inplace kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["min"] = min;
     attrs["max"] = max;
     attrs["seed"] = seed;
     attrs["diag_num"] = diag_num;
     attrs["diag_step"] = diag_step;
     attrs["diag_val"] = diag_val;
     phi::RecordOpInfoSupplement("uniform_inplace", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("uniform_inplace infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UniformRandomInplaceInferMeta(MakeMetaTensor(origin_input_x), min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("uniform_inplace compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, min, max, seed, diag_num, diag_step, diag_val, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor uniform_random_batch_size_like(const Tensor& input, const std::vector<int>& shape, int input_dim_idx, int output_dim_idx, float min, float max, int seed, int diag_num, int diag_step, float diag_val, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (input.is_dense_tensor()) {

    VLOG(6) << "uniform_random_batch_size_like API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "uniform_random_batch_size_like", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform_random_batch_size_like", kernel_data_type);
    }
    VLOG(6) << "uniform_random_batch_size_like kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"input", {
       (*input_input).dims()}}};
       phi::AttributeMap attrs;
       attrs["shape"] = shape;
       attrs["input_dim_idx"] = input_dim_idx;
       attrs["output_dim_idx"] = output_dim_idx;
       attrs["min"] = min;
       attrs["max"] = max;
       attrs["seed"] = seed;
       attrs["diag_num"] = diag_num;
       attrs["diag_step"] = diag_step;
       attrs["diag_val"] = diag_val;
       phi::RecordOpInfoSupplement("uniform_random_batch_size_like", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("uniform_random_batch_size_like infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UniformRandomBatchSizeLikeInferMeta(MakeMetaTensor(*input_input), shape, input_dim_idx, output_dim_idx, min, max, seed, diag_num, diag_step, diag_val, dtype, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, int, int, float, float, int, int, int, float, DataType, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("uniform_random_batch_size_like compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_input, shape, input_dim_idx, output_dim_idx, min, max, seed, diag_num, diag_step, diag_val, dtype, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (input.is_selected_rows()) {

    VLOG(6) << "uniform_random_batch_size_like API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "uniform_random_batch_size_like_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform_random_batch_size_like", kernel_data_type);
    }
    VLOG(6) << "uniform_random_batch_size_like_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_input = PrepareDataForSelectedRows(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"input", {
       (*input_input).dims()}}};
       phi::AttributeMap attrs;
       attrs["shape"] = shape;
       attrs["input_dim_idx"] = input_dim_idx;
       attrs["output_dim_idx"] = output_dim_idx;
       attrs["min"] = min;
       attrs["max"] = max;
       attrs["seed"] = seed;
       attrs["diag_num"] = diag_num;
       attrs["diag_step"] = diag_step;
       attrs["diag_val"] = diag_val;
       phi::RecordOpInfoSupplement("uniform_random_batch_size_like", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("uniform_random_batch_size_like infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::UniformRandomBatchSizeLikeInferMeta(MakeMetaTensor(*input_input), shape, input_dim_idx, output_dim_idx, min, max, seed, diag_num, diag_step, diag_val, dtype, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const std::vector<int>&, int, int, float, float, int, int, int, float, DataType, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("uniform_random_batch_size_like compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_input, shape, input_dim_idx, output_dim_idx, min, max, seed, diag_num, diag_step, diag_val, dtype, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (uniform_random_batch_size_like) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> unique_consecutive(const Tensor& x, bool return_inverse, bool return_counts, const std::vector<int>& axis, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unique_consecutive API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unique_consecutive", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unique_consecutive", kernel_data_type);
  }
  VLOG(6) << "unique_consecutive kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["return_inverse"] = return_inverse;
     attrs["return_counts"] = return_counts;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("unique_consecutive", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unique_consecutive infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::UniqueConsecutiveInferMeta(MakeMetaTensor(*input_x), return_inverse, return_counts, axis, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, bool, const std::vector<int>&, DataType, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unique_consecutive compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, return_inverse, return_counts, axis, dtype, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor unpool(const Tensor& x, const Tensor& indices, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const IntArray& output_size, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unpool API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unpool", kernel_data_type);
  }
  VLOG(6) << "unpool kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}}};
     phi::AttributeMap attrs;
     attrs["ksize"] = ksize;
     attrs["strides"] = strides;
     attrs["padding"] = padding;
     attrs["output_size"] = output_size.GetData();
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("unpool", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unpool infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnpoolInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_indices), ksize, strides, padding, output_size, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unpool compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, ksize, strides, padding, phi::IntArray(output_size), data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor unpool3d(const Tensor& x, const Tensor& indices, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_size, const std::string& data_format) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unpool3d API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unpool3d", kernel_data_type);
  }
  VLOG(6) << "unpool3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}}};
     phi::AttributeMap attrs;
     attrs["ksize"] = ksize;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_size"] = output_size;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("unpool3d", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unpool3d infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Unpool3dInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_indices), ksize, strides, paddings, output_size, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unpool3d compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, ksize, strides, paddings, output_size, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor unsqueeze(const Tensor& x, const IntArray& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unsqueeze API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unsqueeze", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unsqueeze", kernel_data_type);
  }
  VLOG(6) << "unsqueeze kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("unsqueeze", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  kernel_out->ShareBufferWith(*input_x);
  kernel_out->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unsqueeze infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnsqueezeInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unsqueeze compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

    phi::DenseTensor * x_remap = static_cast<phi::DenseTensor*>(x.impl().get());
    x_remap->ShareBufferWith(*kernel_out);
    kernel_out->ShareInplaceVersionCounterWith(*x_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& unsqueeze_(Tensor& x, const IntArray& axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unsqueeze API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unsqueeze", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unsqueeze", kernel_data_type);
  }
  VLOG(6) << "unsqueeze kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("unsqueeze", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unsqueeze infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnsqueezeInferMeta(MakeMetaTensor(origin_input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unsqueeze compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::vector<Tensor> unstack(const Tensor& x, int axis, int num) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unstack API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unstack", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unstack", kernel_data_type);
  }
  VLOG(6) << "unstack kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["num"] = num;
     phi::RecordOpInfoSupplement("unstack", input_shapes, attrs);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(num, &api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unstack infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnStackInferMeta(MakeMetaTensor(*input_x), axis, num, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, int, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unstack compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, num, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&, Tensor&, Tensor&, Tensor&> update_loss_scaling_(std::vector<Tensor>& x, const Tensor& found_infinite, Tensor& prev_loss_scaling, Tensor& in_good_steps, Tensor& in_bad_steps, int incr_every_n_steps, int decr_every_n_nan_or_inf, float incr_ratio, float decr_ratio, const Scalar& stop_update) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, found_infinite, prev_loss_scaling, in_good_steps, in_bad_steps);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "update_loss_scaling_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "update_loss_scaling", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("update_loss_scaling_", kernel_data_type);
  }
  VLOG(6) << "update_loss_scaling kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_x;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_x_vec;
  if (kernel_result.has_fallback_cpu) {
    input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_x.resize(input_x_vec->size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      input_x[i] = &input_x_vec->at(i);
    }
  }
  else {
    input_x = TensorToConstDenseTensorPtr(x);
  }
  auto input_found_infinite = PrepareData(found_infinite, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_prev_loss_scaling = PrepareData(prev_loss_scaling, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_good_steps = PrepareData(in_good_steps, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_in_bad_steps = PrepareData(in_bad_steps, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"found_infinite", {
     (*input_found_infinite).dims()}},
     {"prev_loss_scaling", {
     (*input_prev_loss_scaling).dims()}},
     {"in_good_steps", {
     (*input_in_good_steps).dims()}},
     {"in_bad_steps", {
     (*input_in_bad_steps).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     attrs["incr_every_n_steps"] = incr_every_n_steps;
     attrs["decr_every_n_nan_or_inf"] = decr_every_n_nan_or_inf;
     attrs["incr_ratio"] = incr_ratio;
     attrs["decr_ratio"] = decr_ratio;
    switch (stop_update.dtype()) {
      case DataType::FLOAT32:
          attrs["stop_update"] = static_cast<float>(stop_update.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["stop_update"] = static_cast<double>(stop_update.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["stop_update"] = static_cast<float>(stop_update.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["stop_update"] = static_cast<float>(stop_update.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["stop_update"] = static_cast<int32_t>(stop_update.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["stop_update"] = static_cast<int64_t>(stop_update.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["stop_update"] = static_cast<int16_t>(stop_update.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["stop_update"] = static_cast<int8_t>(stop_update.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["stop_update"] = static_cast<uint16_t>(stop_update.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["stop_update"] = static_cast<uint8_t>(stop_update.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["stop_update"] = static_cast<bool>(stop_update.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["stop_update"] = static_cast<float>(stop_update.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["stop_update"] = static_cast<double>(stop_update.to<complex128>());
          break;
      default:
          attrs["stop_update"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("update_loss_scaling_", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>&, Tensor&, Tensor&, Tensor&> api_output{x, prev_loss_scaling, in_good_steps, in_bad_steps};
  auto kernel_out_0 = SetInplaceVectorKernelOutput(x.size(), &std::get<0>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_0.size(); ++i) {
      kernel_out_0[i] = const_cast<phi::DenseTensor*>(input_x[i]);
    }
  }
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("update_loss_scaling_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto origin_input_prev_loss_scaling = *input_prev_loss_scaling;

  auto origin_input_in_good_steps = *input_in_good_steps;

  auto origin_input_in_bad_steps = *input_in_bad_steps;

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::UpdateLossScalingInferMeta(x_metas, MakeMetaTensor(*input_found_infinite), MakeMetaTensor(origin_input_prev_loss_scaling), MakeMetaTensor(origin_input_in_good_steps), MakeMetaTensor(origin_input_in_bad_steps), kernel_out_0_metas, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, float, const phi::Scalar&, std::vector<phi::DenseTensor*>, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("update_loss_scaling_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, *input_found_infinite, origin_input_prev_loss_scaling, origin_input_in_good_steps, origin_input_in_bad_steps, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, phi::Scalar(stop_update), kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    for (size_t i = 0; i < x.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(x.at(i).impl().get());
      *target_ptr = *kernel_out_0.at(i);
    }
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);

  return api_output;
}

PADDLE_API Tensor view_dtype(const Tensor& input, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "view_dtype API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "view_dtype", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("view_dtype", kernel_data_type);
  }
  VLOG(6) << "view_dtype kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("view_dtype", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("view_dtype infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("view_dtype compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor view_shape(const Tensor& input, const std::vector<int64_t>& dims) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "view_shape API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "view_shape", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("view_shape", kernel_data_type);
  }
  VLOG(6) << "view_shape kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}}};
     phi::AttributeMap attrs;
     attrs["dims"] = dims;
     phi::RecordOpInfoSupplement("view_shape", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("view_shape infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("view_shape compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, dims, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> viterbi_decode(const Tensor& potentials, const Tensor& transition_params, const Tensor& lengths, bool include_bos_eos_tag) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(potentials);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(potentials, transition_params, lengths);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "viterbi_decode API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "viterbi_decode", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("viterbi_decode", kernel_data_type);
  }
  VLOG(6) << "viterbi_decode kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_potentials = PrepareData(potentials, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_transition_params = PrepareData(transition_params, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_lengths = PrepareData(lengths, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"potentials", {
     (*input_potentials).dims()}},
     {"transition_params", {
     (*input_transition_params).dims()}},
     {"lengths", {
     (*input_lengths).dims()}}};
     phi::AttributeMap attrs;
     attrs["include_bos_eos_tag"] = include_bos_eos_tag;
     phi::RecordOpInfoSupplement("viterbi_decode", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("viterbi_decode infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ViterbiDecodeInferMeta(MakeMetaTensor(*input_potentials), MakeMetaTensor(*input_transition_params), MakeMetaTensor(*input_lengths), include_bos_eos_tag, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("viterbi_decode compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_potentials, *input_transition_params, *input_lengths, include_bos_eos_tag, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor warpctc(const Tensor& logits, const Tensor& label, const paddle::optional<Tensor>& logits_length, const paddle::optional<Tensor>& labels_length, int blank, bool norm_by_times) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(logits);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(logits, label, logits_length, labels_length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "warpctc API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "warpctc", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("warpctc", kernel_data_type);
  }
  VLOG(6) << "warpctc kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_logits = PrepareData(logits, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_logits_length = PrepareData(logits_length, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_labels_length = PrepareData(labels_length, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> logits_length_record_shapes;
     if(input_logits_length){
       logits_length_record_shapes.push_back((*input_logits_length).dims());
     }
     std::vector<phi::DDim> labels_length_record_shapes;
     if(input_labels_length){
       labels_length_record_shapes.push_back((*input_labels_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"logits", {
     (*input_logits).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"logits_length", logits_length_record_shapes},
     {"labels_length",
     labels_length_record_shapes}};
     phi::AttributeMap attrs;
     attrs["blank"] = blank;
     attrs["norm_by_times"] = norm_by_times;
     phi::RecordOpInfoSupplement("warpctc", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("warpctc infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::WarpctcInferMeta(MakeMetaTensor(*input_logits), MakeMetaTensor(*input_label), MakeMetaTensor(input_logits_length), MakeMetaTensor(input_labels_length), blank, norm_by_times, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("warpctc compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_logits, *input_label, input_logits_length, input_labels_length, blank, norm_by_times, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor warprnnt(const Tensor& input, const Tensor& label, const Tensor& input_lengths, const Tensor& label_lengths, int blank, float fastemit_lambda) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, input_lengths, label_lengths);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "warprnnt API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "warprnnt", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("warprnnt", kernel_data_type);
  }
  VLOG(6) << "warprnnt kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_input_lengths = PrepareData(input_lengths, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label_lengths = PrepareData(label_lengths, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"input_lengths", {
     (*input_input_lengths).dims()}},
     {"label_lengths", {
     (*input_label_lengths).dims()}}};
     phi::AttributeMap attrs;
     attrs["blank"] = blank;
     attrs["fastemit_lambda"] = fastemit_lambda;
     phi::RecordOpInfoSupplement("warprnnt", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("warprnnt infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::WarprnntInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), MakeMetaTensor(*input_input_lengths), MakeMetaTensor(*input_label_lengths), blank, fastemit_lambda, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("warprnnt compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, *input_input_lengths, *input_label_lengths, blank, fastemit_lambda, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor weight_dequantize(const Tensor& x, const Tensor& scale, const std::string& algo, DataType out_dtype, int group_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(out_dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "weight_dequantize API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "weight_dequantize", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("weight_dequantize", kernel_data_type);
  }
  VLOG(6) << "weight_dequantize kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["algo"] = algo;
     attrs["group_size"] = group_size;
     phi::RecordOpInfoSupplement("weight_dequantize", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("weight_dequantize infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::WeightDequantizeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), algo, out_dtype, group_size, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, DataType, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("weight_dequantize compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, algo, out_dtype, group_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor weight_only_linear(const Tensor& x, const Tensor& weight, const paddle::optional<Tensor>& bias, const Tensor& weight_scale, const std::string& weight_dtype, int arch, int group_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, weight, bias, weight_scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "weight_only_linear API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "weight_only_linear", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("weight_only_linear", kernel_data_type);
  }
  VLOG(6) << "weight_only_linear kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight_scale = PrepareData(weight_scale, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"bias", bias_record_shapes},
     {"weight_scale", {
     (*input_weight_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["weight_dtype"] = weight_dtype;
     attrs["arch"] = arch;
     attrs["group_size"] = group_size;
     phi::RecordOpInfoSupplement("weight_only_linear", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("weight_only_linear infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::WeightOnlyLinearInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), MakeMetaTensor(*input_weight_scale), weight_dtype, arch, group_size, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("weight_only_linear compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_weight, input_bias, *input_weight_scale, weight_dtype, arch, group_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> weight_quantize(const Tensor& x, const std::string& algo, int arch, int group_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "weight_quantize API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "weight_quantize", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("weight_quantize", kernel_data_type);
  }
  VLOG(6) << "weight_quantize kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["algo"] = algo;
     attrs["arch"] = arch;
     attrs["group_size"] = group_size;
     phi::RecordOpInfoSupplement("weight_quantize", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("weight_quantize infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::WeightQuantizeInferMeta(MakeMetaTensor(*input_x), algo, arch, group_size, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::string&, int, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("weight_quantize compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, algo, arch, group_size, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> weighted_sample_neighbors(const Tensor& row, const Tensor& colptr, const Tensor& edge_weight, const Tensor& input_nodes, const paddle::optional<Tensor>& eids, int sample_size, bool return_eids) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(row, colptr, edge_weight, input_nodes, eids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "weighted_sample_neighbors API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "weighted_sample_neighbors", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("weighted_sample_neighbors", kernel_data_type);
  }
  VLOG(6) << "weighted_sample_neighbors kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_row = PrepareData(row, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_colptr = PrepareData(colptr, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_edge_weight = PrepareData(edge_weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_input_nodes = PrepareData(input_nodes, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_eids = PrepareData(eids, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> eids_record_shapes;
     if(input_eids){
       eids_record_shapes.push_back((*input_eids).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"row", {
     (*input_row).dims()}},
     {"colptr", {
     (*input_colptr).dims()}},
     {"edge_weight", {
     (*input_edge_weight).dims()}},
     {"input_nodes", {
     (*input_input_nodes).dims()}},
     {"eids",
     eids_record_shapes}};
     phi::AttributeMap attrs;
     attrs["sample_size"] = sample_size;
     attrs["return_eids"] = return_eids;
     phi::RecordOpInfoSupplement("weighted_sample_neighbors", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("weighted_sample_neighbors infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::WeightedSampleNeighborsInferMeta(MakeMetaTensor(*input_row), MakeMetaTensor(*input_colptr), MakeMetaTensor(*input_edge_weight), MakeMetaTensor(*input_input_nodes), MakeMetaTensor(input_eids), sample_size, return_eids, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("weighted_sample_neighbors compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_row, *input_colptr, *input_edge_weight, *input_input_nodes, input_eids, sample_size, return_eids, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor where(const Tensor& condition, const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(condition, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "where API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "where", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("where", kernel_data_type);
  }
  VLOG(6) << "where kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_condition = PrepareData(condition, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"condition", {
     (*input_condition).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("where", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("where infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::WhereInferMeta(MakeMetaTensor(*input_condition), MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("where compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_condition, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& where_(const Tensor& condition, Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(condition, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "where API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "where", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("where", kernel_data_type);
  }
  VLOG(6) << "where kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_condition = PrepareData(condition, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"condition", {
     (*input_condition).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("where", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("where infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::WhereInferMeta(MakeMetaTensor(*input_condition), MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("where compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_condition, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> yolo_box(const Tensor& x, const Tensor& img_size, const std::vector<int>& anchors, int class_num, float conf_thresh, int downsample_ratio, bool clip_bbox, float scale_x_y, bool iou_aware, float iou_aware_factor) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, img_size);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolo_box API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_box", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("yolo_box", kernel_data_type);
  }
  VLOG(6) << "yolo_box kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_img_size = PrepareData(img_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"img_size", {
     (*input_img_size).dims()}}};
     phi::AttributeMap attrs;
     attrs["anchors"] = anchors;
     attrs["class_num"] = class_num;
     attrs["conf_thresh"] = conf_thresh;
     attrs["downsample_ratio"] = downsample_ratio;
     attrs["clip_bbox"] = clip_bbox;
     attrs["scale_x_y"] = scale_x_y;
     attrs["iou_aware"] = iou_aware;
     attrs["iou_aware_factor"] = iou_aware_factor;
     phi::RecordOpInfoSupplement("yolo_box", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("yolo_box infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::YoloBoxInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_img_size), anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, int, float, int, bool, float, bool, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("yolo_box compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor yolo_box_head(const Tensor& x, const std::vector<int>& anchors, int class_num) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolo_box_head API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_box_head", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("yolo_box_head", kernel_data_type);
  }
  VLOG(6) << "yolo_box_head kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["anchors"] = anchors;
     attrs["class_num"] = class_num;
     phi::RecordOpInfoSupplement("yolo_box_head", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("yolo_box_head infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::YoloBoxHeadInferMeta(MakeMetaTensor(*input_x), anchors, class_num, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("yolo_box_head compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, anchors, class_num, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> yolo_box_post(const Tensor& boxes0, const Tensor& boxes1, const Tensor& boxes2, const Tensor& image_shape, const Tensor& image_scale, const std::vector<int>& anchors0, const std::vector<int>& anchors1, const std::vector<int>& anchors2, int class_num, float conf_thresh, int downsample_ratio0, int downsample_ratio1, int downsample_ratio2, bool clip_bbox, float scale_x_y, float nms_threshold) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(boxes0);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(boxes0, boxes1, boxes2, image_shape, image_scale);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolo_box_post API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_box_post", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("yolo_box_post", kernel_data_type);
  }
  VLOG(6) << "yolo_box_post kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_boxes0 = PrepareData(boxes0, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes1 = PrepareData(boxes1, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes2 = PrepareData(boxes2, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_image_shape = PrepareData(image_shape, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_image_scale = PrepareData(image_scale, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"boxes0", {
     (*input_boxes0).dims()}},
     {"boxes1", {
     (*input_boxes1).dims()}},
     {"boxes2", {
     (*input_boxes2).dims()}},
     {"image_shape", {
     (*input_image_shape).dims()}},
     {"image_scale", {
     (*input_image_scale).dims()}}};
     phi::AttributeMap attrs;
     attrs["anchors0"] = anchors0;
     attrs["anchors1"] = anchors1;
     attrs["anchors2"] = anchors2;
     attrs["class_num"] = class_num;
     attrs["conf_thresh"] = conf_thresh;
     attrs["downsample_ratio0"] = downsample_ratio0;
     attrs["downsample_ratio1"] = downsample_ratio1;
     attrs["downsample_ratio2"] = downsample_ratio2;
     attrs["clip_bbox"] = clip_bbox;
     attrs["scale_x_y"] = scale_x_y;
     attrs["nms_threshold"] = nms_threshold;
     phi::RecordOpInfoSupplement("yolo_box_post", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("yolo_box_post infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::YoloBoxPostInferMeta(MakeMetaTensor(*input_boxes0), MakeMetaTensor(*input_boxes1), MakeMetaTensor(*input_boxes2), MakeMetaTensor(*input_image_shape), MakeMetaTensor(*input_image_scale), anchors0, anchors1, anchors2, class_num, conf_thresh, downsample_ratio0, downsample_ratio1, downsample_ratio2, clip_bbox, scale_x_y, nms_threshold, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, int, float, int, int, int, bool, float, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("yolo_box_post compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_boxes0, *input_boxes1, *input_boxes2, *input_image_shape, *input_image_scale, anchors0, anchors1, anchors2, class_num, conf_thresh, downsample_ratio0, downsample_ratio1, downsample_ratio2, clip_bbox, scale_x_y, nms_threshold, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor yolo_loss(const Tensor& x, const Tensor& gt_box, const Tensor& gt_label, const paddle::optional<Tensor>& gt_score, const std::vector<int>& anchors, const std::vector<int>& anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, gt_box, gt_label, gt_score);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolo_loss API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_loss", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("yolo_loss", kernel_data_type);
  }
  VLOG(6) << "yolo_loss kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_box = PrepareData(gt_box, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_label = PrepareData(gt_label, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_score = PrepareData(gt_score, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> gt_score_record_shapes;
     if(input_gt_score){
       gt_score_record_shapes.push_back((*input_gt_score).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"gt_box", {
     (*input_gt_box).dims()}},
     {"gt_label", {
     (*input_gt_label).dims()}},
     {"gt_score",
     gt_score_record_shapes}};
     phi::AttributeMap attrs;
     attrs["anchors"] = anchors;
     attrs["anchor_mask"] = anchor_mask;
     attrs["class_num"] = class_num;
     attrs["ignore_thresh"] = ignore_thresh;
     attrs["downsample_ratio"] = downsample_ratio;
     attrs["use_label_smooth"] = use_label_smooth;
     attrs["scale_x_y"] = scale_x_y;
     phi::RecordOpInfoSupplement("yolo_loss", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("yolo_loss infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::YoloLossInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_gt_box), MakeMetaTensor(*input_gt_label), MakeMetaTensor(input_gt_score), anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, int, float, int, bool, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("yolo_loss compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_gt_box, *input_gt_label, input_gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return std::get<0>(api_output);
}

PADDLE_API Tensor zeros(const IntArray& shape, DataType dtype, const Place& place) {
  return full(shape, 0, dtype, place);
}
PADDLE_API Tensor zeros_like(const Tensor& x, DataType dtype, const Place& place) {
  return full_like(x, 0, dtype, place);
}
PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> chunk_eval(const Tensor& inference, const Tensor& label, const paddle::optional<Tensor>& seq_length, int num_chunk_types, const std::string& chunk_scheme, const std::vector<int>& excluded_chunk_types) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(DataType::FLOAT32);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(inference, label, seq_length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "chunk_eval API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "chunk_eval", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("chunk_eval", kernel_data_type);
  }
  VLOG(6) << "chunk_eval kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_inference = PrepareData(inference, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seq_length = PrepareData(seq_length, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> seq_length_record_shapes;
     if(input_seq_length){
       seq_length_record_shapes.push_back((*input_seq_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"inference", {
     (*input_inference).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"seq_length",
     seq_length_record_shapes}};
     phi::AttributeMap attrs;
     attrs["num_chunk_types"] = num_chunk_types;
     attrs["chunk_scheme"] = chunk_scheme;
     attrs["excluded_chunk_types"] = excluded_chunk_types;
     phi::RecordOpInfoSupplement("chunk_eval", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("chunk_eval infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::ChunkEvalInferMeta(MakeMetaTensor(*input_inference), MakeMetaTensor(*input_label), MakeMetaTensor(input_seq_length), num_chunk_types, chunk_scheme, excluded_chunk_types, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, const std::string&, const std::vector<int>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("chunk_eval compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_inference, *input_label, input_seq_length, num_chunk_types, chunk_scheme, excluded_chunk_types, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor moe(const Tensor& x, const Tensor& gate, const Tensor& bmm0, const Tensor& bias0, const Tensor& bmm1, const Tensor& bias1, const std::string& act_type) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, gate, bmm0, bias0, bmm1, bias1);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "moe API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "moe", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("moe", kernel_data_type);
  }
  VLOG(6) << "moe kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gate = PrepareData(gate, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bmm0 = PrepareData(bmm0, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias0 = PrepareData(bias0, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bmm1 = PrepareData(bmm1, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias1 = PrepareData(bias1, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"gate", {
     (*input_gate).dims()}},
     {"bmm0", {
     (*input_bmm0).dims()}},
     {"bias0", {
     (*input_bias0).dims()}},
     {"bmm1", {
     (*input_bmm1).dims()}},
     {"bias1", {
     (*input_bias1).dims()}}};
     phi::AttributeMap attrs;
     attrs["act_type"] = act_type;
     phi::RecordOpInfoSupplement("moe", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("moe infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MoeInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_gate), MakeMetaTensor(*input_bmm0), MakeMetaTensor(*input_bias0), MakeMetaTensor(*input_bmm1), MakeMetaTensor(*input_bias1), act_type, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("moe compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_gate, *input_bmm0, *input_bias0, *input_bmm1, *input_bias1, act_type, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor number_count(const Tensor& numbers, int upper_range) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(numbers);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(numbers);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "number_count API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "number_count", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("number_count", kernel_data_type);
  }
  VLOG(6) << "number_count kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_numbers = PrepareData(numbers, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"numbers", {
     (*input_numbers).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper_range"] = upper_range;
     phi::RecordOpInfoSupplement("number_count", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("number_count infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NumberCountInferMeta(MakeMetaTensor(*input_numbers), upper_range, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("number_count compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_numbers, upper_range, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor add(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add", kernel_data_type);
  }
  VLOG(6) << "add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("add", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& add_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add", kernel_data_type);
  }
  VLOG(6) << "add kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("add", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor add_n(const std::vector<Tensor>& inputs) {
  return add_n_impl(inputs);
}
PADDLE_API Tensor arange(const Tensor& start, const Tensor& end, const Tensor& step, DataType dtype, const Place& place) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(start, end, step);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "arange API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "arange_tensor", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("arange", kernel_data_type);
  }
  VLOG(6) << "arange_tensor kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_start = PrepareData(start, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_end = PrepareData(end, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_step = PrepareData(step, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"start", {
     (*input_start).dims()}},
     {"end", {
     (*input_end).dims()}},
     {"step", {
     (*input_step).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("arange", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("arange infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ArangeTensorInferMeta(MakeMetaTensor(*input_start), MakeMetaTensor(*input_end), MakeMetaTensor(*input_step), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("arange compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_start, *input_end, *input_step, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor assign(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign", kernel_data_type);
  }
  VLOG(6) << "assign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("assign", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& assign_(Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign", kernel_data_type);
  }
  VLOG(6) << "assign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("assign", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> batch_norm(const Tensor& x, const Tensor& mean, const Tensor& variance, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, bool is_test, float momentum, float epsilon, const std::string& data_format, bool use_global_stats, bool trainable_statistics) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, mean, variance, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "batch_norm API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "batch_norm", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("batch_norm", kernel_data_type);
  }
  VLOG(6) << "batch_norm kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}},
     {"scale", scale_record_shapes},
     {"bias",
     bias_record_shapes}};
     phi::AttributeMap attrs;
     attrs["is_test"] = is_test;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["data_format"] = data_format;
     attrs["use_global_stats"] = use_global_stats;
     attrs["trainable_statistics"] = trainable_statistics;
     phi::RecordOpInfoSupplement("batch_norm", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
      kernel_out_1->ShareBufferWith(*input_mean);
      kernel_out_1->ShareInplaceVersionCounterWith(*input_mean);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
      kernel_out_2->ShareBufferWith(*input_variance);
      kernel_out_2->ShareInplaceVersionCounterWith(*input_variance);
      VLOG(3) << "Perform View between Output and Input Tensor, share allocation and inplace version.";
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("batch_norm infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_5(kernel_out_5, kernel_result.is_stride_kernel);

  phi::BatchNormInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mean), MakeMetaTensor(*input_variance), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), is_test, momentum, epsilon, data_format, use_global_stats, trainable_statistics, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr, kernel_out_5 ? &meta_out_5 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, bool, float, float, const std::string&, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("batch_norm compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mean, *input_variance, input_scale, input_bias, is_test, momentum, epsilon, data_format, use_global_stats, trainable_statistics, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);

    phi::DenseTensor * mean_remap = static_cast<phi::DenseTensor*>(mean.impl().get());
    mean_remap->ShareBufferWith(*kernel_out_1);
    kernel_out_1->ShareInplaceVersionCounterWith(*mean_remap);

    phi::DenseTensor * variance_remap = static_cast<phi::DenseTensor*>(variance.impl().get());
    variance_remap->ShareBufferWith(*kernel_out_2);
    kernel_out_2->ShareInplaceVersionCounterWith(*variance_remap);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor c_embedding(const Tensor& weight, const Tensor& x, int64_t start_index, int64_t vocab_size) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(weight);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(weight, x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "c_embedding API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_embedding", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_embedding", kernel_data_type);
  }
  VLOG(6) << "c_embedding kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"weight", {
     (*input_weight).dims()}},
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["start_index"] = start_index;
     attrs["vocab_size"] = vocab_size;
     phi::RecordOpInfoSupplement("c_embedding", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_embedding infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CEmbeddingInferMeta(MakeMetaTensor(*input_weight), MakeMetaTensor(*input_x), start_index, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_embedding compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_weight, *input_x, start_index, vocab_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>, std::vector<Tensor>, Tensor> distribute_fpn_proposals(const Tensor& fpn_rois, const paddle::optional<Tensor>& rois_num, int min_level, int max_level, int refer_level, int refer_scale, bool pixel_offset) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(fpn_rois);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(fpn_rois, rois_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "distribute_fpn_proposals API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "distribute_fpn_proposals", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("distribute_fpn_proposals", kernel_data_type);
  }
  VLOG(6) << "distribute_fpn_proposals kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_fpn_rois = PrepareData(fpn_rois, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_rois_num = PrepareData(rois_num, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> rois_num_record_shapes;
     if(input_rois_num){
       rois_num_record_shapes.push_back((*input_rois_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"fpn_rois", {
     (*input_fpn_rois).dims()}},
     {"rois_num",
     rois_num_record_shapes}};
     phi::AttributeMap attrs;
     attrs["min_level"] = min_level;
     attrs["max_level"] = max_level;
     attrs["refer_level"] = refer_level;
     attrs["refer_scale"] = refer_scale;
     attrs["pixel_offset"] = pixel_offset;
     phi::RecordOpInfoSupplement("distribute_fpn_proposals", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>, std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(max_level - min_level + 1, &std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(max_level - min_level + 1, &std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("distribute_fpn_proposals infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::DistributeFpnProposalsInferMeta(MakeMetaTensor(*input_fpn_rois), MakeMetaTensor(input_rois_num), min_level, max_level, refer_level, refer_scale, pixel_offset, kernel_out_0_metas, kernel_out_1_metas, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, int, int, int, int, bool, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("distribute_fpn_proposals compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_fpn_rois, input_rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor divide(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "divide API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "divide", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("divide", kernel_data_type);
  }
  VLOG(6) << "divide kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("divide", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("divide infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("divide compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& divide_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "divide API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "divide", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("divide", kernel_data_type);
  }
  VLOG(6) << "divide kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("divide", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("divide infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("divide compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> einsum(const std::vector<Tensor>& x, const std::string& equation) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "einsum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "einsum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("einsum", kernel_data_type);
  }
  VLOG(6) << "einsum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     attrs["equation"] = equation;
     phi::RecordOpInfoSupplement("einsum", input_shapes, attrs);
  }

  std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(x.size(), &std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(x.size(), &std::get<2>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("einsum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);

  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  phi::EinsumRawInferMeta(x_metas, equation, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1_metas, kernel_out_2_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::string&, phi::DenseTensor*, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("einsum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, equation, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor elementwise_pow(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elementwise_pow API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elementwise_pow", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elementwise_pow", kernel_data_type);
  }
  VLOG(6) << "elementwise_pow kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("elementwise_pow", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elementwise_pow infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elementwise_pow compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor embedding(const Tensor& x, const Tensor& weight, int64_t padding_idx, bool sparse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(weight);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor() && weight.is_dense_tensor()) {

    VLOG(6) << "embedding API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "embedding", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("embedding", kernel_data_type);
    }
    VLOG(6) << "embedding kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"weight", {
       (*input_weight).dims()}}};
       phi::AttributeMap attrs;
       attrs["padding_idx"] = padding_idx;
       attrs["sparse"] = sparse;
       phi::RecordOpInfoSupplement("embedding", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("embedding infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::EmbeddingInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), padding_idx, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("embedding compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_weight, padding_idx, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_dense_tensor() && weight.is_selected_rows()) {

    VLOG(6) << "embedding API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "sparse_weight_embedding", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("embedding", kernel_data_type);
    }
    VLOG(6) << "sparse_weight_embedding kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_weight = PrepareDataForSelectedRows(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {});

    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"weight", {
       (*input_weight).dims()}}};
       phi::AttributeMap attrs;
       attrs["padding_idx"] = padding_idx;
       attrs["sparse"] = sparse;
       phi::RecordOpInfoSupplement("embedding", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("embedding infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::EmbeddingInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), padding_idx, &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::SelectedRows&, int64_t, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("embedding compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_weight, padding_idx, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (embedding) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor embedding_grad_dense(const Tensor& x, const Tensor& weight, const Tensor& out_grad, int64_t padding_idx, bool sparse) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(weight);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, weight, out_grad);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "embedding_grad_dense API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "embedding_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("embedding_grad_dense", kernel_data_type);
  }
  VLOG(6) << "embedding_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["padding_idx"] = padding_idx;
     attrs["sparse"] = sparse;
     phi::RecordOpInfoSupplement("embedding_grad_dense", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("embedding_grad_dense infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_weight), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("embedding_grad_dense compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_weight, *input_out_grad, padding_idx, sparse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor equal(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("equal", kernel_data_type);
  }
  VLOG(6) << "equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("equal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& equal_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("equal", kernel_data_type);
  }
  VLOG(6) << "equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("equal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor floor_divide(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor_divide API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor_divide", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("floor_divide", kernel_data_type);
  }
  VLOG(6) << "floor_divide kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("floor_divide", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("floor_divide infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("floor_divide compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& floor_divide_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor_divide API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor_divide", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("floor_divide", kernel_data_type);
  }
  VLOG(6) << "floor_divide kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("floor_divide", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("floor_divide infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("floor_divide compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> fused_adam_(std::vector<Tensor>& params, const std::vector<Tensor>& grads, const Tensor& learning_rate, std::vector<Tensor>& moments1, std::vector<Tensor>& moments2, std::vector<Tensor>& beta1_pows, std::vector<Tensor>& beta2_pows, paddle::optional<std::vector<Tensor>>& master_params, const paddle::optional<Tensor>& skip_update, const Scalar& beta1, const Scalar& beta2, const Scalar& epsilon, int chunk_size, float weight_decay, bool use_adamw, bool multi_precision, bool use_global_beta_pow) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(params);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(params, grads, learning_rate, moments1, moments2, beta1_pows, beta2_pows, master_params, skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fused_adam_ API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_adam", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_adam_", kernel_data_type);
  }
  VLOG(6) << "fused_adam kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_params;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_params_vec;
  if (kernel_result.has_fallback_cpu) {
    input_params_vec = PrepareData(params, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_params.resize(input_params_vec->size());
    for (size_t i = 0; i < input_params.size(); ++i) {
      input_params[i] = &input_params_vec->at(i);
    }
  }
  else {
    input_params = TensorToConstDenseTensorPtr(params);
  }
  auto input_grads_vec = PrepareData(grads, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_grads(input_grads_vec->size());
  for (size_t i = 0; i < input_grads.size(); ++i) {
    input_grads[i] = &input_grads_vec->at(i);
  }
  auto input_learning_rate = PrepareData(learning_rate, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_moments1;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_moments1_vec;
  if (kernel_result.has_fallback_cpu) {
    input_moments1_vec = PrepareData(moments1, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_moments1.resize(input_moments1_vec->size());
    for (size_t i = 0; i < input_moments1.size(); ++i) {
      input_moments1[i] = &input_moments1_vec->at(i);
    }
  }
  else {
    input_moments1 = TensorToConstDenseTensorPtr(moments1);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_moments2;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_moments2_vec;
  if (kernel_result.has_fallback_cpu) {
    input_moments2_vec = PrepareData(moments2, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_moments2.resize(input_moments2_vec->size());
    for (size_t i = 0; i < input_moments2.size(); ++i) {
      input_moments2[i] = &input_moments2_vec->at(i);
    }
  }
  else {
    input_moments2 = TensorToConstDenseTensorPtr(moments2);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_beta1_pows;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_beta1_pows_vec;
  if (kernel_result.has_fallback_cpu) {
    input_beta1_pows_vec = PrepareData(beta1_pows, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_beta1_pows.resize(input_beta1_pows_vec->size());
    for (size_t i = 0; i < input_beta1_pows.size(); ++i) {
      input_beta1_pows[i] = &input_beta1_pows_vec->at(i);
    }
  }
  else {
    input_beta1_pows = TensorToConstDenseTensorPtr(beta1_pows);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  std::vector<const phi::DenseTensor*> input_beta2_pows;
  std::unique_ptr<std::vector<phi::DenseTensor>> input_beta2_pows_vec;
  if (kernel_result.has_fallback_cpu) {
    input_beta2_pows_vec = PrepareData(beta2_pows, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    input_beta2_pows.resize(input_beta2_pows_vec->size());
    for (size_t i = 0; i < input_beta2_pows.size(); ++i) {
      input_beta2_pows[i] = &input_beta2_pows_vec->at(i);
    }
  }
  else {
    input_beta2_pows = TensorToConstDenseTensorPtr(beta2_pows);
  }
  // inplace vector of tensors should also be transferred to CPU when kernel has fallen back
  paddle::optional<std::vector<const phi::DenseTensor*>> input_master_params;
  paddle::optional<std::vector<phi::DenseTensor>> input_master_params_vec;
  if (kernel_result.has_fallback_cpu) {
    input_master_params_vec = PrepareData(master_params, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if (input_master_params_vec){
      input_master_params = paddle::optional<std::vector<const phi::DenseTensor*>>(input_master_params_vec->size());
      for (size_t i = 0; i < input_master_params_vec->size(); ++i) {
        input_master_params->at(i) = &input_master_params_vec->at(i);
      }
    }
  }
  else {
    input_master_params = TensorToConstDenseTensorPtr(master_params);
  }
  auto input_skip_update = PrepareData(skip_update, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> skip_update_record_shapes;
     if(input_skip_update){
       skip_update_record_shapes.push_back((*input_skip_update).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"learning_rate", {
     (*input_learning_rate).dims()}},
     {"skip_update",
     skip_update_record_shapes}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_params.size());
     for (size_t i = 0; i < input_params.size(); ++i) {
       ddims_vec.emplace_back((*input_params[i]).dims());
     }
     input_shapes.emplace_back("params", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_grads.size());
     for (size_t i = 0; i < input_grads.size(); ++i) {
       ddims_vec.emplace_back((*input_grads[i]).dims());
     }
     input_shapes.emplace_back("grads", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_moments1.size());
     for (size_t i = 0; i < input_moments1.size(); ++i) {
       ddims_vec.emplace_back((*input_moments1[i]).dims());
     }
     input_shapes.emplace_back("moments1", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_moments2.size());
     for (size_t i = 0; i < input_moments2.size(); ++i) {
       ddims_vec.emplace_back((*input_moments2[i]).dims());
     }
     input_shapes.emplace_back("moments2", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_beta1_pows.size());
     for (size_t i = 0; i < input_beta1_pows.size(); ++i) {
       ddims_vec.emplace_back((*input_beta1_pows[i]).dims());
     }
     input_shapes.emplace_back("beta1_pows", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_beta2_pows.size());
     for (size_t i = 0; i < input_beta2_pows.size(); ++i) {
       ddims_vec.emplace_back((*input_beta2_pows[i]).dims());
     }
     input_shapes.emplace_back("beta2_pows", ddims_vec);
     ddims_vec.clear();
     if (input_master_params){
       ddims_vec.reserve(input_master_params->size());
       for (size_t i = 0; i < input_master_params->size(); ++i) {
         ddims_vec.emplace_back((*input_master_params->at(i)).dims());
       }
     }
     input_shapes.emplace_back("master_params", ddims_vec);
     phi::AttributeMap attrs;
    switch (beta1.dtype()) {
      case DataType::FLOAT32:
          attrs["beta1"] = static_cast<float>(beta1.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta1"] = static_cast<double>(beta1.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta1"] = static_cast<float>(beta1.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta1"] = static_cast<int32_t>(beta1.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta1"] = static_cast<int64_t>(beta1.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta1"] = static_cast<int16_t>(beta1.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta1"] = static_cast<int8_t>(beta1.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta1"] = static_cast<uint16_t>(beta1.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta1"] = static_cast<uint8_t>(beta1.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta1"] = static_cast<bool>(beta1.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta1"] = static_cast<float>(beta1.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta1"] = static_cast<double>(beta1.to<complex128>());
          break;
      default:
          attrs["beta1"] = "";
          break;
    }
    switch (beta2.dtype()) {
      case DataType::FLOAT32:
          attrs["beta2"] = static_cast<float>(beta2.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["beta2"] = static_cast<double>(beta2.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["beta2"] = static_cast<float>(beta2.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["beta2"] = static_cast<int32_t>(beta2.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["beta2"] = static_cast<int64_t>(beta2.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["beta2"] = static_cast<int16_t>(beta2.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["beta2"] = static_cast<int8_t>(beta2.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["beta2"] = static_cast<uint16_t>(beta2.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["beta2"] = static_cast<uint8_t>(beta2.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["beta2"] = static_cast<bool>(beta2.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["beta2"] = static_cast<float>(beta2.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["beta2"] = static_cast<double>(beta2.to<complex128>());
          break;
      default:
          attrs["beta2"] = "";
          break;
    }
    switch (epsilon.dtype()) {
      case DataType::FLOAT32:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["epsilon"] = static_cast<double>(epsilon.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["epsilon"] = static_cast<float>(epsilon.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["epsilon"] = static_cast<int32_t>(epsilon.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["epsilon"] = static_cast<int64_t>(epsilon.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["epsilon"] = static_cast<int16_t>(epsilon.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["epsilon"] = static_cast<int8_t>(epsilon.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["epsilon"] = static_cast<uint16_t>(epsilon.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["epsilon"] = static_cast<uint8_t>(epsilon.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["epsilon"] = static_cast<bool>(epsilon.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["epsilon"] = static_cast<float>(epsilon.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["epsilon"] = static_cast<double>(epsilon.to<complex128>());
          break;
      default:
          attrs["epsilon"] = "";
          break;
    }
     attrs["chunk_size"] = chunk_size;
     attrs["weight_decay"] = weight_decay;
     attrs["use_adamw"] = use_adamw;
     attrs["multi_precision"] = multi_precision;
     attrs["use_global_beta_pow"] = use_global_beta_pow;
     phi::RecordOpInfoSupplement("fused_adam_", input_shapes, attrs);
  }

  std::tuple<std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, std::vector<Tensor>&, paddle::optional<std::vector<Tensor>>&> api_output{params, moments1, moments2, beta1_pows, beta2_pows, master_params};
  auto kernel_out_0 = SetInplaceVectorKernelOutput(params.size(), &std::get<0>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_0.size(); ++i) {
      kernel_out_0[i] = const_cast<phi::DenseTensor*>(input_params[i]);
    }
  }
  auto kernel_out_1 = SetInplaceVectorKernelOutput(params.size(), &std::get<1>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_1.size(); ++i) {
      kernel_out_1[i] = const_cast<phi::DenseTensor*>(input_moments1[i]);
    }
  }
  auto kernel_out_2 = SetInplaceVectorKernelOutput(params.size(), &std::get<2>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_2.size(); ++i) {
      kernel_out_2[i] = const_cast<phi::DenseTensor*>(input_moments2[i]);
    }
  }
  auto kernel_out_3 = SetInplaceVectorKernelOutput(params.size(), &std::get<3>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_3.size(); ++i) {
      kernel_out_3[i] = const_cast<phi::DenseTensor*>(input_beta1_pows[i]);
    }
  }
  auto kernel_out_4 = SetInplaceVectorKernelOutput(params.size(), &std::get<4>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_4.size(); ++i) {
      kernel_out_4[i] = const_cast<phi::DenseTensor*>(input_beta2_pows[i]);
    }
  }
  auto kernel_out_5 = SetInplaceOptionalVectorKernelOutput(params.size(), std::get<5>(api_output));
  if (kernel_result.has_fallback_cpu) {
    for (size_t i = 0; i < kernel_out_5.size(); ++i) {
      kernel_out_5[i] = const_cast<phi::DenseTensor*>(input_master_params->at(i));
    }
  }
  auto backup0 = ProcessStrideBackup(&kernel_out_0);
  auto backup1 = ProcessStrideBackup(&kernel_out_1);
  auto backup2 = ProcessStrideBackup(&kernel_out_2);
  auto backup3 = ProcessStrideBackup(&kernel_out_3);
  auto backup4 = ProcessStrideBackup(&kernel_out_4);
  auto backup5 = ProcessStrideBackup(&kernel_out_5);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_adam_ infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto params_meta_vec = MakeMetaTensor(input_params);
  std::vector<const phi::MetaTensor*> params_metas(params_meta_vec.size());
  for (size_t i = 0; i < params_meta_vec.size(); ++i) {
    params_metas[i] = &params_meta_vec[i];
  }

  auto grads_meta_vec = MakeMetaTensor(input_grads);
  std::vector<const phi::MetaTensor*> grads_metas(grads_meta_vec.size());
  for (size_t i = 0; i < grads_meta_vec.size(); ++i) {
    grads_metas[i] = &grads_meta_vec[i];
  }

  auto moments1_meta_vec = MakeMetaTensor(input_moments1);
  std::vector<const phi::MetaTensor*> moments1_metas(moments1_meta_vec.size());
  for (size_t i = 0; i < moments1_meta_vec.size(); ++i) {
    moments1_metas[i] = &moments1_meta_vec[i];
  }

  auto moments2_meta_vec = MakeMetaTensor(input_moments2);
  std::vector<const phi::MetaTensor*> moments2_metas(moments2_meta_vec.size());
  for (size_t i = 0; i < moments2_meta_vec.size(); ++i) {
    moments2_metas[i] = &moments2_meta_vec[i];
  }

  auto beta1_pows_meta_vec = MakeMetaTensor(input_beta1_pows);
  std::vector<const phi::MetaTensor*> beta1_pows_metas(beta1_pows_meta_vec.size());
  for (size_t i = 0; i < beta1_pows_meta_vec.size(); ++i) {
    beta1_pows_metas[i] = &beta1_pows_meta_vec[i];
  }

  auto beta2_pows_meta_vec = MakeMetaTensor(input_beta2_pows);
  std::vector<const phi::MetaTensor*> beta2_pows_metas(beta2_pows_meta_vec.size());
  for (size_t i = 0; i < beta2_pows_meta_vec.size(); ++i) {
    beta2_pows_metas[i] = &beta2_pows_meta_vec[i];
  }

  auto master_params_meta_vec = MakeMetaTensor(input_master_params);
  paddle::optional<std::vector<const phi::MetaTensor*>> master_params_metas(master_params_meta_vec.size());
  for (size_t i = 0; i < master_params_meta_vec.size(); ++i) {
    master_params_metas->at(i) = &master_params_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] = kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  auto kernel_out_3_meta_vec = MakeMetaTensor(kernel_out_3);
  std::vector<phi::MetaTensor*> kernel_out_3_metas(kernel_out_3_meta_vec.size());
  for (size_t i = 0; i < kernel_out_3_meta_vec.size(); ++i) {
    kernel_out_3_metas[i] = kernel_out_3[i] ? &kernel_out_3_meta_vec[i] : nullptr;
  }
  auto kernel_out_4_meta_vec = MakeMetaTensor(kernel_out_4);
  std::vector<phi::MetaTensor*> kernel_out_4_metas(kernel_out_4_meta_vec.size());
  for (size_t i = 0; i < kernel_out_4_meta_vec.size(); ++i) {
    kernel_out_4_metas[i] = kernel_out_4[i] ? &kernel_out_4_meta_vec[i] : nullptr;
  }
  auto kernel_out_5_meta_vec = MakeMetaTensor(kernel_out_5);
  std::vector<phi::MetaTensor*> kernel_out_5_metas(kernel_out_5_meta_vec.size());
  for (size_t i = 0; i < kernel_out_5_meta_vec.size(); ++i) {
    kernel_out_5_metas[i] = kernel_out_5[i] ? &kernel_out_5_meta_vec[i] : nullptr;
  }
  phi::FusedAdamInferMeta(params_metas, grads_metas, MakeMetaTensor(*input_learning_rate), moments1_metas, moments2_metas, beta1_pows_metas, beta2_pows_metas, master_params_metas, MakeMetaTensor(input_skip_update), beta1, beta2, epsilon, chunk_size, weight_decay, use_adamw, multi_precision, use_global_beta_pow, kernel_out_0_metas, kernel_out_1_metas, kernel_out_2_metas, kernel_out_3_metas, kernel_out_4_metas, kernel_out_5_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, const phi::Scalar&, const phi::Scalar&, int, float, bool, bool, bool, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_adam_ compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_params, input_grads, *input_learning_rate, input_moments1, input_moments2, input_beta1_pows, input_beta2_pows, input_master_params, input_skip_update, phi::Scalar(beta1), phi::Scalar(beta2), phi::Scalar(epsilon), chunk_size, weight_decay, use_adamw, multi_precision, use_global_beta_pow, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4, kernel_out_5);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    for (size_t i = 0; i < params.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(params.at(i).impl().get());
      *target_ptr = *kernel_out_0.at(i);
    }
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    for (size_t i = 0; i < moments1.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(moments1.at(i).impl().get());
      *target_ptr = *kernel_out_1.at(i);
    }
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    for (size_t i = 0; i < moments2.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(moments2.at(i).impl().get());
      *target_ptr = *kernel_out_2.at(i);
    }
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    for (size_t i = 0; i < beta1_pows.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(beta1_pows.at(i).impl().get());
      *target_ptr = *kernel_out_3.at(i);
    }
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    for (size_t i = 0; i < beta2_pows.size(); ++i) {
      auto target_ptr = static_cast<phi::DenseTensor*>(beta2_pows.at(i).impl().get());
      *target_ptr = *kernel_out_4.at(i);
    }
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    if (master_params) {
      for (size_t i = 0; i < master_params->size(); ++i) {
        auto target_ptr = static_cast<phi::DenseTensor*>(master_params->at(i).impl().get());
        *target_ptr = *kernel_out_5.at(i);
      }
    }

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out_0, backup0);
  TransStride(dev_ctx, kernel_out_1, backup1);
  TransStride(dev_ctx, kernel_out_2, backup2);
  TransStride(dev_ctx, kernel_out_3, backup3);
  TransStride(dev_ctx, kernel_out_4, backup4);
  TransStride(dev_ctx, kernel_out_5, backup5);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> fused_gemm_epilogue(const Tensor& x, const Tensor& y, const Tensor& bias, bool trans_x, bool trans_y, const std::string& activation) {
  return fused_gemm_epilogue_impl(x, y, bias, trans_x, trans_y, activation);
}
PADDLE_API Tensor greater_equal(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("greater_equal", kernel_data_type);
  }
  VLOG(6) << "greater_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("greater_equal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("greater_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("greater_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& greater_equal_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("greater_equal", kernel_data_type);
  }
  VLOG(6) << "greater_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("greater_equal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("greater_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("greater_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor greater_than(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_than API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_than", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("greater_than", kernel_data_type);
  }
  VLOG(6) << "greater_than kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("greater_than", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("greater_than infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("greater_than compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& greater_than_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_than API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_than", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("greater_than", kernel_data_type);
  }
  VLOG(6) << "greater_than kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("greater_than", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("greater_than infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("greater_than compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor hardswish(const Tensor& x) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hardswish API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardswish", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardswish", kernel_data_type);
  }
  VLOG(6) << "hardswish kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("hardswish", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardswish infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardswish compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor less_equal(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("less_equal", kernel_data_type);
  }
  VLOG(6) << "less_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("less_equal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("less_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("less_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& less_equal_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("less_equal", kernel_data_type);
  }
  VLOG(6) << "less_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("less_equal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("less_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("less_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor less_than(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_than API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_than", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("less_than", kernel_data_type);
  }
  VLOG(6) << "less_than kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("less_than", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("less_than infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("less_than compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& less_than_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_than API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_than", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("less_than", kernel_data_type);
  }
  VLOG(6) << "less_than kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("less_than", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("less_than infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("less_than compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor matmul(const Tensor& x, const Tensor& y, bool transpose_x, bool transpose_y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matmul API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matmul", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matmul", kernel_data_type);
  }
  VLOG(6) << "matmul kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     attrs["transpose_x"] = transpose_x;
     attrs["transpose_y"] = transpose_y;
     phi::RecordOpInfoSupplement("matmul", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matmul infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MatmulInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), transpose_x, transpose_y, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matmul compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, transpose_x, transpose_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor maximum(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "maximum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maximum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("maximum", kernel_data_type);
  }
  VLOG(6) << "maximum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("maximum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("maximum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("maximum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor min(const Tensor& x, const IntArray& axis, bool keepdim) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "min API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "min", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("min", kernel_data_type);
  }
  VLOG(6) << "min kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("min", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("min infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ReduceIntArrayAxisInferMeta(MakeMetaTensor(*input_x), axis, keepdim, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("min compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor minimum(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "minimum API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "minimum", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("minimum", kernel_data_type);
  }
  VLOG(6) << "minimum kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("minimum", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("minimum infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("minimum compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor multiply(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor() && y.is_dense_tensor()) {

    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "multiply", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply", kernel_data_type);
    }
    VLOG(6) << "multiply kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"y", {
       (*input_y).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("multiply", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("multiply infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("multiply compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  if (x.is_selected_rows() && y.is_dense_tensor()) {

    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "multiply_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply", kernel_data_type);
    }
    VLOG(6) << "multiply_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"y", {
       (*input_y).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("multiply", input_shapes, attrs);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("multiply infer_meta", phi::TracerEventType::OperatorInner, 1);
    }
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::DenseTensor&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("multiply compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (multiply) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor& multiply_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }


  if (x.is_dense_tensor() && y.is_dense_tensor()) {

    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "multiply", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply", kernel_data_type);
    }
    VLOG(6) << "multiply kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"y", {
       (*input_y).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("multiply", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("multiply infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("multiply compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  if (x.is_selected_rows() && y.is_dense_tensor()) {

    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
    auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
        "multiply_sr", {kernel_backend, kernel_layout, kernel_data_type}, true);
    const auto& kernel = kernel_result.kernel;
    if (FLAGS_low_precision_op_list) {
      phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply", kernel_data_type);
    }
    VLOG(6) << "multiply_sr kernel: " << kernel;
    // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
    Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
    auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

    auto input_x = PrepareDataForSelectedRows(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {});

    auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
    if(phi::RecordOpInfoSupplement::IsEnabled()){
       std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
       {"x", {
       (*input_x).dims()}},
       {"y", {
       (*input_y).dims()}}};
       phi::AttributeMap attrs;
       phi::RecordOpInfoSupplement("multiply", input_shapes, attrs);
    }

    Tensor& api_output = x;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    auto backup0 = ProcessStrideBackup(&kernel_out);

    phi::RecordEvent *infer_shape_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      infer_shape_record_event = new phi::RecordEvent("multiply infer_meta", phi::TracerEventType::OperatorInner, 1);
    }

    auto origin_input_x = *input_x;
    phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

    phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

    if(infer_shape_record_event != nullptr){
      delete infer_shape_record_event;
    }
    using kernel_signature = void(*)(const phi::DeviceContext&, const phi::SelectedRows&, const phi::DenseTensor&, phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    phi::RecordEvent* kernel_record_event = nullptr;
    if(phi::RecordEvent::IsEnabled()){
      kernel_record_event = new phi::RecordEvent("multiply compute", phi::TracerEventType::OperatorInner, 1);
    }
      (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
    if(kernel_record_event != nullptr){
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {

      TransDataBackend(kernel_out, kernel_backend, kernel_out);

    }
    dev_ctx = GetDeviceContextByBackend(kernel_backend);
    TransStride(dev_ctx, kernel_out, backup0);

    return api_output;
  }

  PADDLE_THROW(common::errors::Unimplemented(
          "The kernel of (multiply) for input tensors is unimplemented, please check the type of input tensors."));
}

PADDLE_API Tensor not_equal(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "not_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "not_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("not_equal", kernel_data_type);
  }
  VLOG(6) << "not_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("not_equal", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("not_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("not_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& not_equal_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "not_equal API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "not_equal", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("not_equal", kernel_data_type);
  }
  VLOG(6) << "not_equal kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("not_equal", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("not_equal infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CompareInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("not_equal compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor remainder(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "remainder API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "remainder", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("remainder", kernel_data_type);
  }
  VLOG(6) << "remainder kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("remainder", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("remainder infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("remainder compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& remainder_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "remainder API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "remainder", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("remainder", kernel_data_type);
  }
  VLOG(6) << "remainder kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("remainder", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("remainder infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("remainder compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor set_value(const Tensor& x, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, const std::vector<phi::Scalar>& values) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "set_value API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value", kernel_data_type);
  }
  VLOG(6) << "set_value kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     attrs["shape"] = shape;
     attrs["values"] = "";
     phi::RecordOpInfoSupplement("set_value", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SetValueInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<phi::Scalar>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, shape, values, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& set_value_(Tensor& x, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, const std::vector<phi::Scalar>& values) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "set_value API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value", kernel_data_type);
  }
  VLOG(6) << "set_value kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     attrs["shape"] = shape;
     attrs["values"] = "";
     phi::RecordOpInfoSupplement("set_value", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SetValueInferMeta(MakeMetaTensor(origin_input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<phi::Scalar>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, shape, values, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor softmax(const Tensor& x, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softmax", kernel_data_type);
  }
  VLOG(6) << "softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("softmax", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SoftmaxInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& softmax_(Tensor& x, int axis) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softmax API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softmax", kernel_data_type);
  }
  VLOG(6) << "softmax kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("softmax", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softmax infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SoftmaxInferMeta(MakeMetaTensor(origin_input_x), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softmax compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor subtract(const Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "subtract API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("subtract", kernel_data_type);
  }
  VLOG(6) << "subtract kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("subtract", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("subtract infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("subtract compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API Tensor& subtract_(Tensor& x, const Tensor& y) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "subtract API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("subtract", kernel_data_type);
  }
  VLOG(6) << "subtract kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("subtract", input_shapes, attrs);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  auto backup0 = ProcessStrideBackup(&kernel_out);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("subtract infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto origin_input_x = *input_x;
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ElementwiseInferMeta(MakeMetaTensor(origin_input_x), MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("subtract compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, origin_input_x, *input_y, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);
  TransStride(dev_ctx, kernel_out, backup0);

  return api_output;
}

PADDLE_API Tensor tile(const Tensor& x, const IntArray& repeat_times) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tile API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tile", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tile", kernel_data_type);
  }
  VLOG(6) << "tile kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["repeat_times"] = repeat_times.GetData();
     phi::RecordOpInfoSupplement("tile", input_shapes, attrs);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tile infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TileInferMeta(MakeMetaTensor(*input_x), repeat_times, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tile compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(repeat_times), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> unique(const Tensor& x, bool return_index, bool return_inverse, bool return_counts, const std::vector<int>& axis, DataType dtype) {

  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED
        || kernel_layout == DataLayout::UNDEFINED
        || kernel_data_type == DataType::UNDEFINED ) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unique API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unique", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unique", kernel_data_type);
  }
  VLOG(6) << "unique kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}}};
     phi::AttributeMap attrs;
     attrs["return_index"] = return_index;
     attrs["return_inverse"] = return_inverse;
     attrs["return_counts"] = return_counts;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("unique", input_shapes, attrs);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unique infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::UniqueInferMeta(MakeMetaTensor(*input_x), return_index, return_inverse, return_counts, axis, dtype, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, bool, bool, bool, const std::vector<int>&, DataType, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unique compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, return_index, return_inverse, return_counts, axis, dtype, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  dev_ctx = GetDeviceContextByBackend(kernel_backend);

  return api_output;
}


}  // namespace experimental
}  // namespace paddle

namespace paddle {
PD_DECLARE_API(from_blob);
#ifdef PADDLE_WITH_DISTRIBUTE
PD_DECLARE_API(reshard);
#endif
}  // namespace paddle
